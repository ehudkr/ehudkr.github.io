[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications ",
    "section": "",
    "text": "My current research focus revolves mainly around causal inference. Both applications (mainly around healthcare) and methodology (including usability). I also dabble in information visualization and genetic-based risk models (polygenic risk scores). Prior to that, I worked on computational methods in molecular biology (genomics and transcriptomics). In addition to academic publications, I also issued several patents in the US.\nI will do my best to keep this listing updated, but in the plausible case I won’t, please see my Google Scholar page for the latest version."
  },
  {
    "objectID": "publications/index.html#publications",
    "href": "publications/index.html#publications",
    "title": "Publications ",
    "section": "Publications",
    "text": "Publications"
  },
  {
    "objectID": "publications/2023-causalvis/index.html",
    "href": "publications/2023-causalvis/index.html",
    "title": "Causalvis: Visualizations for Causal Inference",
    "section": "",
    "text": "Workflow summary"
  },
  {
    "objectID": "publications/2023-causalvis/index.html#citation",
    "href": "publications/2023-causalvis/index.html#citation",
    "title": "Causalvis: Visualizations for Causal Inference",
    "section": "Citation",
    "text": "Citation\n@inproceedings{guo2023causalvis,\n  title={Causalvis: Visualizations for Causal Inference},\n  author={Guo, Grace and Karavani, Ehud and Endert, Alex and Kwon, Bum Chul},\n  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},\n  pages={1--20},\n  year={2023}\n}"
  },
  {
    "objectID": "publications/2021-non-respiratory-covid19/index.html",
    "href": "publications/2021-non-respiratory-covid19/index.html",
    "title": "Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2",
    "section": "",
    "text": "Association of SARS-CoV-2 infection with several severe outcomes from 425,988 admitted individuals."
  },
  {
    "objectID": "publications/2021-non-respiratory-covid19/index.html#citation",
    "href": "publications/2021-non-respiratory-covid19/index.html#citation",
    "title": "Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2",
    "section": "Citation",
    "text": "Citation\n@article{el2021trends,\n  title={Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2},\n  author={El-Hay, Tal and Karavani, Ehud and Peretz, Asaf and Ninio, Matan and Ravid, Sivan and Chorev, Michal and Rosen-Zvi, Michal and Patalon, Tal and Shimoni, Yishai and Jain, Anil},\n  journal={medRxiv},\n  pages={2021--03},\n  year={2021},\n  publisher={Cold Spring Harbor Laboratory Press}\n}"
  },
  {
    "objectID": "publications/2019-positivitree/index.html",
    "href": "publications/2019-positivitree/index.html",
    "title": "A discriminative approach for finding and characterizing positivity violations using decision trees",
    "section": "",
    "text": "A tree-based approach for finding easily-characterizable positivity violations, commonly referred to as PositiviTree."
  },
  {
    "objectID": "publications/2019-positivitree/index.html#citation",
    "href": "publications/2019-positivitree/index.html#citation",
    "title": "A discriminative approach for finding and characterizing positivity violations using decision trees",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019discriminative,\n  title={A discriminative approach for finding and characterizing positivity violations using decision trees},\n  author={Karavani, Ehud and Bak, Peter and Shimoni, Yishai},\n  journal={arXiv preprint arXiv:1907.08127},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2019-causal-inference-evaluation-toolkit/index.html",
    "href": "publications/2019-causal-inference-evaluation-toolkit/index.html",
    "title": "An evaluation toolkit to guide model selection and cohort definition in causal inference",
    "section": "",
    "text": "This manuscript combines the evaluation suite with a specific use-case. A readers-digest version focusing on ROC curves for propensity score models is also available here"
  },
  {
    "objectID": "publications/2019-causal-inference-evaluation-toolkit/index.html#citation",
    "href": "publications/2019-causal-inference-evaluation-toolkit/index.html#citation",
    "title": "An evaluation toolkit to guide model selection and cohort definition in causal inference",
    "section": "Citation",
    "text": "Citation\n@article{shimoni2019evaluation,\n  title={An evaluation toolkit to guide model selection and cohort definition in causal inference},\n  author={Shimoni, Yishai and Karavani, Ehud and Ravid, Sivan and Bak, Peter and Ng, Tan Hung and Alford, Sharon Hensley and Meade, Denise and Goldschmidt, Yaara},\n  journal={arXiv preprint arXiv:1906.00442},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2018-rnase3-cleavage-rules/index.html",
    "href": "publications/2018-rnase3-cleavage-rules/index.html",
    "title": "In vivo cleavage rules and target repertoire of RNase III in Escherichia coli",
    "section": "",
    "text": "@article{altuvia2018vivo,\n  title={In vivo cleavage rules and target repertoire of RNase III in Escherichia coli},\n  author={Altuvia, Yael and Bar, Amir and Reiss, Niv and Karavani, Ehud and Argaman, Liron and Margalit, Hanah},\n  journal={Nucleic acids research},\n  volume={46},\n  number={19},\n  pages={10380--10394},\n  year={2018},\n  publisher={Oxford University Press}\n}"
  },
  {
    "objectID": "publications/2018-rnase3-cleavage-rules/index.html#citation",
    "href": "publications/2018-rnase3-cleavage-rules/index.html#citation",
    "title": "In vivo cleavage rules and target repertoire of RNase III in Escherichia coli",
    "section": "",
    "text": "@article{altuvia2018vivo,\n  title={In vivo cleavage rules and target repertoire of RNase III in Escherichia coli},\n  author={Altuvia, Yael and Bar, Amir and Reiss, Niv and Karavani, Ehud and Argaman, Liron and Margalit, Hanah},\n  journal={Nucleic acids research},\n  volume={46},\n  number={19},\n  pages={10380--10394},\n  year={2018},\n  publisher={Oxford University Press}\n}"
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Materials",
    "section": "",
    "text": "Please properly attribute if using.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCausaliTea\n\n\nA casual reading club on causality.\n\n\n\n\ncausal inference\n\n\n\n\nA casual reading club on causality.\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCausal Inference - Motivation\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\nMotivating causal inference\n\n\n\n\n\n\nMay 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCausal Inference - Concepts\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\nBroad overview of causal inference concepts\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCausal Inference - Methods\n\n\n\n\n\n\n\ncausal inference\n\n\n\n\nStatistical methods for causal inference\n\n\n\n\n\n\nNov 18, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html",
    "href": "materials/causal-inference/methods/index.html",
    "title": "Causal Inference - Methods",
    "section": "",
    "text": "An overview of many causal inference methods."
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#zoo-of-causal-methods-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/methods/index.html#zoo-of-causal-methods-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Methods",
    "section": "",
    "text": "An overview of many causal inference methods."
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#the-simpsons-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/methods/index.html#the-simpsons-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Methods",
    "section": "The Simpsons ",
    "text": "The Simpsons \n Matching and Inverse Probability Weighting with The Simpsons.\n(This is before the time of Powerpoint morph; this was painstaking.)"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#causal-inference-with-deep-learning-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/methods/index.html#causal-inference-with-deep-learning-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Methods",
    "section": "Causal inference with deep learning ",
    "text": "Causal inference with deep learning \nRelevant up to ~2020"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#interrupted-time-series-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/methods/index.html#interrupted-time-series-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Methods",
    "section": "Interrupted time series ",
    "text": "Interrupted time series \nFrom difference-in-differences to synthetic controls."
  },
  {
    "objectID": "materials/causal-inference/causalitea/index.html",
    "href": "materials/causal-inference/causalitea/index.html",
    "title": "CausaliTea",
    "section": "",
    "text": "CausaliTea is a reading club on causality [source ]\n\n\n\nCausaliTea’s logo"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html",
    "href": "blog/2023/03-sequential_trial_design/index.html",
    "title": "Sequential trial design for causal inference",
    "section": "",
    "text": "I’m not special, so I’ve spent a lot of time addressing confounding bias in causal inference. I even created and still maintain an open-source Python package whose main focus is flexible causal inference modeling (pip install causallib). There’s a place to discuss why confounding bias is the most popular bias, but this blog post is not it.  I have a different bias I want to focus on.\nThere’s actually some compelling evidence (well, at least anecdotal) that confounding bias has less of a biasing effect than what one would expect. One example is Garcı́a-Albéniz, Hsu, and Hernán (2017) examining the effect of colorectal cancer screening on colorectal cancer incidence. It showed the same survival curves for treatment and control units when adjusting and not adjusting for confounding factors. This, in and of itself, is a poor evidence, but the exact same shape of survival curves also appeared in Bretthauer et al. (2022), an RCT examing the exact same question. This suggests that confounding bias did not affect the observational study.1\n\n\nThis bias originates from improperly setting the time-zero. Time zero (or “index date”) is the point in time which splits the baseline period from the follow-up period. History from future, retrospectively speaking. It usually the time in which treatment is initiated, and therefore the point in time from which we look backwards to obtain historical data (baseline covariates) and look forward to gather follow-up data (outcomes).\nAs such, to properly set up time zero, three things must align:\n\nThe subject must meet the eligibility criteria\n“Treatment” must be assigned\nOutcomes are beginning to be counted.\n\nLearning causal inference, we are used to getting datasets with nice and precise binary treatment variables and covariates. But where did these come from? People’s lives are not cross sectional, they are a trajectory through time. So there are lots of design decisions to be made in order to squeeze all of it into tabular form. Having longitudinal data makes it more possible, but not more easy. How do we decide how to assign a person into either treatment groups (or not at all if not eligible).\nGiven this presentation, you might see why active-comparator designs are so appealing. It is more straightforward to compare to active treatments, head-to-head, since we just define the first treatment initiation of each drug (the two groups) as our time zero.\nThe trickier part is when we want to design a study to compare treatment initators with non-users. This is often of interest in pragmatic trials. What is the index date for someone who just lived their life never getting treated? Non-initators have no point in time where they start treatment, making it harder to align this non-existing “treatment assignment” to the follow-up and eligibilty.\nThis often leads to comparing either persistent users or those who taken the drug at some time (i.e., all or any), to those who never used it. This study design does very little to inform physicians how to act on the patient they are currently facing. “Take this drug and if you survive the next 5 years (i.e., become a persistent user), then you will reduce your risk by x percent” does not inspire much confidence and is not very helpful for the patient who is living now in the present and not five years into the future.2"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#introduction",
    "href": "blog/2023/03-sequential_trial_design/index.html#introduction",
    "title": "Sequential trial design for causal inference",
    "section": "",
    "text": "I’m not special, so I’ve spent a lot of time addressing confounding bias in causal inference. I even created and still maintain an open-source Python package whose main focus is flexible causal inference modeling (pip install causallib). There’s a place to discuss why confounding bias is the most popular bias, but this blog post is not it.  I have a different bias I want to focus on.\nThere’s actually some compelling evidence (well, at least anecdotal) that confounding bias has less of a biasing effect than what one would expect. One example is Garcı́a-Albéniz, Hsu, and Hernán (2017) examining the effect of colorectal cancer screening on colorectal cancer incidence. It showed the same survival curves for treatment and control units when adjusting and not adjusting for confounding factors. This, in and of itself, is a poor evidence, but the exact same shape of survival curves also appeared in Bretthauer et al. (2022), an RCT examing the exact same question. This suggests that confounding bias did not affect the observational study.1\n\n\nThis bias originates from improperly setting the time-zero. Time zero (or “index date”) is the point in time which splits the baseline period from the follow-up period. History from future, retrospectively speaking. It usually the time in which treatment is initiated, and therefore the point in time from which we look backwards to obtain historical data (baseline covariates) and look forward to gather follow-up data (outcomes).\nAs such, to properly set up time zero, three things must align:\n\nThe subject must meet the eligibility criteria\n“Treatment” must be assigned\nOutcomes are beginning to be counted.\n\nLearning causal inference, we are used to getting datasets with nice and precise binary treatment variables and covariates. But where did these come from? People’s lives are not cross sectional, they are a trajectory through time. So there are lots of design decisions to be made in order to squeeze all of it into tabular form. Having longitudinal data makes it more possible, but not more easy. How do we decide how to assign a person into either treatment groups (or not at all if not eligible).\nGiven this presentation, you might see why active-comparator designs are so appealing. It is more straightforward to compare to active treatments, head-to-head, since we just define the first treatment initiation of each drug (the two groups) as our time zero.\nThe trickier part is when we want to design a study to compare treatment initators with non-users. This is often of interest in pragmatic trials. What is the index date for someone who just lived their life never getting treated? Non-initators have no point in time where they start treatment, making it harder to align this non-existing “treatment assignment” to the follow-up and eligibilty.\nThis often leads to comparing either persistent users or those who taken the drug at some time (i.e., all or any), to those who never used it. This study design does very little to inform physicians how to act on the patient they are currently facing. “Take this drug and if you survive the next 5 years (i.e., become a persistent user), then you will reduce your risk by x percent” does not inspire much confidence and is not very helpful for the patient who is living now in the present and not five years into the future.2"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#sequential-trial-design",
    "href": "blog/2023/03-sequential_trial_design/index.html#sequential-trial-design",
    "title": "Sequential trial design for causal inference",
    "section": "Sequential trial design",
    "text": "Sequential trial design\nSo how can we force the alignment of eligibility, treatment initiation, and follow-up into a proper time zero?\n\nA single “sequential” trial\nOne simple approach is to use calendar time. Set the time-zero to a specific date. Say, for instance, January 1st, 2020. This will be the point in time splitting history from future. Keep whoever is eligibile to participate at this date, discard the rest; data from before this date are the baseline covariates; data from this date forward is the follow-up; and whoever got treated on this date is considered in the treatment group, the rest are controls.\nIndividuals treated before January 1st are probably ineligible (most study designs might enforce participants to be treatment naive). Individuals treated after January 1st are considered controls, because time-zero is set at January 1st, not in the future. Though a per-protocol analysis might decide to censor them from follow-up because they deviated from their original (control) assignment. This creates a design that answers a question of treat now vs. don’t treat now (but possible treat later).\nHowever, this single trial is too naive and not very efficient. We miss all the treated individuals in the past and in the future. There might even not be anyone treated on January 1st. We throw away a lot of information. That’s a big blow to statistical power.\n\n\nA sequence of sequential trials\nTherefore, a natural extension would be to just repeat the process. Repeat it for January 2nd, 2020, for January 3rd, 2020, for January 4th, and so forth. For each time point we:\n\nConsider whoever meets the eligibility criteria and discard the rest,\nSet the treatment indicator based on who got treated,\nExtract baseline covariates prior to that time point,\nExtract outcome information from that time onwards.\n\n\n\n\nAn illustration of the sequential trial process at a data-set level3. At each time \\(t\\) (starting with \\(t=0\\), like Jan 1st), we check whoever is eligible and assign them to their appropriate treatment group according to their treatmeent status at that time. At the next time step (\\(t=1\\) or Jan 2nd), the pool of eligible individuals can change in several ways: 1) treated individuals from the past are probably no longer eligible, at least not before some washout period passes; 2) eligible people can become ineligible regardless of their past treatment status (e.g., they can turn 65 and no longer fit the inclusion criteria). 3) Likewise, ineligible people from the past can become eligible if they suddenly fit the criteria (e.g., an individual can turn 18 at this time and become eligible). At each time we collect baseline and follow-up data according to the study design relative to this index date. This process repeates for all available time steps.\n\n\nThe way I imagine this process is that we have stencil-like mold, defined by our eligibility criteria, from which we funnel observations like a strainer. We go to January 1st, put the stencil-strainer, shake the database and filter only eligible individuals at the time. We then go to January 2nd, filter; and so on and so forth, rinse and repeat. A moving window through time, only that window is specially shaped by the eligibilty criteria.\nThis means a single person can have multiple records (entries, or rows) in the dataset. One for each trial (time-zero).\nWe can see this illustrated below with a person’s timeline and its corresponding person-time tabular form. In this study design, baseline covariates are taken 4 time-steps relative to time-zero (e.g., 2 years prior to time-zero), and follow-up is taken 4 time steps starting time-zero (e.g., outcome within 2 years). Thus, the covariate profile changes as time-zero progresses. At times 4-7, 🏊‍♂️ is included as it is within the baseline period dictated by the study design, but it is no longer counted in time 8. Similarly, the outcome 🚑 is not observed at time 4, only starting at time 5.4 To make things more explicitly, I also indicate eligibility; in this example, the study design requires participant to no been treated in the last 4 time-steps (treatment-naive), and so at times 7 and 8 the person is no longer eligibile as they were treated in time 6.5\n\n\n\nAn illustration of a person timeline and how its corrsponding person-time tabular form. It shows multiple time-zeros set from time 4 throughout time 8 (think Jan 1st to Jan 4th from above). Different time-zeros have different covariate (🏊‍♂️) profile, different outcome (🚑) based on the moving follow-up window, and - although that person is treated (💊) - they are not always considered as such.\n\n\n\n\nAn uncontrolled control explosion\nOften, getting treatment is rarer than not getting treated. This imbalance is not inherently important, but can lead to some difficulties in practice. Looking at the resulting dataset above, we see that sequential trials exacerbate this even further since even treated individuals contribute (at-least more) non-treated rows, all the while never-treated individuals just multiply throughout time.\nThis control group inflation can be undesireable in practice. It increases demand of compute resources, both in terms of memory and runtime, while not necessarily contributing enough information to justify it. Statistical power is determined by the smallest group, anyway; and, while I don’t know how to formally extend this argument into a repetaed-measurement setting, I have an intuition that dozens and dozens of duplicate (and near-duplicate) records hit the diminishing returns curve somewhat quickly.\nThere are several valid strategies to deal with settings where eligibility criteria is met multiple times per person and lead to this cambrian-like explosion of controls records.\nFirst, we can just randomly subsample a fixed sample (e.g., 10%) from all the controls records from our dataset (Dickerman et al. (2019)). Since this blindly regards person and time information, it can have some unintended consequences like discarding entire time steps or individuals by some unfortunate random chance. This can reduce variability in the dataset and hurt the infromation content of it.\nFortunately, this can be easily solvable by forcing some structure on the discarding, like stratified sampling by time-steps and/or person IDs.\nWe can take this structural subsampling a step forward and force a single entry per-person, i.e., reduce it to a single eligibile record per person. When doing so, it is important to reduce it without taking strict advantage of “future infromation”, namely by the treatment and outcome status of people’s timeline. Two common ways are taking the first eligibility record per-person, or taking one randomly (Hernán and Robins (2016)). Both are valid, non-biasing approaches, but they suffer from similar disadvantages presented above when randomly subsampling controls. Namely, they can discard more useful infromation and keep more redundant infromation instead. Specifically, they can blindly discard treatment records, which can be very precious. In reasonable cases where a treated person-time is not treated at first eligibile time, or in cases where it simply been randomly discarded. As said above, treated person-time records are few to begin with, and discarding them reduces the size of the treatment group, which decrease precious statistical power unnecessarily.\nA forth option that maximizes the size of the (smaller) treatment group is a combined approach: first, gather all the treated person-time records; and second, take first (or random) eligibility time per-person (Garcı́a-Albéniz, Hsu, and Hernán (2017)). This is basically a different view on the first approach. Both first fix (or take) all the treated person-times and then subsample just the control person-times. Only here, the subsampling is further structured to keep one record per person (including individuals who are at some point treated) by determinstic subsampling (taking records at first eligibilty time) or random subsampling (taking a random record from all eligible times of an individual).\n\n\n\nIllustration of the four strategies for dealing with the inflation of control person-times in multiple eligibility settings. It describes 3 people: A, B, and C and their treatment status over 5 time steps. First, using all person-times6 from which we can derive the rest. Second, first eligibilty only, where B1, A1, and C2 participate (note C1 is ineligible in this example). Third, a random eligibility time per person. Note that in both cases we lose A3 as a treated unit. Forth, a mixed approach where we take all treated person-times and 1st eligibilty time. Lastly, a fifth strategy taking all treated person-times and randomly sampling a fixed proportions from the controls is not showed.\n\n\nIn the mixed approaches, it is important to choose control person-times while ignoring treatment indication in a person’s timeline. For instance, in the example above, it might be tempting to discard control trials of person A, because person A is eventually treated. Or, put differently, after selecting all the treated person-times, select a subsample of person-times controls while excluding people chosen in the treated person-times. However, this can bias the analysis as it will create a control group that is, by construction, never-treated; and never-treated might not be a comparable control for incidence users we care about (Garcı́a-Albéniz, Hsu, and Hernán (2017)).\n\nContemporary controls vs. self controls\nAs mention above, using all person-times can create computational strains and inefficiencies. All while in single-eligibility designs we might discard precious information. Mixed approaches where we first select all treated person-times is a good middle ground solution, but there is no free lunch.\nWe originally perform a trial at each time step. Looking at each trial independently, we adjust for time confounding as each treated person at that time has corresponding contemporary controls. However, when we dillute the controls (and the mixed approach of all-treated plus 1st eligibilty time is the most extreme approach), we reduce the weight of those contemporary controls.\nInstead, vieweing the dataset as a whole, we increase the weight of self controls. This is because for each person treated not at first eligibilty time we will have, by construction, two instances - one as treated and one as a control. When we discard records of other people from that time step, because this is not their first eligibility time, we reduce the relative weight (number) of contemporary controls and, by definition, increase the relative weight of the first eligible control person-times of those later treated.\nTo simplify with the example above, in the all-person-time approach, A3 is controlled by both its past self (A1) and a contemporary record (C3). When we go to the mixed approach, we drop C3, making A1 the sole control for A3. Therefore, we reduced the relative weight of contemrporary controls (at time 3) and increased the relative weight of self-controls.\nThis can create bias if person \\(i\\) at time \\(t\\) is not exchangeable with itself (person \\(i\\)) at time \\(t^*\\).\nThis is quite reasonable to assume, as people’s condition might trend over time, like disease progression status. Furthermore, if this progression is not captured by the covariates, it can create confounding bias (time confounding). Therefore, another cosideration should be to check for such cases, and, following footnote 7, keep trial ID in mind when adjusting and make sure every time-step can act as a quasi-independent trial."
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#discussion",
    "href": "blog/2023/03-sequential_trial_design/index.html#discussion",
    "title": "Sequential trial design for causal inference",
    "section": "Discussion",
    "text": "Discussion\n\nImmortal (bias) Combat\nImmortal time bias is when participants of a cohort cannot, by construction, experience the outcome during some period of the follow-up time8. It usually happens when the researcher fails to align the cohort entry time (eligibility time) with the time of getting the treatment (time zero). If the researcher classifies that individual as treated, and start counting the outcome from cohort entry - even though they are not treated at the start of eligibility - it necessarily means that that person did not (and couldn’t have) experience the outcome in the time period between eligibility and time of actual being treated. If you are not familiar with immortal time bias, I suggest going over its entry in the Catalog of Bias website.\nOne approach to overcome immortal time bias is to do a time-dependent analysis. Namely, we need to somehow associate the time between eligibility and treatment with the control group, and only the time between treatment and outcome (or end of follow-up) with the treatment group. It can be done in several ways, depending on the analytical approach, like using an offset in a Poisson risk-rate regression. But one generic solution is to work in person-time data. Basically, allow the person-time records between eligibility and treatment to be classified to the control group, and the person-time records between treatment onwards to hte treatment group.\nThis general solution is essentially what happens in the sequential trials design. Although the proposed solution is person-centric – assigning the person’s time to separate treatment groups – and sequential trial is cohort-centric – assigning a person to its appropriate group at each time-step. However, the resulting view is basically the same, as the end product is a table where that same person is classified to the control group in the period starting at its eligibility time until they are assigned a treatement and move to the treatment group.\nThe only nuance is in how we construct the outcome in the follow-up time. For sequential trials to be absolutely equivalent to the person-time analysis, the control person-times should be censored at the time that person receive treatment. This ensures the outcome contributed to the control group is solely for the period that person is considered untreated. This censor-at-change is also known as a per-protocol analysis, where we censor individuals who deviate from their originally assigned protocol (controls who eventually get teated).\n\n\nMeta-analysis: pooling estimates vs. pooling records\nIn the sequential trial approach we essentially create multiple studies. Meta-analysis is the science of synthesizing evidence across multiple similar studies. There are two main approaches for doing meta analyses. The classic approach would use aggregate summaries from different studies, like the estimated effect and confidence interval, and combine them into a single estimate. However, advances in data management and digital communication made it increasingly feasible to share the original data on individual participants and give rise to a second type of meta analysis: one that combines individual-level data, basically consolidating datasets (Riley, Lambert, and Abo-Zaid (2010), Lin and Zeng (2010)).\nSequential trial design results in a bunch of trials, and, therefore, these two approaches are applicable to analyzing sequential trials. Either estimate the effect from each trial separately and then synthesize the estimated effects. Or just consolidate the extracted person-time records over the separate trials to obtain one big dataset.\nThere seems to be some benefits for doing individual-level meta-analysis (Riley, Lambert, and Abo-Zaid (2010)), but I’m not expert enough to judge. However, since we extract all the trials from the same database, using the same study design with the same covariates, treatment and outcome defintions, it seems even more appropriate (or natural) to consolidate the raw data. This post even hints to this approach, as this, to the best of my knowledge, the only way sequential trials have been analyzied in the epidemiological literature. However, it is also worth putting the finger on the fact that this is essentially becomes a meta-analysis, and as such, other approaches are applicable.\nThe individual-level analysis should respect the person-time structure of the combined trials. They might apply a multilevel (hierarchical) model, partially pooling estimates across time (trials). They should defintely use robust variance estimation, either by sandwich (HC) estimates or bootstrap, to estimate standard errors since the records in the consolidated dataset are no longer independent but are repeated measures of the same pool of individuals.\n\n\nLimitations\nI have already mentioned the additional computational strain caused by the inflation of the dataset moving from person to person-time format; how control-dilution can hurt exchangeability; and the additional considerations that are required in computing standard errors as the rows of this dataset are no longer independent.\nHowever, in my opinion the biggest limitation of this method is that it requires the analysts to discretize time from the get go. The granularity of the time-steps is not just predefined, but is baked into the earliest point in the analytic pipeline - the data extraction from the database. As such, it means a sensitivity analysis on this time-step parameter is laborious as it requires applying the entire pipeline from data extraction to effect estimation.\nThis is problematic as the time granularity has both computational and practicical implications. Ideally, the time-step size should not be too small to avoid creating too redundant repeatitions. Too small and the size of the expanded person-time dataset will be too large and incur heavy computational costs (but may also cause floating-point issues as some survival models multiply probabilities [fractions] over time-steps, so too many time-steps may become numerically unstable). On the other hand, too large time-steps can bunch together too much information (like treatment assignment and outcome) and reduce information content of real treatment-outcome associations. The size of the time-step should be clinically (or practically) meaningful, but it’s ok to find a more proper value around it that also optimizes computational aspects. This is computationally harder in sequential trials since the size of the time-steps is baked deep into the construction of the analytical datasets.\n\n\nSummary\nSequential trials design is a generic approach that can solve many time-related biases (Hernán et al. (2016)). This was a high-level introduction, which I think is missing from the methodological literature, but I also hope it entails some interesting point of views."
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#footnotes",
    "href": "blog/2023/03-sequential_trial_design/index.html#footnotes",
    "title": "Sequential trial design for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA similar phenomenon was also observed in Hernán et al. (2008), finding similar hazard ratios (in terms of point estimation and confidence intervals) between adjusted and unadjusted estimates, suggesting the cause of discrepency between previous observational studies (even using the same data as they did) and RCTs was not confounding bias but rather the time-zero-related biases that are the subject of this blog post.↩︎\nFurthermore, I believe this discrepancy in how to define treatment groups, namely, how to define the \\(1\\) and \\(0\\) in \\(Y^1\\) and \\(Y^0\\), boils down to the causal consistency assumptions. Defining a poor treatment mechanism in the study design means the outcome one observes in their data (\\(E[Y|A=a]\\)) will not equal the hypothetical outcome they care about (\\(E[Y^a]\\)).↩︎\nThis also illustrates why sequential trials may be sometimes called “nested” trials. This is because, in some common and simple settings, the trial at time \\(t\\) (eligible people at time \\(t\\)) are nested within the controls of time \\(t-1\\). However I think “nested trials” has a different meaning in the RCT landscape, so “sequential trials” is both more general and non-taken.↩︎\nIn time-to-event settings, the follow-up window is often as long as can be possibly observed, avoiding explicitly stating a follow-up timeframe. In such scenarios, we can see the time to outcome (🚑) decreases as time-zero progresses. ↩︎\nFirst, note that they might also be inelgible because they don’t have sufficient follow-up. Second, that person may also be ineligible at time 10 as they are no longer outcome-naive, too.↩︎\nNote time 2 (Jan 2nd) may be redundant as it containd only control units. If time has some effect — i.e., people at time \\(t\\) might not be fully exchangeable with people at time \\(t^*\\) — then not having any comparable treated units at time 2 might violate a positiity (overlap) requirement. Thus time steps with all-controls may be further discarded in the preprocess step (drop the rows) or in the analytic step (e.g., apply exact matching on the trial ID [time step] and consequently discard such records).↩︎\nNote time 2 (Jan 2nd) may be redundant as it containd only control units. If time has some effect — i.e., people at time \\(t\\) might not be fully exchangeable with people at time \\(t^*\\) — then not having any comparable treated units at time 2 might violate a positiity (overlap) requirement. Thus time steps with all-controls may be further discarded in the preprocess step (drop the rows) or in the analytic step (e.g., apply exact matching on the trial ID [time step] and consequently discard such records).↩︎\nIt is called “immortal” since often that outcome is death. So if one cannot experience death for a period of time, they are basically immortal.↩︎"
  },
  {
    "objectID": "blog/2023/01-causal-inference-is-a-mindset/index.html",
    "href": "blog/2023/01-causal-inference-is-a-mindset/index.html",
    "title": "Causal inference is a mindset",
    "section": "",
    "text": "I’m a causal inference enthusiast, practitioner, and advocate (borderline missionary if we’re completely honest). I believe people are causal thinkers who ask causal questions and expect causal answers; that we understand the world through causal relationships and causal mechanisms. But when people come to me for help or education, they usually end up disappointed. They come for a solution and discover more questions. They come for a code library to import and end up with a study design to implement.\nThis is because causal inference is first and foremost a mindset, not a particular tool. There is no neural network architecture that will just spit out causal effects. On the contrary, if you organize your data properly, causal inference can be a mere average or a simple linear regression at most.\n\nExcellent study designs make simple analytics.\n\nThis is why causal inference is a bait and switch scheme. We talk a lot about causality but then provide associational statistical tools. And once we estimate a statistical association, claiming it is causal is actually a leap of faith. Of course we have mathematical theory to ground on, but its assumptions are untestable. So at the end of the day, causal claims from observational data are an educated leap of (rational) faith.\n\nCausation = Association + Logic\n\nCausation consists of identification and estimation. Deep learning driven thinking has somewhat sidelined practitioners from thinking of identification. We rarely consider whether the question is even solvable, or is it solvable with the data we have; and if so — what approach or tools can solve it. We easily throw it to a kitchen sink of neural networks to get an answer— we focus exclusively on estimation, forgetting some problems cannot be solved by a single prediction model. But estimation can get you no further than association, and without identification you can’t get causation. Causality requires embracing the beyond-the-data logic that transcends an association into causation, but it’s a hard pill to swallow— and even harder to convince others to join — in the current Big-Data♡AI climate.\nPeople expect a magic library to import in R or to conjure up a model in PyTorch like some causal alchemists. But then I shoot up my slides and start blubbering about causal roadmaps, target trial emulation, careful time-zero considerations and variable selection. People are so used to post-AlexNet machine learning approaches, they are baffled that I have so much to say about study design and DAGs.\nThis is not to say that causal estimation models are bad, on the contrary, they are great (there’s a good reason why I champion making flexible causal models approachable with causallib); they make the counterfactual prediction explicit and clear. This also doesn’t mean causal estimation is easy, far from it — I’m a strong proponent for sequeling The Book of Why with The Book of How — causal learning suffers from the same complexities machine learning does and then some. Also, my intention is not to bash deep-learning, which I myself sometimes use for causal effect estimation. But overcoming confounding bias given observed confounders is just one stair in the staircase towards claiming causation.\nIn the trenches, causality doesn’t always stand up to its hype because it is different from machine learning. The truth is that causal inference is more of a mindset, than a particular tool. And this truth is a tougher sell."
  },
  {
    "objectID": "blog/2021/02-why-balance-covariates/index.html",
    "href": "blog/2021/02-why-balance-covariates/index.html",
    "title": "Why we care for covariate balancing in comparative studies",
    "section": "",
    "text": "Comparative analysis is the scientific method for discovering effectiveness. Whether it is in medicine, economics, or elsewhere, we perform studies that collect information and compares two groups in order to see which is better and if we can learn any means to improve. These studies can be experiments where we control for the group assignment via randomization, or observational studies where participants self-assign.\nOne of the first tasks in analyzing data from such studies will be to compare the characteristics of the two groups. It isn’t called table one for nothing. In which, we summarize the distribution of variables between the two groups and might even perform a statistical test on how the groups differ. There’s a long-lasting debate on whether that’s a good practice or not. In general, biostatisticians tend to avoid it, while economists tend to prefer it. The recent Data Colada analysis of Dan Arielly fraudulent data — which part of the evidence was big baseline differences albeit the randomization — reopened this Pandora box, pitting academics against one another on Twitter.\nIn this post is my theory on why we compare balancing in baseline characteristics between groups.\n\nHow we infer causality\nFormalizing causality is a task so hard, it left philosophers baffled to this day. In the Rubin’s causal model, we have two unobserved quantities: \\(Y^1\\) is the hypothetical outcome a person would have had — had he been assigned a treatment, and \\(Y^0\\) is the hypothetical outcome that that same person would have had had he not been treated. Once we have these two hypothetical trajectories, two parallel universes in which everything is equal except the treatment assignment of that person — we can isolate the impact of the treatment. If we observe the outcome in both worlds, we can see the causal effect of the treatment — the changes in outcomes that were caused only due to the treatment.\nHowever, in reality — outside the Marvel Cinematic Universe of Rick and Morty — we don’t have access to this multiverse. People either get treated or they don’t.\nAnd so, because we can’t both give and not-give a treatment to the same person, the next best thing is to give a group of people the treatment and another group a placebo and compare the two groups. And this is where we start regarding group balancing. It is only but intuitive that we will want the groups to be identical in all regards, except to the treatment assignment, so we can isolate the contribution of the treatment, and cancel out changes in outcomes that may be due to differences in baseline characteristics.\n\n\nComparing factors is theoretically redundant\nHere, I said it. There are no theoretical justifications to compare the distribution of covariates between groups.\nStatistical theory of causal inference lays out three conditions in order to obtain causal effects from data: positivity, consistency, and exchangeability/ignorability. The crux of the argument considers the latter.\nIntuitively, the exchangeability assumption captures the notion that there shouldn’t be any systematic differences between the groups.\nMathematically, the exchangeability assumption regards the independency of the potential outcomes from the treatment assignment: \\(Y^0, Y^1 \\perp \\!\\!\\! \\perp A\\) .\nYou see what’s in there? This means thatit is the distribution of potential outcomes that should be balanced between groups, not the baseline factors!\n\n\nWhy we still do it\nIn case you missed it, the necessary exchangeability condition above regards potential outcomes — a hypothetical quantity we can't really observe.\nAnd if we can't observe it, we can't test to validate whether it holds in any particular case.\nHowever, we do have some prior understanding of how the world works. For example, clinicians may know that the risk for stroke is dependent on one's age, sex, and history of blood pressure; economists may know that one's salary depends on their past salaries and education level. Therefore, we are able to, at-least mentally, construct some function from baseline factors to the outcome. In turn, this means that if the variables \\(X\\) are balanced between groups then \\(f(X)\\) (which is some projection of them onto a scalar) is also balanced between groups, which suggests the potential outcomes are balanced between groups.\n\n\n\nA mental model of how we move from the available balancing of covariates to the desired balancing of potential outcomes. Because the outcome is a function of the covariates, we are willing to hypothesize that balanced covariates are likely to result in balanced potential outcomes.\n\n\nTesting for covariate balancing is an educated mental shortcut we do. A sort of an ambiguous control test — good balancing is reassuring but not definitive, bad balancing is alarming but not devastating. We just grasp on whatever data we can observe (covariates) and try to use our expertise, common sense, and logic to project it onto what we can't observe (potential outcomes).\n\n\nConclusions\nCovariates will be balanced due to randomization, but it is not the main reason why we randomize. Balanced covariates are just a convenient side effect while we try to balance the distribution of potential outcomes.\nTherefore, it is important to understand that testing for covariate balancing is not the end goal. It is, at best, a proxy for what we really want to know but cannot have. And being the humans that we are — instead of doing nothing about it, we try doing our best.\n\n\n\nAppendix: what can we benefit from covariate balancing tests and at what cost?\nI hope this explained why a debate exists regarding balancing tests. There's no factual right or wrong, only a debate on whether that mental leap from covariates to potential outcomes is uncalled for or not.\nIn my opinion the criticism against it is valid: First and foremost, variables must not be balanced — it the potential outcomes that should. Second, even if they are not — this is what statistical adjustment is for. Employing statistical tests in randomized data is even more baseless — there are no two population, just groups randomized from the same population — the null hypothesis is known to be true. Besides, for a large enough sample even meaningless small differences can become statistically significant, but it doesn't mean they are of practical importance. And for large enough number of covariates, some may pass the statistically significant threshold simply by chance of sampling (a false-positive, or type-1 error), rather than inherent difference in the characteristics between groups. Therefore, having statistical tests (and p-values) in a table one may be considered redundant or even counterproductive — causing confusion by focusing readers — not all of whom are statistical experts — on unessential information.\nHowever, the criticism for it is also valid: randomization might not be that simple to pull off (especially blindly, across multiple facilities, etc.) and humans do tend to err whenever they have the chance to. What guarantees do we have randomization was applied correctly? Do we blindly believe the process? Shouldn't we validate it? Unfortunately, verifying random assignment is near-impossible due to, well, the randomness inherent to the process. There are explicit tests for randomness but checking for balancing is a simple one to employ. Again, it can only hint a problem exist, flagging out more attention is deserved, but it cannot be definitive.\nTherefore, in my opinion, when working on randomized data, it's ok to test for balancing as a sanity check, but don't show it off (and if you do, bury it in an appendix where no one will read it anyway. Just like this section. Consider commenting with a penguin if you do?). It can be misleading at worse or be an avenue for misguided criticism at best (don't feed the trolls)."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#background",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#background",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Background",
    "text": "Background\n\nThe fundamental problem of causal inference\nEvaluating causal inference models is literary impossible. Few scientific concepts are so pompously named — yet accurately describe the gravity of an issue — as the notorious “fundamental problem of causal inference”.\nBriefly, the prediction task in causal inference is different than that of supervised machine learning (ML). While in ML we interpolate the target to new unseen samples, in causal inference we extrapolate the target from units in one group to units in the other group. Because in any given time a unit can only be in one group and not the other (e.g., you either have received a drug or you haven’t), we lack the ground-truth labels to compare against our predictions. Counterfactual outcome prediction cannot be derived like regular supervised prediction, nor can it be evaluated as one.\n\n\nCausal models as meta-learners\nMost causal inference algorithms usually have some machine learning core — a statistical model that predicts the outcome or treatment assignment. Once a mapping between features to targets is obtained, causal models can then have various ways to indirectly apply those statistical predictions to obtain a causal estimate.\nFor example, inverse probability weighting (IPW) is a causal model that estimates the causal effect by first modelling the treatment assignment. It takes any machine learning classifier that can also output a continuous score between 0 and 1 and assume it to model the probability of being treated: \\(\\hat{p}=\\Pr[T|X]\\). It regresses the binary treatment assignment (\\(T\\)) against the features (\\(X\\)), then takes the inverse of that predicted scores and use them to create a weighted average of the outcome.\nHaving this machine-learning backbone allows us to interrogate it using commonly known metrics from machine learning; and just like IPW adjusts a binary classifier to obtain a causal estimate, we can adjust these ML metrics to obtain a causal-inference-relevant view.\nThis post is a an effort to breakdown a larger manuscript into bite-size chunks, and will focus on what ROC curves can tell us about propensity models.\n\n\nROC curves recap\nClassifications models can be evaluated for their calibration — how well they behave as probability models — and for their discrimination — how well they separate positive from negative examples. AUC is a metric for discrimination. A more in-depth overview is slightly out of scope for this article, but I do want you to keep in mind two ways for generating ROC curves from a list of prediction scores and labels.\nFirst view is the naïve one. For each possible threshold we will calculate the true-positive and false-positive rates, plotting that point in ROC space. Note that the TPR and FPR can be affected by the weight each unit contributes to the classification, which is not necessarily 1.\nSecond view is more computationally efficient. It involves sorting the scores and traversing the list such that each positive unit moves you one step up and each negative unit moves you one step right. The size of the step is correspondingly determined by the fraction of positive and negative units, but we can weigh each unit so that the step size changes arbitrarily.\n\n\n\n\nObtaining an ROC curve from scores. On the left (a) an explicit view of threshold (taken from Dariya Sydykova). One the right (b) a computationally efficient view (taken from ML wiki)\n\n\n\nIn our case, the prediction scores are the propensity estimations (probability to be in the treatment group) and the labels are the actual treatment assignment. Moreover, controlling the ROC space through sample-weighting is the basis for the additional ROC curves to be presented."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Classification metrics for propensity models — overfit, underfit, and positivity violations",
    "text": "Classification metrics for propensity models — overfit, underfit, and positivity violations\nComing from machine learning, this can be somewhat counterintuitive, so let’s get done with it right out of the gate: good prediction performance usually suggests a bad propensity model and a bad causal model downstream. Propensity scores should not be able to discriminate well between the treatment and control units.\nIf you’re lucky, your good prediction performance is due to good-old overfit. You can use your ML knowledge to solve for that. Causal inference models are prone to all the same pitfalls in statistics, they are simply blessed with a few additional ones.\nIf you’re not lucky, your good discrimination ability may hint you have a positivity violation in your data. Positivity is an essential assumption if wanting to extrapolate outcomes across treatment groups, as in causal inference. It states that the treated should have some chance (i.e. positive probability) to be in the control group and vice versa. In other words, the groups should have some common support — in each subspace of features we should have both treated and control units, so both groups have their covariates overlap. Otherwise, how could you generalize the predicted outcome from the treated to the control if all treated units are males and all control units are females? Perfect discrimination between treated and controls suggests the groups occupy mutually exclusive regions in feature-space violating a necessary assumption for causal inference.\nConversely, bad discrimination performance is not necessarily bad. It might simply suggest the treatment groups are well mixed — an encouraging step towards the validity of a causal analysis. However, it might also be due to underfit. The response surface of treatment assignment might be a complex one to model. Therefore, you should experiment in iteratively increase the expressiveness of your model to the point you overfit just to verify it is indeed the data that is balanced and not the model that is under-specified.\nSolving for lack of overlap is possible, but out of scope for this post. Just to namedrop a few strategies: you should revise the inclusion criteria of your data, rethink your confounder selection, stratify your analysis on highly predictive features, or use domain knowledge to thoughtfully help you extrapolate through mechanism rather than data.\n\nROC curves for propensity models\nFocusing on propensity-based causal models, we have three relevant ROC curves: the regular one based on propensity scores and two novel curves created by reweighting the scores. They all work in tandem, and I’ll present each one: how to obtain them and how to interpret them.\n\nVanilla ROC curve\nHow: This is the regular ROC curve simply obtained by taking the propensity scores against the binary treatment assignment.\nroc_auc_score(t, p_hat)\nInterpretation: We already discussed the issue that the AUC should not be too high as it suggests good discrimination, which is bad for causal inference. The ROC curve allows us to detect such regions of perfect discrimination. Ideally, there should not be long vertical or horizontal segments in the curve. A sharp long vertical contour suggests there’s a bunch of data points for which we only get true positives (upward movement) without paying any false negatives (rightward movement). That is, the treated units are very separable from the untreated — they are not well-mixed. Reiterating the above: this can hint that we have a positivity violation in the feature subspace that is mapped into this region of scores (thresholds) causing the vertical line.\n\n\nIP-weighted ROC curve\nHow: in this curve we weight the contribution of each unit’s propensity score to the ROC curve by the corresponding inverse-probability weight of that unit.\nip_weights = (t / p_hat) + ((1 - t)) / ((1 - p_hat))\nroc_auc_score(t, p_hat, sample_weight=ip_weights)\nInterpretation: Ideally, like every post-weight discrimination metric, we should expect a random-like performance. Namely, a ROC curve that aligns with the diagonal and an AUC around 0.5.\nIntuition: This curve shows how well the weights balance the groups. IPW creates a pseudo-population in which the treated and control have similar characteristics — it weighs the sample so that in each region in the feature-space we should have similar amount of (weighted) units. If we were to apply a classifier in this weighted population, it would be difficult to discriminate the treated from the controls. For example, if we have the same amount of males and females we can’t use sex as a predictive feature, and if we have the same amount of young and adults we can’t use age, etc. Therefore, poor discrimination post-weighting is welcomed.\n\n\nExpected ROC curve\nHow: We obtain this curve by weighing the scores such that each unit contributes its propensity score to the positive label (treatment group) and its complementary score (1 minus propensity) to the negative label (control group)\nweights = concatenate([p_hat, 1 - p_hat])\nt = concatenate([ones_like(p_hat), zeros_like(p_hat)])\np_hat = concatenate([p_hat, p_hat])\nroc_auc_score(t, p_hat, sample_weight=weights)\nInterpretation: Ideally, we would want the expected propensity to align with the vanilla (unweighted) propensity curve (and have same AUC).\nIntuition: The propensity-to-be-treated is never observed, we only see one instantiation of it in the form of treatment assignment. However, we can model the average propensity of units with similar features. If we assume the statistical model represents the true propensity, then we move from a binary classification task to a smoother calibration-like task where units with high confidence (extreme propensity) contribute almost like they would in the vanilla ROC curve, and low-confidence units (propensity around 0.5) contribute a segment parallel to the diagonal.\n\n\n\nA view of the propensity ROC curves. Blue: the unweighted propensity score. Orange: the inverse-probability weighted curve of the propensity. Green the Expected propensity curve ."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Connection to propensity distribution plots",
    "text": "Connection to propensity distribution plots\nTraditionally, practitioners will plot the propensity distribution, colored by the treatment and control groups, and look for overlap. ROC curves are another view of that propensity distribution.\nThere is a direct transformation from scores distribution to ROC curves, as seen in the figure below taken from Janssens and Martens.\n\n\n\n\nTransforming a distribution of (propensity) scores into an ROC curve [figure from Janssens and Martens].\n\n\n\nAnd the gif below from Dariya Sydykova show how separability of scores affect how sharp the curves are.\n\n\n\n\nThe effect of separability of (propensity) scores on the sharpness (i.e. amount of long vertical/horizontal segments) of the ROC curve [figure by Dariya Sydykova].\n\n\n\nFollowing this perspective, the propensity histogram weighted by the inverse propensity serves the same purpose. The bar heights are no longer determined by the number of individuals in each bin, but by their accumulated weights. In the weighted scheme (right), the bars corresponding to the same propensity bucket (i.e. x-axis bin) have the same height in the treatment and control groups, relative to the unweighted version (left) in which the heights of the same bins differ.\n\n\n\nInverse-probability-weighted propensity histogram (right) has corresponding bars slightly more similar in height then the regular (unweighted) propensity histogram (left).\n\n\nHowever, I would argue that viewing this in ROC space provides an easier interpretation, since we can convert the fuzzy notion of “distribution overlap” to a concrete AUC score."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#summary",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#summary",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Summary",
    "text": "Summary\nWe have seen how to interpret pre-weighting classification metrics (good performance is bad) and post-weighting classification metrics (bad performance is good).\nI focused on ROC curves for propensity models, presented two novel curves and discussed how to interpret them. \nHere are three take-aways for three curves:\n\nRegular ROC curves should not have sharp, long vertical segments.\nInverse-probability weighted AUC should be around 0.5.\nExpected AUC should be close to the regular AUC.\n\nThese presents an off-the-shelf intuitive measure to verify a causal model is not omitting complete nonsense. Using such simple AUC-based criteria can be implemented to automatically select causal inference models that perform better than others through cross-validation, similar to how we apply model selection in machine learning.\nI believe that deploying a propensity model and examining its behavior is beneficial in any causal inference analysis. Even if you end up modeling the response surface directly without using the propensity scores, it can still provide meaningful insights into the structure of the data and the assumption needed for a valid causal conclusion.\nFor additional thoughts and evaluations, see our preprint: https://arxiv.org/abs/1906.00442."
  },
  {
    "objectID": "blog/2020/01-simpsons-paradox-ipw/index.html",
    "href": "blog/2020/01-simpsons-paradox-ipw/index.html",
    "title": "Solving Simpson’s Paradox with Inverse Probability Weighting",
    "section": "",
    "text": "Originally published on  Medium.\n\nStatisticians love using the word “paradox” to describe simply unintuitive results, regardless of how much it upsets their fellow logicians. To get back at them, we’ll apply causality to solve one of their most famous paradoxes — Simpson’s Paradox.\nIn this post, I will briefly introduce what IPW is in the context of causal inference and present a simple intuition to how it works. I will then provide a popular example of the paradox from the medical domain and we’ll see, visually, how IPW solves it.\nWant to skip the details? Scroll to the end of the article.\n\nCausal Inference with IPW\nIPW, short for Inverse Probability (sometimes Propensity) Weighting, is a popular method for estimating causal effects from data. It’s a simple yet powerful tool to eliminate confounding or selection bias. To keep it casual (ha!), let’s introduce it via a simple hypothetical example of estimating the causal effect of drug \\(D\\) on the risk of stroke.\nFor that, we’ll make a quick detour through randomized control trials.\n\nRandomized Control Trials\nUsually, to estimate the causal effect of a drug, we would construct a Randomized Control Trial (RCT). This means, we would recruit people and flip a coin to assign them either to take drug \\(D\\) or to take placebo (control). Then we would measure the rate of stroke in each group, compare them, and conclude whether \\(D\\) increased or decreased the risk of illness.\nWe know that there are multiple contributing factors to stroke. For example, sex (vascular systems can be differentiated between males and females) and age (veins tend to clog with time). The reason we could simply compare the two groups and disregard those other factors has everything to do with the randomization we applied.\nThe randomization creates a similar distribution of age and sex between the two groups (on average). Consequently, when comparing the groups, the contributions of those variables cancel themselves out. The only parameter consistently different between the groups is whether they took \\(D\\) or placebo, and therefore, the differences we observed in the risk can be contributed only to \\(D\\), making them the causal effect of \\(D\\).\nWe can decompose the risk of stroke in our simple example into a slightly more concise mathematical notation:\n\\[\n\\begin{array}{c}\n\\text{risk stroke in treated}=\\text{risk due to age} + \\text{risk due to sex} + \\text{risk due to }D \\\\\n- \\\\\n\\text{risk stroke in control}=\\text{risk due to age} + \\text{risk due to sex} + \\underbrace{\\text{risk due to placebo}}_{=0} \\\\\n= \\\\\n\\text{risk due to }D\n\\end{array}\n\\]\nWe compare the risk between between our groups by taking the difference.\nSince age and sex are distributed similarly between groups, they contribute the same risk in both groups and so they cancel themselves out. Since placebo is a small sugar pill, its contribution is zero. Hence, we are left only with \\(\\text{risk due to }D\\).\nVoilà! The causal effect of \\(D\\) on stroke.\n\n\nCausal Effect from Non-Experimental Data\nHowever, performing a randomized control trial costs money and takes a long time (among other disadvantages). Still paying our student-loan, we don’t have the resources to conduct such an experiment. What we do have is data, because data is becoming ever cheaper and HMOs collect them easily.\nThe problem with such observational data from HMOs is that it no longer comes from our nice experimental distribution. Unfortunately for us (but really luckily for us), physicians are not random. They assign treatments based on our characteristics (say, age and sex). Therefore, there might be an overall tendency to prescribe certain groups with one drug and not the other. In this case, if we were to simply compare those who did take D with those who did not, the distribution of those factors will not necessarily be the same and so their contribution will no longer cancel out.\nConsequently, the “effect” we’ll observe will no longer be the causal effect of D, but rather a quantity entangling both causal and non-causal impacts, essentially contaminating the causal effect of D with the contribution of those other factors.\nTo estimate the true causal effect, we’ll first need to make the two groups comparable, and the way to make them comparable is where causal inference, and specifically IPW, comes into play.\n\n\nInverse Probability/Propensity Weighting\nNow that we have set the scene, we can finally present what IPW is. As we said, IPW stands for Inverse Propensity Weighting. It’s a method to balance groups by giving each data-point a weight, so that the weighted-distribution of features in first group is similar to the weighted-distribution of the second one.\nWe mentioned that physicians don’t prescribe drugs randomly, but rather base it on the features of their patients. Therefore, each patient will have a different likelihood to be prescribed to D, based on their characteristics. This likelihood is referred to the propensity to be treated. To put it mathematically, if we mark our patient features as X, the propensity is the probability of patients getting or not getting the treatment: \\(\\Pr[D|X]\\). Once we estimated this probability to treat, the weight we assign is simply its inverse: \\(1 / \\Pr[D|X]\\).\nWhen we have a large number of features, we will need some machine learning model to crunch all those high dimensional data into one probability scalar. But in order to see why this process even results in a balanced population, let’s take the simple example with one feature, say being an adult male.\n\nIPW by a Simple Example\n\n\n\nOur original population is unbalanced because we have more untreated adult males than treated ones. If we were to compare the groups, we wouldn’t be able disentangle the contributions of drug and sex, and tell whether the observed effect is due to being treated or due to being male.\n\n\nExamining the distribution in the above figure, we see that our groups are imbalanced with regard to males. Therefore, if we were to simply calculate an average risk in each group, we would not be able to say whether the difference we see is due to being treated or simply because of being a male.\nOur first step is to calculate the probability of each individual to be in the group they are actually assigned to. We have 5 men in total, 1 treated and 4 that are not. Hence, we can make a simple estimate that the probability for males to get the drug is ⅕ and the probability for males to not get the drug is ⅘.\nOur second step is to inverse those probabilities and assign them to each individual. Homer, therefore, getting a weight of 5, while Moe, Barney, Groundskeeper Willie, and Principal Skinner each get a weight of 5/4. We basically create a pseudo population where we have 5 Homers, and we have 5/4 of each of the untreated — meaning we have one Moe, one Barney, one Willie, and one Skinner, and another hybrid-person which is ¼ of each. Making that a total of 5 treated and 5 controls.\n\n\n\nOur pseudo-population includes a similar number of adult males (note the duplicated Homer and the hybrid untreated), so when comparing the groups - the effect of adult-males will cancel it-self and we’ll be left only with the effect of the treatment.\n\n\nSee, we were able to create a population in which males are evenly distributed between groups.\nIn real life, of course, there will be more than one feature to handle, and that’s why we’ll need a model to estimate \\(\\Pr[D|X]\\).\nTL;DR\n\n\n\nIPW takes an unbalanced population and creates a balanced pseudo-population (Simpsons components from Wikipedia).\n\n\n\n\n\n\nSimpson’s Paradox\nBy now you might have a hunch how we can use IPW to solve Simpson’s paradox, but before we do, let’s briefly introduce what this paradox is all about.\nSimpson’s Paradox, a term coined by Blyte¹, is named after Edward Simpson, the first statistician to explicitly point to this problem. In short, the paradox happens when an overall average trend in the population is reversed or canceled-out when examining its composing sub-groups.\nThe intuition in the continuous case is very clear, as suggested by this GIF:\n\n\n\nA visual intuition on how a trend in the overall population can reverse itself in the composing sub-populations.\n\n\nTo get a better understanding of the phenomena, let’s examine a real-world example:\nAccording to the paper, we have two ways to treat kidney-stones. Either with an open-surgery (A) or with a new non-invasive method using shock-waves (B). We gather medical records from 700 people, 350 from each treatment-group and compare their success rates.\nWishing to conclude which method is better, we compare the success rates among group A and B and see they are 78% and 82.5%, respectively. Naively, we want to deduce B is better (by 4.5%), but we remember we read somewhere (where?) that physicians are not random. We suspect there’s probably some reason as to why patients got a certain treatment but not the other, and that it probably has to do with their prior medical condition.\nLo and behold, when we split the patients based on stone size — we see a different trend. Suddenly it is treatment A that is better for both small stones and large stones — 93.1 and 73% respectively.\nThe averaging process probably masks some vital information, so let’s look at the raw numbers.\nTreatment A has 81/87 (93.1%) success rate for small stones and 192/263 (73%) for large stones. Meanwhile, treatment B has 234/270 (86.67%) success rate for small stones and 55/80 (68.75%) for large stones.\n\nB has better success rate overall, but A is better in both small and large kidney stones. This happens because B got the majority of easy cases (small stones), while A got the majority of hard cases.\n\n\n\n\n\n\n\nStone size \\ Treatment\nA\nB\n\n\n\n\nBoth sizes\n273/350 =\n78.0%\n289/350 =\n82.5%\n\n\nSmall\n81/87 =\n93.1%\n234/270 =\n86.67%\n\n\nLarge\n192/263 =\n73.0%\n55/80 =\n68.75%\n\n\n\nDo you see what the denominator is hiding? The game is rigged. B got the majority of easy cases (small stones), while A got the majority of hard cases. Severity of the patients is not similarly distributed across treatment groups. Since larger stones also have a lower chance of success — because they are more difficult to handle, we find ourselves in a biased situation where comparison of treatments is not fair. This is common in day-to-day scenarios, because when physicians encounter tougher cases (larger stones), they will tend to use bigger guns (open surgery).\nGenerally speaking, a variable that affects both the treatment assignment and the outcome is called a confounding variable. In our case, the severity of patients, expressed by the size of their kidney stones, is a confounder since it increases both the likelihood to be assigned to a harsher medical procedure (a surgery more likely to be effective), and increases the risk of failing (stones not being completely removed).\nIn our final act of this post, we’ll fix this discrepancy in features (severity through stone sizes) between the treated and controls using IPW.\n\n\nUsing IPW to Solve Simpson’s Paradox\nWe have an imbalanced situation whereas the probability to get treatment A if you have small stones is 87/(87+270)=24.4% and if you have large stones it’s 263/(263+80)=76.67%. Similarly, for treatment B it’s 270/(270+87)=75.6% and 80/(80+263)=23.33%.\nTherefore, we can easily calculate the weight of each individual by taking the inverted fraction of people from each group (since we look only at binary stone size, our individuals are basically identical within their groups, so we can use group-level weights). To compute an unbiased success risk, we simply calculate the average of the success risk weighted by these weights:\n\\[\n\\begin{array}{c}\n\\text{For treatment } A: \\left( \\frac{357}{87} \\cdot \\frac{81}{87} + \\frac{343}{263} \\cdot \\frac{192}{263} \\right) /  \\left(\\frac{357}{87} + \\frac{343}{263} \\right) = 88.26\\% \\\\\n\\text{For treatment } B: \\left( \\frac{357}{270} \\cdot \\frac{234}{270} + \\frac{343}{80} \\cdot \\frac{55}{80} \\right) /  \\left(\\frac{357}{270} + \\frac{343}{80} \\right) = 72.97\\%\n\\end{array}\n\\]\nAnd we see that now, even if we aggregate the size of stones, treatment A is better than B (by ~15%). This is consistent with A also being better in every sub-group, and so the reversal we saw before no longer exists.\nWe have solved Simpson's Paradox.\nTL;DR \nin case you haven't read a word, here's the entire post summed up as a GIF:\n\n\n\nBreaking down Simpson’s Paradox visually and solving it by creating an average risk weighted by IP weights. Visualization, by the author, is based on a similar one by John Burn-Murdoch I remember seeing on Twitter"
  },
  {
    "objectID": "blog/2018/01-deep-learning-genetic-prediction/index.html",
    "href": "blog/2018/01-deep-learning-genetic-prediction/index.html",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2018/01-deep-learning-genetic-prediction/index.html#footnotes",
    "href": "blog/2018/01-deep-learning-genetic-prediction/index.html#footnotes",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nstill looking at this cute little ImageNet feature space.↩︎\nWhat is science if not baby-stepping all the way to the moon.↩︎\nP-value-based variable selection strategies are bad, see Sun, Shook, and Kay.↩︎\n*It may be interesting to evaluate the model’s performance when different chromosomes will share some of the deeper convolutions. Namely, replacing the DNN box in the next figure with another CNN.↩︎"
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html",
    "href": "blog/2020/02-against-agile-research/index.html",
    "title": "The case against Agile research",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#agile-methodologies",
    "href": "blog/2020/02-against-agile-research/index.html#agile-methodologies",
    "title": "The case against Agile research",
    "section": "Agile methodologies",
    "text": "Agile methodologies\nAgile methodologies are project management approaches becoming increasingly popular in software development. At its core, Agile software development is iterative and incremental. It promises frequent product delivery that can accommodate for the fast pace demand for changes from users. The main idea is to break down the software into independent self-contained components and implement them cyclically, ever improving the product.\nThis bottom-up approach has stemmed as a countermeasure to the more traditional top-down approach, known as Waterfall. In the Waterfall method, the entire system's architecture is first designed, down to the smallest components, and then coded from start to end. This has made software much slower to improve, which is the main reason why the method's popularity is decreasing in our fast-pace computing world."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#research-and-data-science-pipeline",
    "href": "blog/2020/02-against-agile-research/index.html#research-and-data-science-pipeline",
    "title": "The case against Agile research",
    "section": "Research and data science pipeline",
    "text": "Research and data science pipeline\nAgile has become so popular that it had percolated outside the realm of traditional software development. Specifically, it had trickled down to research, and more specifically, to data-driven research. The heavier the computational infrastructure of the research, the more likely it is to adapt Agile methodology.\nIn the data-science version of Agile, results are iteratively refined. For example, data definitions are constantly refined, models are iteratively tuned, and reports are continually generated.\nFrom my personal experience, this is especially the case in industry research. As opposed to academia, companies usually subject you to a greater corporate hierarchy. You have several lines of management, each requires periodically updates on the project, usually in the form of a report or a presentation. This demand requires you to set an initial pipeline quickly, just to obtain some results, and then iteratively improve it (refine the data, add analyses, prettify graphs, etc.) until the next meeting or report.\nThis article would like to argue that iteratively reporting should be avoided because it can creep in unconscious bias."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#unconscious-bias",
    "href": "blog/2020/02-against-agile-research/index.html#unconscious-bias",
    "title": "The case against Agile research",
    "section": "Unconscious bias",
    "text": "Unconscious bias\nUnconscious (or implicit) bias is a term used to depict one's preference towards prior beliefs on the expense of evidence at hand. It has been affecting science since ever and is the reason why researchers nowadays prefer methods such as blind peer reviews or randomized control experiments. In the former, reviewers aren't affected by the identity of the author, and in the latter, participants don't know whether they get treatment or placebo. Both neutralize a psychological effect that can bias one's response in the process.\nScientists are no exception and subconscious bias can also affect the research protocol and analysis itself. Even in presumably \"pure\" tasks, like measuring physical constants, it has been observed that new measurements tend to cluster around previous measurements, rather than what we nowadays know to be the true value.\nWe can mitigate such unconscious bias by adapting the same trickery we use for blind peer reviews and blind medical trials: blind analysis.\nIn his excellent book Statistics Done Wrong, Alex Reinhart gives a beautiful example of how blind analysis was applied in experimental physics: when Frank Dunnington wanted to measure the ratio between an electron's charge to its mass, it required him to build a dedicated machinery. Dunnington built the entire apparatus but left the detector in a slightly off angle. As the exact measurements are worthless, Dunnington could construct the entire experimental protocol without being exposed to the value he was interested in. Thus avoiding overfitting the protocol to his prior beliefs. Only upon finalizing the analysis plan, had he corrected the off-angled detector and correctly measured the desired ratio."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#blind-statistical-analysis",
    "href": "blog/2020/02-against-agile-research/index.html#blind-statistical-analysis",
    "title": "The case against Agile research",
    "section": "Blind statistical analysis",
    "text": "Blind statistical analysis\nTo be honest, there's no straightforward equivalent to a dislocated detector in the data science pipeline. One might use mock data, or a very (very) small random subset of the data they can later discard. The data will be used just to verify the code runs end to end, outputs are correctly saved, errors are correctly logged, graphs look as intended, etc. Only when the pipeline is finalized will one run their code on the entirety of the data, generating results.\nTo quote Alex: \"no analysis plan survives contact with the data\", and I will add that technical issues should be debugged and solved as data blind as possible."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#avoiding-agile-research",
    "href": "blog/2020/02-against-agile-research/index.html#avoiding-agile-research",
    "title": "The case against Agile research",
    "section": "Avoiding Agile research",
    "text": "Avoiding Agile research\nIn research, our product is usually some kind of a report. Therefore, in Agile-like research, we continuously generate periodic reports and therefore constantly re-estimating our estimand of interest. The more rounds we go, the greater the chances to be affected by the ever-lurking subconscious bias toward desired results.\nFurthermore, changing analysis plan as we go, especially when based on half-baked analyses, can wildly increase false positive discoveries. For example, if we decide to change the primary outcome we measure in the middle of the project, just because we saw (or any management reading your monthly report suggested) there's no effect, it will direct us into analyzing our data, rather than analyzing the phenomena the data measure.\nData are like a perishable resource. Each time you glance at it, the quality of the story it tells deteriorates. Soon you'll be reading whatever you want to read, rather than what the data wants to tell.\nSimilarly, Results are just a set of transformations applied to the data and glancing at them is like glancing at the data via proxy. Namely, that too drains out our data resource, making it less reliable."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#summary",
    "href": "blog/2020/02-against-agile-research/index.html#summary",
    "title": "The case against Agile research",
    "section": "Summary",
    "text": "Summary\nAs short-cycle iterative management methodology become increasingly popular in research, so does our susceptibility to subconscious bias. This is not to say Agile-ish methodologies are never to be used in research or data science, only that they require some adaptations. Adaptation like more careful data management, avoiding peeping at the results just like we avoid peeking at the data, and pre-registering the analysis plan.\nEventually, the scientists themselves also adapt to the evidence they work with, just like any other statistical model adapts to the data. Therefore, scientists should limit their own access to data and results, just as they do to the models they apply, to avoid overfitting their own analysis to their desires or prior beliefs"
  },
  {
    "objectID": "blog/2021/01-visualizing-micro-macro-averages/index.html",
    "href": "blog/2021/01-visualizing-micro-macro-averages/index.html",
    "title": "A visual way to think of macro and micro averages in classification metrics",
    "section": "",
    "text": "For completeness, this article spends most its words explaining what a confusion matrix is. If you already familiar, you can probably understand the article by just scrolling down through the pretty pictures.\n\nConfusion matrix\nAlmost every classification metric can be defined by a confusion matrix. Those that cannot — can be defined by several. This makes the confusion matrix the most basic way to evaluate classification models.\nEvaluating a binary classification, we can either be right or wrong. But we can cross-tabulate the four possible combinations of predicted labels and true labels into a contingency table. On the diagonal, we count the correct predictions, and off the diagonal we count our mistakes.\nFirst, we can be correct on the positive label, these predictions are truly positive (TP). Second, we can be correct on the negative label, making these predictions truly negative (TN). But correct predictions are all alike; every wrong prediction is wrong in its own way. The first type of error is type-1 error — positive predictions that actually belong to the negative class — these are falsely positive predictions. Lastly, the fourth combination and the second type of error, we can wrongly predict the positive label to be negative — these predictions are falsely negative.\n\n\n\nConfusion matrix is a contingency table enumerating the combinations of predictions and true labels. It defines four important building blocks for most classification metrics: TPs, FPs, FNs, TNs.\n\n\nThis four-cell matrix can define a plethora of metrics. For example, accuracy can be defined by summing the TP and TN and dividing by the sum of the matrix; sensitivity is the TP divided by actual positive observations; specificity is similar for the negatives: dividing TN by the actual negative observations; and there are much, much more. Especially when we can also use the metrics themselves to create compound metrics, the sky is the limit.\n\n\nMulticlass confusion matrix\nConfusion matrices can be naturally expended into multi-class classification. Instead of a 2-by-2 table, we'll have a k-by-k table enumerating the different combinations of predictions and labels. However, the metrics in this multi-class setting are not always well defined. This is because we no longer have a single false positive or false negative count, but rather several ones — one for each pair of misclassified classes.\nTo redefine these metrics for multiple classes, we must first convert our single k-by-k table to k 2-by-2 tables. We simply aggregate the multiple false-positives, false negatives, and true negatives into one of each. This is the same as thinking of our multiclass classifier as multiple one-vs.-rest binary classifiers.\n\n\n\nMulticlass confusion matrix can be reformulated as multiple one-vs-rest 2-by-2 matrices. This will help us redefine what false positives and false negatives are in a multiple\n\n\nWe now have a bunch of 2-by-2 confusion matrices, let's stack them up. Think of it as a volume, or a 3D tensor. In each simple confusion matrix, the metrics — like precision, specificity, or recall — are well defined. However, we need to extract a single metric from them. This is when averaging — micro and macro — come into play.\n\n\n\nWe’ll benefit from thinking of these multiple one-vs-rest confusion matrices as a 3D stack volume of confusion matrices. Then macro- and micro-averages are just the order of axes reduced.\n\n\n\n\nMacro-averaging\nIn macro-averaging, we first reduce each of the k confusion matrices into a desired metric, and then average out the k scores into a single score.\n\n\n\nIn macro-average, we first calculate a metric from each confusion matrix, and then average out the scores of these metrics.\n\n\n\n\nMicro-averaging\nIn micro-averaging, we will first sum the TPs, TNs, FPs, and FNs across the different confusion matrices, say we denote them as ΣTP, ΣTN, ΣFP, and ΣFN. We then use these aggregated measures as if they form a confusion matrix of their own and use them to calculate the desired metrics.\n\n\n\nIn micro-average, we first reduce the confusion matrices into one summed up confusion matrix, and then calculate the metric from aggregated table.\n\n\n\n\nSummary\nMulticlass confusion matrices can be expanded to stacked one-vs.-rest confusion matrix. Under this formulation, micro- and macro-averages differ by which axis is reduced first. Macro-average first reduces the confusion matrices into scores and then averages the scores. Micro-average first reduces the multiple confusion matrices into a single confusion matrix, and then calculates the score."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html",
    "title": "Causal Inference with Continuous Treatments",
    "section": "",
    "text": "Causal inference, the science of estimating causal effects from non-randomized observational data, is usually presented using binary treatment; we either treat or don’t treat; we give drug A or drug B. There’s a good reason for that, as causality is already complex as it is. However, not all interventions are binary (or discrete).\nSometimes, the interventions we care about are continuous. “Taking a drug”, for example, is fairly vague — drugs have active ingredients and those can come in different dosages. Too little and the drug might not seem effective, too much and the drug might be harmful. Therefore, we might be interested in the effect of different dosages of the same drug. This is often called dose-response modeling.\nContinuous exposures are all around us. From drug dosages to number of daily cigarettes smoked or air pollution levels, from how much time you watched an ad before skipping it to how much red a the “unsubscribe” button is on a newsletter, from the interest rate increased by the central bank to the amount of money in a lottery winning. We can’t limit ourselves to studying binary exposures just because the introduction book didn’t cover the other ones.\nIn this post I will introduce a generalized version of inverse probability weighting for continuous treatment. I’ll show different estimation methods and discuss the required assumptions and its limitations. I will assume you are familiar with causal inference and IPW for binary treatment, but if you are not — I got you covered in this IPW explainer."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "title": "Causal Inference with Continuous Treatments",
    "section": "From binary to continuous treatment",
    "text": "From binary to continuous treatment\nRecall that in the binary treatment setting, a common way to estimate causal effects is by using inverse probability weighting (sometimes called inverse propensity weighting, but I’ll just use IPW). Given individual \\(i\\) with treatment assignment \\(a_i\\) and characteristics \\(x_i\\), its inverse propensity weight is defined as: \\(w_i=1/\\Pr[A=a_i|X=x_i]\\). Namely, the inverse probability of \\(i\\) to be assigned to their treatment, given their characteristics.\nHowever, when treatment (or any random variable for that matter) is continuous, the notion probability mass fails and we need to speak in terms of probability density. This is because the probability of a single point, say \\(a_i\\), is basically 0, while it may still have density associated with it since density is defined as the derivative of the cumulative probability function. This is a fundamental theoretical difference, but we can capture it in a small notation change, instead of \\(\\Pr \\left[A=a_i|X=x_i \\right]\\) we will use \\(f \\left(a_i|x_i \\right)\\).\n\n\n\nGradually approximating a discrete binomial distribution with a continuous Gaussian one."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#modelling",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#modelling",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Modelling",
    "text": "Modelling\nRecall that estimating the treatment effect with IPW is comprised of two main steps. First, model the treatment and obtain IP-weights. Second, model the outcome using those weights. In the binary case, once we have the weights, the simplest way to estimate the potential outcomes is to simply take the weighted average in the treated and untreated (often called the Horvitz-Thompson estimator). However, an equivalent way is to use a simple univariable regression: regress the outcome against the treatment (and an intercept) weighted by the IP-weights. Then, the average treatment effect is simply defined by the coefficient corresponding to the treatment variable. This is often called a marginal structural model in the epidemiology literature.\nNote that in the continuous treatment case, the first option is not applicable. Often, there will be many unique treatment values and it will be rare to have enough samples with the exact same continuous treatment value, for all treatment values. Binning them will solve it, but we’re here for continuous treatment modeling. Therefore, we will need to use the latter option and create an additional (parametric) model between the outcome and the treatment. This will be our dose-response function.\nLet’s examine those two steps in more details.\n\nStep 1: modeling the treatment\nWith categorical treatments, we needed to model the probability of getting treated. We could have done that by regressing the treatment assignment against the covariates, basically using any “classifier” that outputs predictions in the 0–1 interval which we can then interpret as probabilities. Logistic regression, for example is a generalized linear model that is defined by the binomial distribution — a discrete probability function. With continuous treatment, however, we will need a regression model instead. For example, in generalized linear models, a linear regression model is defined by the Gaussian distribution. And as the animation shows above — the more categories a binomial distribution has the better it is approximated by a normal distribution.\nOnce we fitted a model, we can obtain the conditional expectation \\(E \\left[A|X \\right]\\). But unlike the binomial case, in the continuous case, this is not sufficient to generate densities. For simplicity, let’s assume the common Gaussian distribution, which is parameterized by a mean and variance. The conditional mean of that distribution will be the estimated conditional expectations (the predictions); the variance will be constant and will be set to be the variance of the residuals between the treatment and the predictions. Once we defined the distribution, we take the density of the observed treatment values with respect to this distribution. The generalized IP-weights are the inverse of these densities.\nTo summarize step 1:\n\nFit a function \\(g(x)\\), regressing the treatment \\(A\\) on covariates \\(X\\).\n\\(A=g(X)+\\epsilon=\\alpha_0+\\alpha_1 X+\\epsilon\\)\nDefine the conditional distribution \\(D_i \\sim \\text{Normal}(g(x_i), \\text{Var} \\left(a_i-g(x_i) \\right)\\).\n\nThe conditional mean of each sample is its prediction.\nThe variance is fixed and is the variance of the prediction residuals.\n\nDefine the density \\(d_i\\) as the value of \\(a_i\\) from \\(D_i\\).\nDefine the weight \\(w_i\\) to be the inverse of the density: \\(1/d_i\\) .\n\n\n\nStep 2: modeling the outcome\nOnce we obtained the balancing weights w, we can model the counterfactual outcomes using the observed outcomes and treatments. To do that, we regress the outcome against the treatment, weighted by the IP-weights obtained from step 1. However, unlike the binary treatment case, the functional form of the continuous treatment should be flexible enough to avoid bias due to misspecification. For example, we will add a quadratic term of the treatment or model it using a spline, etc.\nWhen we have non-linear transformations of the main treatment variable, we can no longer interpret the treatment effect as the coefficient of the treatment covariate. Instead, to make counterfactual outcome predictions, we will set some treatment value and run it through our model to get the predicted outcome, and average it out across the units to obtain the average outcome had everyone been assigned that specific treatment value.\nWe can repeat that for two different treatment values. Then the causal effect will be the difference (or ratio) between these two potential outcome predictions. Alternatively, we can repeat that for every treatment value in a range we care about and obtain a dose-response curve — see how the counterfactual outcome prediction changes as a function of assigning different dosages.\n\n\n\n\nMarginal Structural Model - regress the outcome on the treatment weighted by the generalized IP-weights. As proposed by Robins, Hernan, and Brumback1.\n\n\n\n\nCode\nBelow is a Python code demonstrating the estimation process described above.\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom causallib.datasets import load_nhefs\n\n\ndef conditional_densities(data, use_confounders=True):\n    formula = \"smkintensity82_71 ~ 1\"\n    if use_confounders:\n        formula += \"\"\" + \n        C(sex) + C(race) + age + I(age**2) + \n        smokeintensity + I(smokeintensity**2) +\n        smokeyrs + I(smokeyrs**2) + \n        C(exercise) + C(active) + C(education) + \n        wt71  + I(wt71**2)\n        \"\"\"\n    model = sm.formula.ols(formula, data=data).fit()\n    density = stats.norm(\n        loc=model.fittedvalues,\n        scale=model.resid.std(),\n    )\n    densities = density.pdf(data[\"smkintensity82_71\"])\n    densities = pd.Series(densities, index=model.fittedvalues.index)\n    return densities\n\n\ndata = load_nhefs(raw=True)[0]\ndata = data.loc[data[\"smokeintensity\"] &lt;= 25]  # Above 25 intensity is sparser\n\ndenominator = conditional_densities(data, use_confounders=True)\nnumerator = conditional_densities(data, use_confounders=False)\ngeneralized_ipw = numerator / denominator\n\nmsm = sm.formula.wls(  # Using GEE instead will lead to better (more conservative) uncertainty intervals\n    \"wt82_71 ~ 1 + smkintensity82_71 + I(smkintensity82_71**2)\",\n    data=data,\n    weights=generalized_ipw,\n).fit()\n\ndosage = list(range(26))\ndosage = pd.DataFrame(\n    data={\"smkintensity82_71\": dosage, \"I(smkintensity82_71**2)\": dosage},\n    index=dosage,\n)\nresponse = msm.predict(dosage)\nax = response.plot(\n    kind=\"line\",\n    xlabel=\"Increase in cigarettes per day\",\n    ylabel=\"Change in weight after 10 years [kg]\",\n    title=\"Smoking more cigarettes led to smaller weight increase\"\n)\n\n# Example adjusted from Hernan and Robins' What If book\n\n\n\nThe dose response curve resulting from the snipped code above.\n\n\n\n\nExtensions\nThe above describes on simple flavor of estimation. It could, however, be extended in multiple parts. Below are a few such extensions. Feel free to skip if you had enough.\n\nStabilized weights\nIn IPW for binary treatment, we commonly calculate the weights as 1 over the probabilities. This results in pseudo population twice the size of our sample — since the weighting result in each treatment group being the size of our original sample.\nStabilized weights are a version in which the numerator is not 1, but the treatment prevalence (average of binary treatment). This shrinks the weights so that the overall pseudo-population size is the size of the original sample, not twice the size.\nThis stabilization is also applicable to the continuous treatment setting. Instead of setting the numerator to be 1, we can take the numerator to be the density of treatment values under the average treatment value (or, more generally, the prediction of an intercept-only model. Under this formulation we can also stabilize on effect modifiers, but this is for a different post). The code above shows a stabilized version that is more recommended.\n\n\nReplacing weighted regression with a clever covariate\nIn the second step, when modeling the outcomes based on the treatments, we incorporated the generalized propensity scores as weights in a weighted regression. This is usually referred to as marginal structural models as described by Robins, Hernan, and Brumback. However, similar to the different flavors of TMLE, we can also incorporate the generalized propensity scores as an additional covariate in the second-step outcome regression, rather than as weights. This, in fact, what Hirano and Imbens suggested.\nIn this version, we add the densities (not their inverse) as an additional feature. However, since it is another continuous measure, prone to misspecification, we will add it flexibly. Usually by also adding a squared term and an interaction with the treatment variable (or a spline).\n\n\n\nOutcome model with the generalized IP-weights as a predictor. As proposed by Hirano and Imbens.\n\n\nOne small but important detail to note is that during prediction, when we set the treatment value of interest for all individuals, we will now first need to calculate the density for that specific value and then insert these densities as the predictors to the outcome model we apply.\n\n\nHeteroskedastic density and other distributions\nIn item (2) of step 1, we estimated the density with a fixed variance for all individuals. This assumption, called homoskedasticity, is reasonable (and can be empirically tested by examining the residuals) but can be relaxed. Similar to how the mean of the density function was conditioned on covariates (i.e., a prediction), the variance can also be a function that changes with covariates. Or in other ways like density-weighted regression.\nAdditionally, we could parameterize the density function using other distributions, like t-distribution, truncated normal, etc. Alternatively, it can further be de-parametrized by using kernel density estimation, but there ain’t no such thing as free lunch — and this will require much denser data for a reliable estimation.\n\n\n\nPositivity assumptions under continuous treatment\nSo far, we have discussed how to obtain statistical associations between treatment and outcome. However, to convert them into causal claims, we will need to make additional assumptions. These assumptions are necessary no matter how sophisticated the statistical estimation is. It is up to us to apply additional logic on top of it to justify these associational differences are indeed causal effects.\n\nCausation = Association + Logic\n\nRecall that we have three main assumptions: consistency, exchangeability, and positivity. Consistency is an assumption on the treatment mechanism and is therefore the same as in the categorical treatment setting. Exchangeability assumes there are no unmeasured confounding, and that each potential outcome is independent of the observed treatment assignment (no bias) given the covariates — this is also the same. Positivity is the assumption requiring some adjustments.\nRecall that in the categorical case, positivity assumes each unit has some chance (positive probability) to be assigned to every treatment. This means the treatment groups share a common support, and their characteristics overlap. It is formally defined as \\(\\Pr[A=a|X]&gt;0\\) for all treatment values a across the entire space covariate space X.\nBut in the continuous case, we need to replace the probability with density. However, the rest remains the same. We will require \\(f(A=a|X)&gt;0\\) for all treatment values a across the entire covariate space X. Namely, we need positive density for all available combination of treatment and covariate levels. Luckily, this can be empirically tested (like regular positivity) by examining density of different treatment values, especially those we are most interested in, under the density model we obtained (the one whose means are our regression predictions).\n\n\n\n\n\n\nPlotting the histogram of conditional densities of the smoking example to assess overlap. Here testing for two treatment values: 0 = no change in smoking intensity, and 1 = an increase of a cigarette a day. This is the equivalent of plotting the probability to treat in the binary treatment case to assess overlap between groups.\n\n\n\n\n\n\n\nOr more broadly - A ridge plot examining the conditional density distribution of all possible dosage increases. We see the further the dosages are from each other - the smaller the overlap. Therefore we might trust the effect of local changes (say, increase of 5 vs. no change) than global changes (say, increase of 25 vs. no change)."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#limitations",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#limitations",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Limitations",
    "text": "Limitations\nIncreasing the number of treatment values does not come without a cost. There are several limitations we should be aware of when modeling continuous treatment.\nFirst, theoretical assumptions are harder to conforms to. It is harder to achieve exchangeability since we now have more treatment values for which we want to achieve unconfoundedness. It is also harder to achieve positivity, for both the same reason and the fact that conditional density might be sparser due to its continuous nature.\nSecond, continuous variables are harder to model. They are more prone to misspecification. We might partially solve it by using flexible estimators (like additive trees) or flexible data transformation (like splines in GAMs), but it can come at a cost — requiring more data or introducing some bias due to bias-variance trade-off. Additionally, densities are notoriously hard to estimate, and our generalized IP-weights can be sensitive to different choices of density estimators.\nThird, often times some continuous measures are actually ordinal. Treatment on the ordinal scale might be approximated as continuous, especially when the number of categories and their ranges increase. But continuous approximation of ordinal variables might also introduce some bias due to misspecification. There are generalizations of IPW to the ordinal scale, which require ordinal regression (similar to how we required linear regression, and regular IPW requires logistic regression), but these are beyond the scope of this post. Just so you're aware of that.\nLastly, to end on a brighter note, throughout this post I had in mind a case of continuous treatment and continuous outcome. However, this is also applicable to other outcomes. Namely, the second-step outcome model can correspond to arbitrary type of the outcome. Most commonly, if we have binary outcome, we can apply a logistic regression (or any other \"classifier\")."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#conclusions",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#conclusions",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post I introduced causal inference with continuous treatments. I presented their importance, described how to model them, how to adjust the required causal assumptions, and their limitations. I hope you find it useful."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#footnotes",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#footnotes",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n↩︎"
  },
  {
    "objectID": "blog/2023/02-hello_world/index.html",
    "href": "blog/2023/02-hello_world/index.html",
    "title": "Hello Quarto!",
    "section": "",
    "text": "I made a personal website."
  },
  {
    "objectID": "blog/2023/02-hello_world/index.html#footnotes",
    "href": "blog/2023/02-hello_world/index.html#footnotes",
    "title": "Hello Quarto!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince I already mentioned Jinja above, it seems like its template language combines both pandoc’s and EJS capabilities together?↩︎"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "List of blog posts\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSequential trial design for causal inference\n\n\nAligning time-zero to reduce time-related biases by taking simplest solution possible and awkwardly repeating it until it’s great.\n\n\n\n\ncausal inference\n\n\nstudy design\n\n\n \n\n\n\n\nOct 24, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nHello Quarto!\n\n\nI made a personal website. With blogs and stuff.\n\n\n\n\n\n\nJul 15, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCausal inference is a mindset\n\n\nCausal inference from observational data is a mindset, not a set of tools.\n\n\n\n\ncausal inference\n\n\n \n\n\n\n\nJan 4, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCausal Inference with Continuous Treatments\n\n\nGeneralizing inverse probability weights for non-categorical treatments.\n\n\n\n\ncausal inference\n\n\n \n\n\n\n\nNov 2, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy we care for covariate balancing in comparative studies\n\n\nBalancing variables in statistical comparative analysis is a proxy, not a goal.\n\n\n\n\ncausal inference\n\n\n \n\n\n\n\nNov 20, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nA visual way to think of macro and micro averages in classification metrics\n\n\nExplaining what macro-average and micro-average metrics are.\n\n\n\n\nmachine learning\n\n\nvisualization\n\n\n \n\n\n\n\nSep 4, 2021\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nUsing machine learning metrics to evaluate causal inference models\n\n\nReinterpreting known machine learning evaluations from a causal inference perspective, focusing on ROC curves for propensity models.\n\n\n\n\ncausal inference\n\n\nmachine learning\n\n\n \n\n\n\n\nDec 28, 2020\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nThe case against Agile research\n\n\nEver popular iterative development approaches can sneak in unconscious-bias that can be harmful to the scientific process.\n\n\n\n\nmeta-science\n\n\n \n\n\n\n\nJul 6, 2020\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nSolving Simpson’s Paradox with Inverse Probability Weighting\n\n\nA visual intuition on how the most popular method in causal-inference works, and how it solves one of the most popular paradoxes in statistics.\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n \n\n\n\n\nFeb 2, 2020\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nApplying Deep Learning to Genetic Prediction\n\n\nWhat classical methods for obtaining polygenic (risk) scores lack, and how deep learning might help mitigated these shortcomings.\n\n\n\n\ngenetics\n\n\ndeep learning\n\n\n \n\n\n\n\nMay 5, 2018\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ehud Karavani",
    "section": "",
    "text": "Hi there!\nI’m a researcher and data scientist, specilizing in causal inference, machine learning, (Bayesian) statistics, and data visualization.\nI’m currently a Research Staff Member at IBM Research, Israel in the Causal Machine Learning for Healthcare & Life Sciences group. There, I focus on high-throughput causal inference for finding new indications for existing drugs using electornic health records and insurance claims data. I’m also the creator and maintainer of causallib, a one-stop shop open-source Python package for flexible causal inference modeling.\n\nEducation\nI have a BSc and MSc in Computer Science and Computational Biology from the Hebrew University. I did my Master’s thesis with Dr. Shai Carmi studying prediction of traits using DNA and its potential effect on selecting embryos for implantation in IVF (aka “designer babies”) .\nPrevious to that I was an undergraduate research associate in Prof. Hanah Marglit’s lab, developing methods for finding novel protein-RNA interactions using RNAseq data ."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html",
    "href": "materials/causal-inference/concepts/index.html",
    "title": "Causal Inference - Concepts",
    "section": "",
    "text": "With an emphasis on counterfactual prediction differing from risk prediction."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#the-basis-of-causal-inference-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#the-basis-of-causal-inference-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "",
    "text": "With an emphasis on counterfactual prediction differing from risk prediction."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#observational-studies-and-rcts-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#observational-studies-and-rcts-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "Observational studies and RCTs ",
    "text": "Observational studies and RCTs"
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#the-goldilocks-principle-of-covariate-adjustment-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#the-goldilocks-principle-of-covariate-adjustment-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "The Goldilocks principle of covariate adjustment ",
    "text": "The Goldilocks principle of covariate adjustment \nThe Goldilocks principle of covariate adjustment in causal inference."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#how-causation-causes-correlations-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#how-causation-causes-correlations-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "How causation causes correlations ",
    "text": "How causation causes correlations \nStructural causal view of statistical associations."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#a-tour-of-causal-assumptions-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#a-tour-of-causal-assumptions-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "A tour of causal assumptions ",
    "text": "A tour of causal assumptions \nCausal inference is a mindset. Therefore, it takes thought, not data, to translate data-based associations into causal claims."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#my-first-ever-lecture-on-causal-inference-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/concepts/index.html#my-first-ever-lecture-on-causal-inference-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Concepts",
    "section": "My first ever lecture on causal inference ",
    "text": "My first ever lecture on causal inference \nAn introduction to causal inference talk gave at a student seminar on November 18, 2018, named Causal Inference 101."
  },
  {
    "objectID": "materials/causal-inference/motivation/index.html#predicting-risk-is-not-enough-iconify-bi-filetype-pptx",
    "href": "materials/causal-inference/motivation/index.html#predicting-risk-is-not-enough-iconify-bi-filetype-pptx",
    "title": "Causal Inference - Motivation",
    "section": "Predicting risk is not enough ",
    "text": "Predicting risk is not enough"
  },
  {
    "objectID": "publications/2018-causal-inference-benchmarking-framework/index.html",
    "href": "publications/2018-causal-inference-benchmarking-framework/index.html",
    "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis",
    "section": "",
    "text": "@article{shimoni2018benchmarking,\n  title={Benchmarking framework for performance-evaluation of causal inference analysis},\n  author={Shimoni, Yishai and Yanover, Chen and Karavani, Ehud and Goldschmnidt, Yaara},\n  journal={arXiv preprint arXiv:1802.05046},\n  year={2018}\n}"
  },
  {
    "objectID": "publications/2018-causal-inference-benchmarking-framework/index.html#citation",
    "href": "publications/2018-causal-inference-benchmarking-framework/index.html#citation",
    "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis",
    "section": "",
    "text": "@article{shimoni2018benchmarking,\n  title={Benchmarking framework for performance-evaluation of causal inference analysis},\n  author={Shimoni, Yishai and Yanover, Chen and Karavani, Ehud and Goldschmnidt, Yaara},\n  journal={arXiv preprint arXiv:1802.05046},\n  year={2018}\n}"
  },
  {
    "objectID": "publications/2019-causal-inference-competitions/index.html",
    "href": "publications/2019-causal-inference-competitions/index.html",
    "title": "Comment: causal inference competitions: where should we aim?",
    "section": "",
    "text": "Can causal inference methodologists use soon-to-be-published experiments (randomized control trials) to benchmark causal inference methods similar to how computational biologists use soon-to-be-published solved protein structures to benchmark their protein folding algorithms."
  },
  {
    "objectID": "publications/2019-causal-inference-competitions/index.html#citation",
    "href": "publications/2019-causal-inference-competitions/index.html#citation",
    "title": "Comment: causal inference competitions: where should we aim?",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019comment,\n  title={Comment: causal inference competitions: where should we aim?},\n  author={Karavani, Ehud and El-Hay, Tal and Shimoni, Yishai and Yanover, Chen},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2019-mammograms-plus-ehrs/index.html#citation",
    "href": "publications/2019-mammograms-plus-ehrs/index.html#citation",
    "title": "Predicting breast cancer by applying deep learning to linked health records and mammograms",
    "section": "Citation",
    "text": "Citation\n@article{akselrod2019predicting,\n  title={Predicting breast cancer by applying deep learning to linked health records and mammograms},\n  author={Akselrod-Ballin, Ayelet and Chorev, Michal and Shoshan, Yoel and Spiro, Adam and Hazan, Alon and Melamed, Roie and Barkan, Ella and Herzel, Esma and Naor, Shaked and Karavani, Ehud and others},\n  journal={Radiology},\n  volume={292},\n  number={2},\n  pages={331--342},\n  year={2019},\n  publisher={Radiological Society of North America}\n}"
  },
  {
    "objectID": "publications/2019-prs-embryo-selection/index.html#citation",
    "href": "publications/2019-prs-embryo-selection/index.html#citation",
    "title": "Screening human embryos for polygenic traits has limited utility",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019screening,\n  title={Screening human embryos for polygenic traits has limited utility},\n  author={Karavani, Ehud and Zuk, Or and Zeevi, Danny and Barzilai, Nir and Stefanis, Nikos C and Hatzimanolis, Alex and Smyrnis, Nikolaos and Avramopoulos, Dimitrios and Kruglyak, Leonid and Atzmon, Gil and others},\n  journal={Cell},\n  volume={179},\n  number={6},\n  pages={1424--1435},\n  year={2019},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "publications/2022-propensity-score-calibration/index.html#citation",
    "href": "publications/2022-propensity-score-calibration/index.html#citation",
    "title": "Propensity score models are better when post-calibrated",
    "section": "Citation",
    "text": "Citation\n@article{gutman2022propensity,\n  title={Propensity score models are better when post-calibrated},\n  author={Gutman, Rom and Karavani, Ehud and Shimoni, Yishai},\n  journal={arXiv preprint arXiv:2211.01221},\n  year={2022}\n}"
  },
  {
    "objectID": "publications/2023-prs-plus-irm/index.html",
    "href": "publications/2023-prs-plus-irm/index.html",
    "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization",
    "section": "",
    "text": "Model overview"
  },
  {
    "objectID": "publications/2023-prs-plus-irm/index.html#citation",
    "href": "publications/2023-prs-plus-irm/index.html#citation",
    "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization",
    "section": "Citation",
    "text": "Citation\n@inproceedings{reyes2023fairprs,\n  title={FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization},\n  author={Reyes, Diego Machado and Bose, Aritra and Karavani, Ehud and Parida, Laxmi},\n  booktitle={Pacific Symposium on Biocomputing},\n  year={2023}\n}"
  }
]