[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications ",
    "section": "",
    "text": "My current research focus revolves mainly around causal inference. Both applications (mainly around healthcare) and methodology (including usability). I also dabble in information visualization and genetic-based risk models (polygenic risk scores). Prior to that, I worked on computational methods in molecular biology (genomics and transcriptomics). In addition to academic publications, I also issued several patents in the US.\nI will do my best to keep this listing updated, but in the plausible case I won’t, please see my Google Scholar page for the latest version."
  },
  {
    "objectID": "publications/index.html#publications",
    "href": "publications/index.html#publications",
    "title": "Publications ",
    "section": "Publications",
    "text": "Publications"
  },
  {
    "objectID": "publications/2024-ketorolac-repurposing/index.html#citation",
    "href": "publications/2024-ketorolac-repurposing/index.html#citation",
    "title": "Single-microglia transcriptomic transition network-based prediction and real-world patient data validation identifies ketorolac as a repurposable drug for Alzheimer’s disease",
    "section": "Citation",
    "text": "Citation\n@article{xu2024single,\nauthor = {Xu, Jielin and Song, Wenqiang and Xu, Zhenxing and Danziger, Michael M. and Karavani, Ehud and Zang, Chengxi and Chen, Xin and Li, Yichen and Paz, Isabela M Rivera and Gohel, Dhruv and Su, Chang and Zhou, Yadi and Hou, Yuan and Shimoni, Yishai and Pieper, Andrew A. and Hu, Jianying and Wang, Fei and Rosen-Zvi, Michal and Leverenz, James B. and Cummings, Jeffrey and Cheng, Feixiong},\ntitle = {Single-microglia transcriptomic transition network-based prediction and real-world patient data validation identifies ketorolac as a repurposable drug for Alzheimer's disease},\njournal = {Alzheimer's \\& Dementia},\ndoi = {https://doi.org/10.1002/alz.14373},\n}"
  },
  {
    "objectID": "publications/2024-bicause-trees/index.html#citation",
    "href": "publications/2024-bicause-trees/index.html#citation",
    "title": "Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation",
    "section": "Citation",
    "text": "Citation\n@article{ter2024hierarchical,\n  title={Hierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation},\n  author={Ter-Minassian, Lucile and Szlak, Liran and Karavani, Ehud and Holmes, Chris and Shimoni, Yishai},\n  journal={arXiv preprint arXiv:2401.17737},\n  year={2024}\n}"
  },
  {
    "objectID": "publications/2023-causalvis/index.html",
    "href": "publications/2023-causalvis/index.html",
    "title": "Causalvis: Visualizations for Causal Inference",
    "section": "",
    "text": "Workflow summary"
  },
  {
    "objectID": "publications/2023-causalvis/index.html#citation",
    "href": "publications/2023-causalvis/index.html#citation",
    "title": "Causalvis: Visualizations for Causal Inference",
    "section": "Citation",
    "text": "Citation\n@inproceedings{guo2023causalvis,\n  title={Causalvis: Visualizations for Causal Inference},\n  author={Guo, Grace and Karavani, Ehud and Endert, Alex and Kwon, Bum Chul},\n  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},\n  pages={1--20},\n  year={2023}\n}"
  },
  {
    "objectID": "publications/2019-prs-embryo-selection/index.html#citation",
    "href": "publications/2019-prs-embryo-selection/index.html#citation",
    "title": "Screening human embryos for polygenic traits has limited utility",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019screening,\n  title={Screening human embryos for polygenic traits has limited utility},\n  author={Karavani, Ehud and Zuk, Or and Zeevi, Danny and Barzilai, Nir and Stefanis, Nikos C and Hatzimanolis, Alex and Smyrnis, Nikolaos and Avramopoulos, Dimitrios and Kruglyak, Leonid and Atzmon, Gil and others},\n  journal={Cell},\n  volume={179},\n  number={6},\n  pages={1424--1435},\n  year={2019},\n  publisher={Elsevier}\n}"
  },
  {
    "objectID": "publications/2019-mammograms-plus-ehrs/index.html#citation",
    "href": "publications/2019-mammograms-plus-ehrs/index.html#citation",
    "title": "Predicting breast cancer by applying deep learning to linked health records and mammograms",
    "section": "Citation",
    "text": "Citation\n@article{akselrod2019predicting,\n  title={Predicting breast cancer by applying deep learning to linked health records and mammograms},\n  author={Akselrod-Ballin, Ayelet and Chorev, Michal and Shoshan, Yoel and Spiro, Adam and Hazan, Alon and Melamed, Roie and Barkan, Ella and Herzel, Esma and Naor, Shaked and Karavani, Ehud and others},\n  journal={Radiology},\n  volume={292},\n  number={2},\n  pages={331--342},\n  year={2019},\n  publisher={Radiological Society of North America}\n}"
  },
  {
    "objectID": "publications/2019-causal-inference-competitions/index.html",
    "href": "publications/2019-causal-inference-competitions/index.html",
    "title": "Comment: causal inference competitions: where should we aim?",
    "section": "",
    "text": "Can causal inference methodologists use soon-to-be-published experiments (randomized control trials) to benchmark causal inference methods similar to how computational biologists use soon-to-be-published solved protein structures to benchmark their protein folding algorithms."
  },
  {
    "objectID": "publications/2019-causal-inference-competitions/index.html#citation",
    "href": "publications/2019-causal-inference-competitions/index.html#citation",
    "title": "Comment: causal inference competitions: where should we aim?",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019comment,\n  title={Comment: causal inference competitions: where should we aim?},\n  author={Karavani, Ehud and El-Hay, Tal and Shimoni, Yishai and Yanover, Chen},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2018-causal-inference-benchmarking-framework/index.html",
    "href": "publications/2018-causal-inference-benchmarking-framework/index.html",
    "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis",
    "section": "",
    "text": "@article{shimoni2018benchmarking,\n  title={Benchmarking framework for performance-evaluation of causal inference analysis},\n  author={Shimoni, Yishai and Yanover, Chen and Karavani, Ehud and Goldschmnidt, Yaara},\n  journal={arXiv preprint arXiv:1802.05046},\n  year={2018}\n}"
  },
  {
    "objectID": "publications/2018-causal-inference-benchmarking-framework/index.html#citation",
    "href": "publications/2018-causal-inference-benchmarking-framework/index.html#citation",
    "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis",
    "section": "",
    "text": "@article{shimoni2018benchmarking,\n  title={Benchmarking framework for performance-evaluation of causal inference analysis},\n  author={Shimoni, Yishai and Yanover, Chen and Karavani, Ehud and Goldschmnidt, Yaara},\n  journal={arXiv preprint arXiv:1802.05046},\n  year={2018}\n}"
  },
  {
    "objectID": "materials/causal-inference/motivation/index.html#predicting-risk-is-not-enough",
    "href": "materials/causal-inference/motivation/index.html#predicting-risk-is-not-enough",
    "title": "Causal Inference - Motivation",
    "section": "Predicting risk is not enough ",
    "text": "Predicting risk is not enough"
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html",
    "href": "materials/causal-inference/concepts/index.html",
    "title": "Causal Inference - Concepts",
    "section": "",
    "text": "With an emphasis on counterfactual prediction differing from risk prediction."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#the-basis-of-causal-inference",
    "href": "materials/causal-inference/concepts/index.html#the-basis-of-causal-inference",
    "title": "Causal Inference - Concepts",
    "section": "",
    "text": "With an emphasis on counterfactual prediction differing from risk prediction."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#observational-studies-and-rcts",
    "href": "materials/causal-inference/concepts/index.html#observational-studies-and-rcts",
    "title": "Causal Inference - Concepts",
    "section": "Observational studies and RCTs ",
    "text": "Observational studies and RCTs"
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#the-goldilocks-principle-of-covariate-adjustment",
    "href": "materials/causal-inference/concepts/index.html#the-goldilocks-principle-of-covariate-adjustment",
    "title": "Causal Inference - Concepts",
    "section": "The Goldilocks principle of covariate adjustment ",
    "text": "The Goldilocks principle of covariate adjustment \nThe Goldilocks principle of covariate adjustment in causal inference."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#how-causation-causes-correlations",
    "href": "materials/causal-inference/concepts/index.html#how-causation-causes-correlations",
    "title": "Causal Inference - Concepts",
    "section": "How causation causes correlations ",
    "text": "How causation causes correlations \nStructural causal view of statistical associations."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#a-tour-of-causal-assumptions",
    "href": "materials/causal-inference/concepts/index.html#a-tour-of-causal-assumptions",
    "title": "Causal Inference - Concepts",
    "section": "A tour of causal assumptions ",
    "text": "A tour of causal assumptions \nCausal inference is a mindset. Therefore, it takes thought, not data, to translate data-based associations into causal claims."
  },
  {
    "objectID": "materials/causal-inference/concepts/index.html#my-first-ever-lecture-on-causal-inference",
    "href": "materials/causal-inference/concepts/index.html#my-first-ever-lecture-on-causal-inference",
    "title": "Causal Inference - Concepts",
    "section": "My first ever lecture on causal inference ",
    "text": "My first ever lecture on causal inference \nAn introduction to causal inference talk gave at a student seminar on November 18, 2018, named Causal Inference 101."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ehud Karavani",
    "section": "",
    "text": "Hi there!\nI’m a researcher and data scientist, specializing in causal inference, machine learning, (Bayesian) statistics, and data visualization.\nI’m currently a Research Staff Member at IBM Research, Israel in the Causal Machine Learning for Healthcare & Life Sciences group. There, I focus on high-throughput causal inference for finding new indications for existing drugs using electronic health records and insurance claims data. I’m also the creator and maintainer of causallib, a one-stop shop open-source Python package for flexible causal inference modeling.\n\nEducation\nI have a BSc and MSc in Computer Science and Computational Biology from the Hebrew University. I did my Master’s thesis with Dr. Shai Carmi studying prediction of traits using DNA and its potential effect on selecting embryos for implantation in IVF (aka “designer babies”) .\nPrevious to that I was an undergraduate research associate in Prof. Hanah Marglit’s lab, developing methods for finding novel protein-RNA interactions using RNAseq data .\n\n\nElsewhere on the Internet\n\n\n    \n        \n            \n            LinkedIn\n        \n    \n    \n        \n            \n            Google Scholar\n        \n    \n    \n        \n            \n            GitHub\n        \n    \n    \n        \n            \n            Medium\n        \n    \n\n\nNo matching items\n\n\n\nQ&A\n\n\n    \n        \n            \n            Stack Overflow\n        \n    \n    \n        \n            \n            Cross Validated\n        \n    \n    \n        \n            \n            Datamethods\n        \n    \n\n\nNo matching items\n\n\n\n\nSocial\n\n\n    \n        \n            \n            Twitter\n        \n    \n    \n        \n            \n            Bluesky\n        \n    \n    \n        \n            \n            Mastodon\n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "List of blog posts\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVisualizing (double) cross-fitting for causal inference\n\n\nA visual way to understand cross-validation, cross-fitting, and double cross-fitting, and to differentiate between them. \n\n\n\ncausal inference\n\n\nvisualization\n\n\n\n\n\n\nDec 30, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty-aware model comparison\n\n\nComparing model performance rigorously and meaningfully using statistics. \n\n\n\nmachine learning\n\n\ndeep learning\n\n\nstatistics\n\n\nmeta-science\n\n\n\n\n\n\nAug 30, 2024\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nOui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference\n\n\nAugmenting the good old Love plot by incorporating covariate-outcome importance measures. \n\n\n\ncausal inference\n\n\nvisualization\n\n\n\n\n\n\nApr 20, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSequential trial design for causal inference\n\n\nAligning time-zero to reduce time-related biases by taking simplest solution possible and awkwardly repeating it until it’s great. \n\n\n\ncausal inference\n\n\nstudy design\n\n\n\n\n\n\nOct 24, 2023\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nHello Quarto!\n\n\nI made a personal website. With blogs and stuff.\n\n\n\n\n\nJul 15, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCausal inference is a mindset\n\n\nCausal inference from observational data is a mindset, not a set of tools. \n\n\n\ncausal inference\n\n\n\n\n\n\nJan 4, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference with Continuous Treatments\n\n\nGeneralizing inverse probability weights for non-categorical treatments. \n\n\n\ncausal inference\n\n\n\n\n\n\nNov 2, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we care for covariate balancing in comparative studies\n\n\nBalancing variables in statistical comparative analysis is a proxy, not a goal. \n\n\n\ncausal inference\n\n\n\n\n\n\nNov 20, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nA visual way to think of macro and micro averages in classification metrics\n\n\nExplaining what macro-average and micro-average metrics are. \n\n\n\nmachine learning\n\n\nvisualization\n\n\n\n\n\n\nSep 4, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing machine learning metrics to evaluate causal inference models\n\n\nReinterpreting known machine learning evaluations from a causal inference perspective, focusing on ROC curves for propensity models. \n\n\n\ncausal inference\n\n\nmachine learning\n\n\n\n\n\n\nDec 28, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe case against Agile research\n\n\nEver popular iterative development approaches can sneak in unconscious-bias that can be harmful to the scientific process. \n\n\n\nmeta-science\n\n\n\n\n\n\nJul 6, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSolving Simpson’s Paradox with Inverse Probability Weighting\n\n\nA visual intuition on how the most popular method in causal-inference works, and how it solves one of the most popular paradoxes in statistics. \n\n\n\ncausal inference\n\n\nstatistics\n\n\n\n\n\n\nFeb 2, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Deep Learning to Genetic Prediction\n\n\nWhat classical methods for obtaining polygenic (risk) scores lack, and how deep learning might help mitigated these shortcomings. \n\n\n\ngenetics\n\n\ndeep learning\n\n\n\n\n\n\nMay 5, 2018\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html",
    "href": "blog/2024/02-model-comparison/index.html",
    "title": "Uncertainty-aware model comparison",
    "section": "",
    "text": "Note\n\n\n\nThis post is still a work in process."
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#motivation",
    "href": "blog/2024/02-model-comparison/index.html#motivation",
    "title": "Uncertainty-aware model comparison",
    "section": "Motivation",
    "text": "Motivation\nI’m under the impression that deep learning models are not benchmarked properly, making it difficult to draw meaningful conclusions. Too often than not, models will be evaluated using a single number, with no uncertainty around that number. Too often than not, the difference in performance of different models will not be assessed directly, and again, no uncertainty around that difference either1.\nThis creates difficulty in understanding which models are comparable and which are superior. Is a point difference of 0.1% between models on the leaderboard meaningful or a fluke? As the monetary (and environmental) costs of training these large language models (LLMs) skyrockets, we should be able to truly tell whether size matters and are larger models really are superior to their slimmer, more economical versions.\nTo solve that, we’ll need to know the range of errors compatible with the model; not just the average score - but the uncertainty around it. Unfortunately (non-Bayesian) machine learning and deep learning models don’t lend themselves to uncertainty estimation easily. Oftentimes, estimating the variability in model performance requires the bootstrap (sampling data with replacement and refitting)2 or cross validation. But fitting just a single model can exhaust unreasonable resources, so fitting it hundreds (or even thousands) of times just to estimate its variance is out of the question. Luckily, there are other ways to estimate the variance of model performance using the variability within the dataset or asymptotics (that may very well kick in as datasets can be large). In this post I’ll show how to utilize them.\n\nWhat’s in it for you\nIn this blog post I’ll explore an approach to rigorously compare the performance of two (or more!) models directly. It will incorporate uncertainty that will allow us to assess whether a model is truly superior or inferior (or non-inferior) to others3. It will be a generalizable framework, allowing for multiple models, multiple metrics, multiple datasets, even at once. Moreover, it is totally decoupled from the actual models - you’ll only need predictions and ground-truth labels. No need for refitting or retrofitting, only a forward pass."
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#setup",
    "href": "blog/2024/02-model-comparison/index.html#setup",
    "title": "Uncertainty-aware model comparison",
    "section": "Setup",
    "text": "Setup\nTo achieve all that I will utilize Statistics; which I personally feel were long overlooked as the field of machine (and then deep) learning became more and more dominated by engineers (or engineering mindset rather than by scientists more versed in the scientific method and its philosophy). It’s not necessarily a bad thing! Engineers get things done, but the skill sets (or the mindset) may de-prioritize Research Methods.\nHowever, I’m a statistics reformer kind of person. So I will not trouble you with the zoo of statistical tests - it appears they are mostly just linear models (Lindeløv 2019). You know, “regression”. Sometimes called “ANOVAs”, but I know this jargon might intimidate some, so let’s stick with “linear models” for now4. The most important point is that I will use this machinery to compare between models. The benefit using this framework is that it comes with 100+ years of statistics research focusing on uncertainty estimation, that – thanks to software available today – we can just apply off-the-shelf.\n\nTo be completely honest, I don’t think this approach is novel, at least not in its basic version. Although I haven’t encountered it exactly as I’m going to present it, it might be the case I just haven’t landed on the right keyword search on google. And I’ll use Python, which I hope is more convenient to those who are versed in the field of deep learning. So, as always, some people will find this post informative and some will just scoff.\n\nPrerequisites\nBefore I start, I’ll cover some of the basics first, for completeness. If you’re comfortable with statistical inference, regression, and model evaluation, then you can skip it (or at least skim it).\n\nNotations\nThroughout the post I’ll use \\(X\\) (X) to denote a feature matrix (covariates), \\(y\\) (y or y_true) will be the ground truth labels coupled to \\(X\\), and \\(\\hat{y}\\) (y_pred) will be the predictions from an arbitrary model \\(f(X)\\) (a linear regression or a deep neural network) aiming to be as closest to \\(y\\) as possible.\nThe approach I’ll present will only need \\(y\\) and \\(\\hat{y}\\), making it independent of \\(f\\) and whatever software or hardware it is attached to.\n\n\nIndividual-level errors\nPerformance metrics are a way to quantify the performance of a model, assessing how the model errs by comparing the ground truth labels (\\(y\\)) to the predictions (\\(\\hat{y}\\)). Most commonly known will be the mean squared error (MSE) if \\(y\\) is continuous or log-loss (aka cross entropy) if \\(y\\) is categorical. If you are, like me, used to Scikit-learn’s metrics API with metric(y_true, y_pred) returning a single number - the average over the dataset, you might have forgotten that these errors can be calculated on an individual level. For instance, \\(l_i = (y_i - \\hat{y}_i)^2\\) is the individual-level squared error of observation \\(i\\) (which is later averaged to obtain the mean squared error).\nNote that not all evaluation metrics allow individual-level errors. For instance, the ROC AUC (and many confusion matrix-based metrics) will often require aggregating over the sample, and individual-level errors might not be well-defined5.\n\n\nRegression models are fancy average machines\nLinear models (those \\(y_i = \\alpha + \\beta X_i\\), for example) end up estimating \\(E[Y|X]\\). Namely, the expected value of \\(Y\\) given some level of \\(X\\). Basically, the average value of \\(Y\\) in some setting. And if there is no \\(X\\) (i.e., there is only an intercept, so \\(y_i = \\alpha\\)), then that’s just the average overall, look:\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(seed=0)\n\ny = rng.normal(loc=3, size=1000)\nols = smf.ols(\n    \"y ~ 1\", \n    data=pd.DataFrame({\"y\": y})\n).fit()\nprint(f\"Taking the mean of `y`: {y.mean():.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params['Intercept']:.4f}\")\n\nTaking the mean of `y`: 2.9520\nThe coefficient of the regression intercept: 2.9520\n\n\nThe nice thing about the regression framework is that we also get inferential results (standard errors, confidence intervals, p-values…) for free, off-the-shelf. In the frequentist (read, “standard”) regression setting, this is done using analytical asymptotic properties (no resampling and refitting)\n\nprint(ols.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nNo. Observations:                1000   F-statistic:                       nan\nCovariance Type:            nonrobust   Prob (F-statistic):                nan\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9520      0.031     95.523      0.000       2.891       3.013\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nAveraging individual-level errors with regression\nTying the two subsections above together, given a ground truth vector \\(y\\) and predictions \\(\\hat{y}\\), we can calculate the mean squared error as an average or with a regression.\n\nfrom sklearn.metrics import mean_squared_error\n\ny = rng.normal(loc=3, size=1000)\ny_pred = y + rng.normal(loc=0, scale=0.5, size=1000)\n\nindividual_errors = (y - y_pred)**2\nols = smf.ols(\n    \"li ~ 1\", \n    data=pd.DataFrame({\"li\": individual_errors})\n).fit()\nprint(f\"Taking the mean of the squared errors with sklearn: {mean_squared_error(y, y_pred):.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params['Intercept']:.4f}\")\n\nTaking the mean of the squared errors with sklearn: 0.2415\nThe coefficient of the regression intercept: 0.2415\n\n\n\n\nGeneralization and statistical inference of model performance\nModel performance is a characteristic of the model. Evaluating it on a given dataset is not the core interest of the researcher. We often care about generalization errors - figuring out what will be the performance when the model will be deployed publicly on unseen data.\nThis notion of generalization carries a very similar meaning to inferential statistics, where the average in a sample is of little importance relative to the mean of the entire underlying population from which the dataset was sampled from6. And since the leap from sample average to population mean comes with some uncertainty to it, we often bound these estimates with uncertainty intervals around them7.\nLike inferential statistics, in generalization we would like to infer what will the average error rate will be on unknown data out there in the world, using the sample of data we do have right now. We don’t really care about the error in any given dataset, even if it is a test set that was not used for training, it is still just a sample from the possible population of data available in the world. The leap from the average error rate in the dataset to the errors out there comes with some uncertainty to it, and we’d like to know the likely range of that average error rate in the actual world."
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#uncertainty-aware-model-comparison-via-regression",
    "href": "blog/2024/02-model-comparison/index.html#uncertainty-aware-model-comparison-via-regression",
    "title": "Uncertainty-aware model comparison",
    "section": "Uncertainty-aware model comparison via regression",
    "text": "Uncertainty-aware model comparison via regression\nAs alluded to above, we’ll use individual-level errors in a regression framework in order to infer the uncertainty around performance measures. Knowing whether all errors tightly clustered around zero or if their variance is high will be useful information when we will take uncertainty-aware differences of performance between models.\n\nSettings up data\nTo demonstrate the process, we will need labeled data and we will need prediction models. I’ll use toy data of a regression, rather than classification, task (i.e., a continuous target) for simplicity as squared errors were already introduced, but this can work with any individual-level metric. I will split it train and test, so the comparison will be performed on unseen-data. As for models, I’ll compare a linear (regression) with a nonlinear (boosting trees) one, just so we can see the difference in performance.\n\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = make_friedman1(n_samples=400, noise=1, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\nlr = LinearRegression().fit(X_train, y_train)\ngbt = GradientBoostingRegressor().fit(X_train, y_train)\n\nNow let’s calculate individual-level squared errors:\n\nlr_res = (y_test - lr.predict(X_test))**2\ngbt_res = (y_test - gbt.predict(X_test))**2\n\n\n\nComparing models\nTo compare the difference in performance between the models, I’ll model it as a regression task setting the individual-level errors as the target and a dummy coding of the models as the feature (together with an intercept). This merely requires concatenating the errors and indicating which model corresponds to them.\n\nsqe = pd.concat({\"lr\": pd.Series(lr_res), \"gbt\": pd.Series(gbt_res)}, names=[\"model\", \"id\"]).rename(\"li\")\nsqe\n\nmodel  id \nlr     0       0.089986\n       1       1.319738\n       2       6.044722\n       3       6.061478\n       4      13.876234\n                ...    \ngbt    195     0.625408\n       196     0.935833\n       197     0.067468\n       198     3.241952\n       199     3.417277\nName: li, Length: 400, dtype: float64\n\n\nAnd putting that data into the regression8:\n\nsqe = sqe.reset_index()\nols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit()\nprint(ols.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     li   R-squared:                       0.019\nModel:                            OLS   Adj. R-squared:                  0.017\nNo. Observations:                 400   F-statistic:                     7.852\nCovariance Type:            nonrobust   Prob (F-statistic):            0.00533\n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          4.2327      0.605      6.994      0.000       3.043       5.422\nC(model)[T.lr]     2.3981      0.856      2.802      0.005       0.716       4.081\n==================================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can now follow a standard interpretation of a linear model. The C(model)[T.lr] variable (i.e., setting lr to be encoded as 1 while gbt was encoded as 0) quantifies the average difference in squared errors (variance) between the linear regression and the boosting trees models (2.4). This allows us to directly assess the difference in model performance, but since we get asymptotic inference too, we can further place uncertainty around that difference - enabling us to tell whether it is statistically significant or not (p-value 0.0053), or at least what is a likely range for that error to be in the “population” of data from which the dataset came from (95% confidence intervals (0.7, 4.1)).\nThis direct comparison is the proper way to assess difference in performance. When deep learning model do get confidence intervals around them, they are often around the performance metrics itself. But then, translating that to confidence intervals around the difference between two models (and their own confidence intervals) is not necessarily straightforward, and requires extra caution not to misinterpret the significance (Gelman and Stern 2006).\nThis was relatively cool. If you stop reading here it’s ok. You are now able to compare models relatively simply and with some notion of uncertainty."
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#going-further",
    "href": "blog/2024/02-model-comparison/index.html#going-further",
    "title": "Uncertainty-aware model comparison",
    "section": "Going further",
    "text": "Going further\n\nNon-inferiority testing\nNoninferiority is an interesting, well-established statistical concept, that I believe might benefit the deep-learning evaluation literature. Imagine a clinical setting where you want to test a new therapeutic treatment. This treatment may not reduce mortality more than the standard of care, but it doesn’t mean it’s useless. Maybe it can be taken at home once a month, instead of 3 times a week at the clinic or have much, much fewer side effects? Wouldn’t that improve patients’ life? Well, it will improve their life assuming the mortality rate of the new treatment is not that worse than the current standard of care. If both treatment have similar or comparable mortality rate, but the newer one is cheaper/easier to administer, etc. wouldn’t we prefer it over the existing treatment?\nOur statistical testing perspective has now shifted - we no longer care if the new treatment is superior to the current treatment, only that it is noninferior. In practical terms, it means we no longer compare our interval against 0 (for difference or 1 for ratio), but against some other margin which we’re willing to accept as similar for all practical considerations. Figure Figure 1 summarizes nicely what we’d like to compare against in a noninferiority setup.\n\n\n\n\n\n\nFigure 1: Noninferiority hypothesis testing taken from Mauri and D’Agostino Sr (2017)\n\n\n\nWe can apply the same logic translating “treatments” to “models”. Wouldn’t we want to assess whether models have comparable performance and then select the model that is simpler/cheaper to train or has a smaller carbon footprint? Is it really worth spending 100 times more FLOPs for a point improvement of 0.1 in some metric, that we don’t even know how the variance around that 0.1 improvement looks like? We should compare models in a way that will show us the fuller picture and allow us to make a truly informed decision about model selection.\n\n\nGoing Bayesian\nNow, I’m positive that the regression framework I presented here is somewhat equivalent to what happens in R when you use anova(mod1, mod2) (ANOVA with two models). In R, you often specify a test parameter to anova (like \"Chisq\" for a likelihood ratio test), but this requires knowing the degrees of freedom the models have in order to properly set up the null distribution to draw p-values from. Now, go figure what the degrees of freedom of ChatGPT9. I’m not sure if the Wald test used for the regression p-value (or confidence intervals) sidesteps it, but regardless, adopting a Bayesian framework will save us from asymptotic requirements, as the uncertainty will be directly assessed from the data at hand.\nAdopting a Bayesian framework will come with multiple strengths:\n\nFirst, it will allow us to focus on magnitude of difference and estimation instead of null-hypothesis significant testing (Kruschke 2013); this will be even more important for non-inferiority tests.\nSecond, it will allow us to interpret the uncertainty intervals more naturally and give probabilistic estimates to how likely it is that one model is better than the other (replacing confidence intervals with credible intervals).\nThird, the Bayesian regression framework is more easily expandable to allow for multiple datasets (mixed/multilevel models), multiple models, and more proper modeling (generalized linear models).\n\n\nRegression modeling in Python with bambi\nbambi is great, our very own brms. It’s a powerful tool to specify regression formulas (like above for statsmodels) but then it will utilize PyMC under the hood to perform an MCMC-based Bayesian estimation, and do lots of additional good stuff like specifying priors automatically etc. Repeating the model comparison from above is as simple as:\n\nimport bambi as bmb\n\n# ols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit()\nbols = bmb.Model(\"li ~ 1 + model\", data=sqe).fit(\n   cores=1,  # play nice with `quarto render` for some reason\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma, Intercept, model]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdeally, after fitting a Bayesian model you should evaluate the fit using some diagnostics and az.trace_plot(), which I will skip (but I checked, it’s ok).\n\n\nBayesian noninferiority testing\nThe Bayesian inference framework, focusing on estimation and providing a full posterior distribution of the difference in performance, makes it really easy to test for noninferiority. We can use the Region of Practical Equivalence (ROPE) to specify an intervals that is equivalent to the null, meaning it has practically no effect of interest and is therefore negligible (e.g., within 1% change in performance between model is good enough (Kruschke 2018)).\n\nimport arviz as az\n\nloss_std = sqe[\"li\"].std()\naz.plot_posterior(\n    bols, var_names=\"model\",\n    ref_val=0, \n    rope=[-0.1*loss_std, 0.1*loss_std],  \n)\n\n\n\n\n\n\n\n\n\nFor example, we see both the High Density Interval (HDI, the interval equivalent to a confidence interval in some sense) and the entire posterior distribution do overlap some with the ROPE, suggesting there’s a ~3.5% chance the difference in performance is actually equivalent. Additionally, ~0.2% of the distribution is below 0, suggesting there’s a miniscule (but existing) chance that gradient boosting trees are actually worse than linear regression on this prediction task, but that the magnitude in which they are worse is practically negligible. However, the lion share of the evidence suggests the linear regression prediction model has higher errors than the boosting trees.\n\n\n\nExtensions\nThis regression framework for comparing models is easily extendable, especially the Bayesian one. Here are some examples:\n\nMultiple datasets: assess overall performance over multiple datasets.\nBy concatenating the individual-level errors from multiple datasets, but still focusing on the model variable that estimates the difference between models. In such case, however, the response variable (errors) might no longer be individual and identically distributed (IID), because it originates from the same models. Ignoring these correlations between observations and assuming they are IID might make our estimates overly confident, which is the counterproductive to our main goal of assessing uncertainty properly. Luckily, we can apply multilevel modeling (mixed linear models) to regress away these dependencies simply by specifying li ~ 1 + model + (1 | dataset)10 11.\nMultiple models: Compare multiple models. Extending the dummy encoding of the model to any other contrast encoding between models can be done in a straightforward way, and allow comparing multiple models in multiple ways simultaneously.\nMultiple metrics: simultaneously If we have several metrics that interest us than we can model the difference between models on all of them simultaneously by stacking the different errors side-by-side and making a multivariate regression model.\nMany types of metrics So far I’ve discussed the squared error, but this framework can support non-continuous errors (metric) too. For example, a classification model can be evaluated using the log-loss in a similar way to how squared-errors were done above, but we can also use the 0-1 loss instead–which is a much more interpretable loss–and use a logistic-regression instead of a linear regression model.\nProper modeling of the errors. I’ve used a linear model so far, but it assumes the response (in our case the individual-level errors) can range between \\(-\\infty\\) to \\(\\infty\\). But errors are almost always strictly positive, and if we want to respect that support, we should, instead, model it with a Gamma regression - a generalized linear model with a Gamma distribution (and a log-link), which operates on the \\((0, \\infty)\\) range.12\nIncorporate metadata Models (and datasets) may come with plenty of metadata (e.g., number of parameters). These metadata can be incorporated into the regression as additional covariates and allow us to make even more interesting conclusions than just comparing models. For example, incorporating model size will enable us to discuss about difference in performance between models while holding model size constant. This will make cross-player comparison (e.g., GPT vs. Gemini vs. Claude) much more juicy.\n\nI can quickly demonstrate some of these13. Let’s generate more data from a different dataset, by repeating the process above:\n\nfrom sklearn.datasets import make_friedman2\n\nX, y = make_friedman2(n_samples=400, noise=1, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\nlr = LinearRegression().fit(X_train, y_train)\ngbt = GradientBoostingRegressor().fit(X_train, y_train)\n\nlr_res = (y_test - lr.predict(X_test))**2\ngbt_res = (y_test - gbt.predict(X_test))**2\n\nfriedman2_sqe = pd.concat({\"lr\": pd.Series(lr_res), \"gbt\": pd.Series(gbt_res)}, names=[\"model\", \"id\"]).rename(\"li\")\nfriedman2_sqe = friedman2_sqe.reset_index()\nfriedman2_sqe\n\n\n\n\n\n\n\n\nmodel\nid\nli\n\n\n\n\n0\nlr\n0\n3595.551049\n\n\n1\nlr\n1\n642.439172\n\n\n2\nlr\n2\n96190.142608\n\n\n3\nlr\n3\n896.453152\n\n\n4\nlr\n4\n388.954325\n\n\n...\n...\n...\n...\n\n\n395\ngbt\n195\n1053.935114\n\n\n396\ngbt\n196\n115.481420\n\n\n397\ngbt\n197\n3266.020568\n\n\n398\ngbt\n198\n7.002061\n\n\n399\ngbt\n199\n1271.759728\n\n\n\n\n400 rows × 3 columns\n\n\n\nNow let’s stack this dataset on top of the previous dataset\n\nsqe = pd.concat(\n    {\"friedman1\": sqe, \"friedman2\": friedman2_sqe.reset_index()},\n    names=[\"dataset\"],\n).reset_index().drop(columns=\"level_1\")\nsqe\n\n\n\n\n\n\n\n\ndataset\nmodel\nid\nli\nindex\n\n\n\n\n0\nfriedman1\nlr\n0\n0.089986\nNaN\n\n\n1\nfriedman1\nlr\n1\n1.319738\nNaN\n\n\n2\nfriedman1\nlr\n2\n6.044722\nNaN\n\n\n3\nfriedman1\nlr\n3\n6.061478\nNaN\n\n\n4\nfriedman1\nlr\n4\n13.876234\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n795\nfriedman2\ngbt\n195\n1053.935114\n395.0\n\n\n796\nfriedman2\ngbt\n196\n115.481420\n396.0\n\n\n797\nfriedman2\ngbt\n197\n3266.020568\n397.0\n\n\n798\nfriedman2\ngbt\n198\n7.002061\n398.0\n\n\n799\nfriedman2\ngbt\n199\n1271.759728\n399.0\n\n\n\n\n800 rows × 5 columns\n\n\n\nNow we can fit a multilevel Gamma regression on the combined data:\n\nbgamma = bmb.Model(\n    \"li ~ 1 + model + (1 | dataset)\", \n    data=sqe,\n    family=\"gamma\",\n    link=\"log\",\n).fit(\n    tune=2000,\n    target_accept=0.94,\n    random_seed=11,\n    cores=1  # play nice with `quarto render` for some reason\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [alpha, Intercept, model, 1|dataset_sigma, 1|dataset_offset]\n\n\n\n\n\n\n\n\n\n\n\nAnd investigate the posteriors:\n\naz.plot_posterior(\n    bgamma, var_names=\"model\",\n    ref_val=0, \n    rope=[-0.1*loss_std, 0.1*loss_std], \n)"
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#conclusions",
    "href": "blog/2024/02-model-comparison/index.html#conclusions",
    "title": "Uncertainty-aware model comparison",
    "section": "Conclusions",
    "text": "Conclusions\nI’ve presented a framework to make model comparison rigorously and meaningfully by incorporating uncertainty measure around performance difference. The framework is super easy because it builds on top of regression models, widely available in every scientific software, taking advantage of them just being fancy extendable averaging machines. This opens up many possibilities of properly comparing multiple models on multiple datasets and multiple metrics. Most importantly, it enables us to perform noninferiority tests and understand whether two models are comparable (have similar performance) even though one of them may be much smaller / easier to train / cheaper to train / have smaller carbon footprint. This may encourage researchers to come up with substantially new approaches, rather than chasing negligible improvements of fraction of a percentage in leaderboards."
  },
  {
    "objectID": "blog/2024/02-model-comparison/index.html#footnotes",
    "href": "blog/2024/02-model-comparison/index.html#footnotes",
    "title": "Uncertainty-aware model comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nand we know the standard error of the difference between means is larger than the standard error of either mean.↩︎\nOne variant I’ve also seen in some settings is that the sample is kept fixed, and just iterates over different random initializations of the model.↩︎\nThese are formal, well-defined statistical concepts that I’ll introduce later down the post.↩︎\nActually, we’ll need (Bayesian) “generalized linear (mixed) models” to enjoy the full capabilities of the framework, but modern software makes it so easy you don’t have to bother much about it.↩︎\nThere might be workarounds, but let’s ignore those for the sake of this post, and focus on easily-accessible individual-level errors.↩︎\nIn the most basic example, say we are interested in the average height of men in a certain country. We sample men from that country and measure their height, but the average height of the sample is of little interest. What’s interesting is what that average tells about the average height of the people in that country, or, in reverse - what can we infer about the population from the sample.↩︎\nIn this setting, I allude to confidence intervals, of course. Strictly speaking, they are not really uncertainty intervals (more like compatibility intervals), but because I will also introduce Bayesian credible intervals later, I don’t want to be sidetracked by the lingo, so I call them all uncertainty intervals.↩︎\nWe use the factor transformation C(model) to convert the model column, which contains strings, into 0-1 coding suitable for regression↩︎\nOr any deep learning model, for that matter. It seems the classical statistical theory does not always correspond to observed evidence from these types of models.↩︎\nThere are frequentist mixed models, and these correlation structures might also be accounted for using Generalized Estimating Equations (GEE), but their uncertainties are not always trustworthy. Bayesian mixed models, however, still shine.↩︎\nOnce in the realm of multiple models multiple dataset, it may be interesting to evaluate performance in the item-response framework to also get a gist of what datasets are easier or harder.↩︎\nActually, since errors can be truly 0, there might be a need to extend the model to be a hurdle Gamma model.↩︎\nno multiple metrics because bambi does not support multivariate gamma models↩︎"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html",
    "href": "blog/2023/03-sequential_trial_design/index.html",
    "title": "Sequential trial design for causal inference",
    "section": "",
    "text": "I’m not special, so I’ve spent a lot of time addressing confounding bias in causal inference. I even created and still maintain an open-source Python package whose main focus is flexible causal inference modeling (pip install causallib). There’s a place to discuss why confounding bias is the most popular bias, but this blog post is not it.  I have a different bias I want to focus on.\nThere’s actually some compelling evidence (well, at least anecdotal) that confounding bias has less of a biasing effect than what one would expect. One example is Garcı́a-Albéniz, Hsu, and Hernán (2017) examining the effect of colorectal cancer screening on colorectal cancer incidence. It showed the same survival curves for treatment and control units when adjusting and not adjusting for confounding factors. This, in and of itself, is a poor evidence, but the exact same shape of survival curves also appeared in Bretthauer et al. (2022), an RCT examing the exact same question. This suggests that confounding bias did not affect the observational study.1\n\n\nThis bias originates from improperly setting the time-zero. Time zero (or “index date”) is the point in time which splits the baseline period from the follow-up period. History from future, retrospectively speaking. It usually the time in which treatment is initiated, and therefore the point in time from which we look backwards to obtain historical data (baseline covariates) and look forward to gather follow-up data (outcomes).\nAs such, to properly set up time zero, three things must align:\n\nThe subject must meet the eligibility criteria\n“Treatment” must be assigned\nOutcomes are beginning to be counted.\n\nLearning causal inference, we are used to getting datasets with nice and precise binary treatment variables and covariates. But where did these come from? People’s lives are not cross sectional, they are a trajectory through time. So there are lots of design decisions to be made in order to squeeze all of it into tabular form. Having longitudinal data makes it more possible, but not more easy. How do we decide how to assign a person into either treatment groups (or not at all if not eligible).\nGiven this presentation, you might see why active-comparator designs are so appealing. It is more straightforward to compare to active treatments, head-to-head, since we just define the first treatment initiation of each drug (the two groups) as our time zero.\nThe trickier part is when we want to design a study to compare treatment initators with non-users. This is often of interest in pragmatic trials. What is the index date for someone who just lived their life never getting treated? Non-initators have no point in time where they start treatment, making it harder to align this non-existing “treatment assignment” to the follow-up and eligibilty.\nThis often leads to comparing either persistent users or those who taken the drug at some time (i.e., all or any), to those who never used it. This study design does very little to inform physicians how to act on the patient they are currently facing. “Take this drug and if you survive the next 5 years (i.e., become a persistent user), then you will reduce your risk by x percent” does not inspire much confidence and is not very helpful for the patient who is living now in the present and not five years into the future.2"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#introduction",
    "href": "blog/2023/03-sequential_trial_design/index.html#introduction",
    "title": "Sequential trial design for causal inference",
    "section": "",
    "text": "I’m not special, so I’ve spent a lot of time addressing confounding bias in causal inference. I even created and still maintain an open-source Python package whose main focus is flexible causal inference modeling (pip install causallib). There’s a place to discuss why confounding bias is the most popular bias, but this blog post is not it.  I have a different bias I want to focus on.\nThere’s actually some compelling evidence (well, at least anecdotal) that confounding bias has less of a biasing effect than what one would expect. One example is Garcı́a-Albéniz, Hsu, and Hernán (2017) examining the effect of colorectal cancer screening on colorectal cancer incidence. It showed the same survival curves for treatment and control units when adjusting and not adjusting for confounding factors. This, in and of itself, is a poor evidence, but the exact same shape of survival curves also appeared in Bretthauer et al. (2022), an RCT examing the exact same question. This suggests that confounding bias did not affect the observational study.1\n\n\nThis bias originates from improperly setting the time-zero. Time zero (or “index date”) is the point in time which splits the baseline period from the follow-up period. History from future, retrospectively speaking. It usually the time in which treatment is initiated, and therefore the point in time from which we look backwards to obtain historical data (baseline covariates) and look forward to gather follow-up data (outcomes).\nAs such, to properly set up time zero, three things must align:\n\nThe subject must meet the eligibility criteria\n“Treatment” must be assigned\nOutcomes are beginning to be counted.\n\nLearning causal inference, we are used to getting datasets with nice and precise binary treatment variables and covariates. But where did these come from? People’s lives are not cross sectional, they are a trajectory through time. So there are lots of design decisions to be made in order to squeeze all of it into tabular form. Having longitudinal data makes it more possible, but not more easy. How do we decide how to assign a person into either treatment groups (or not at all if not eligible).\nGiven this presentation, you might see why active-comparator designs are so appealing. It is more straightforward to compare to active treatments, head-to-head, since we just define the first treatment initiation of each drug (the two groups) as our time zero.\nThe trickier part is when we want to design a study to compare treatment initators with non-users. This is often of interest in pragmatic trials. What is the index date for someone who just lived their life never getting treated? Non-initators have no point in time where they start treatment, making it harder to align this non-existing “treatment assignment” to the follow-up and eligibilty.\nThis often leads to comparing either persistent users or those who taken the drug at some time (i.e., all or any), to those who never used it. This study design does very little to inform physicians how to act on the patient they are currently facing. “Take this drug and if you survive the next 5 years (i.e., become a persistent user), then you will reduce your risk by x percent” does not inspire much confidence and is not very helpful for the patient who is living now in the present and not five years into the future.2"
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#sequential-trial-design",
    "href": "blog/2023/03-sequential_trial_design/index.html#sequential-trial-design",
    "title": "Sequential trial design for causal inference",
    "section": "Sequential trial design",
    "text": "Sequential trial design\nSo how can we force the alignment of eligibility, treatment initiation, and follow-up into a proper time zero?\n\nA single “sequential” trial\nOne simple approach is to use calendar time. Set the time-zero to a specific date. Say, for instance, January 1st, 2020. This will be the point in time splitting history from future. Keep whoever is eligibile to participate at this date, discard the rest; data from before this date are the baseline covariates; data from this date forward is the follow-up; and whoever got treated on this date is considered in the treatment group, the rest are controls.\nIndividuals treated before January 1st are probably ineligible (most study designs might enforce participants to be treatment naive). Individuals treated after January 1st are considered controls, because time-zero is set at January 1st, not in the future. Though a per-protocol analysis might decide to censor them from follow-up because they deviated from their original (control) assignment. This creates a design that answers a question of treat now vs. don’t treat now (but possible treat later).\nHowever, this single trial is too naive and not very efficient. We miss all the treated individuals in the past and in the future. There might even not be anyone treated on January 1st. We throw away a lot of information. That’s a big blow to statistical power.\n\n\nA sequence of sequential trials\nTherefore, a natural extension would be to just repeat the process. Repeat it for January 2nd, 2020, for January 3rd, 2020, for January 4th, and so forth. For each time point we:\n\nConsider whoever meets the eligibility criteria and discard the rest,\nSet the treatment indicator based on who got treated,\nExtract baseline covariates prior to that time point,\nExtract outcome information from that time onwards.\n\n\n\n\nAn illustration of the sequential trial process at a data-set level3. At each time \\(t\\) (starting with \\(t=0\\), like Jan 1st), we check whoever is eligible and assign them to their appropriate treatment group according to their treatmeent status at that time. At the next time step (\\(t=1\\) or Jan 2nd), the pool of eligible individuals can change in several ways: 1) treated individuals from the past are probably no longer eligible, at least not before some washout period passes; 2) eligible people can become ineligible regardless of their past treatment status (e.g., they can turn 65 and no longer fit the inclusion criteria). 3) Likewise, ineligible people from the past can become eligible if they suddenly fit the criteria (e.g., an individual can turn 18 at this time and become eligible). At each time we collect baseline and follow-up data according to the study design relative to this index date. This process repeates for all available time steps.\n\n\nThe way I imagine this process is that we have stencil-like mold, defined by our eligibility criteria, from which we funnel observations like a strainer. We go to January 1st, put the stencil-strainer, shake the database and filter only eligible individuals at the time. We then go to January 2nd, filter; and so on and so forth, rinse and repeat. A moving window through time, only that window is specially shaped by the eligibilty criteria.\nThis means a single person can have multiple records (entries, or rows) in the dataset. One for each trial (time-zero).\nWe can see this illustrated below with a person’s timeline and its corresponding person-time tabular form. In this study design, baseline covariates are taken 4 time-steps relative to time-zero (e.g., 2 years prior to time-zero), and follow-up is taken 4 time steps starting time-zero (e.g., outcome within 2 years). Thus, the covariate profile changes as time-zero progresses. At times 4-7, 🏊‍♂️ is included as it is within the baseline period dictated by the study design, but it is no longer counted in time 8. Similarly, the outcome 🚑 is not observed at time 4, only starting at time 5.4 To make things more explicitly, I also indicate eligibility; in this example, the study design requires participant to no been treated in the last 4 time-steps (treatment-naive), and so at times 7 and 8 the person is no longer eligibile as they were treated in time 6.5\n\n\n\nAn illustration of a person timeline and how its corrsponding person-time tabular form. It shows multiple time-zeros set from time 4 throughout time 8 (think Jan 1st to Jan 4th from above). Different time-zeros have different covariate (🏊‍♂️) profile, different outcome (🚑) based on the moving follow-up window, and - although that person is treated (💊) - they are not always considered as such.\n\n\n\n\nAn uncontrolled control explosion\nOften, getting treatment is rarer than not getting treated. This imbalance is not inherently important, but can lead to some difficulties in practice. Looking at the resulting dataset above, we see that sequential trials exacerbate this even further since even treated individuals contribute (at-least more) non-treated rows, all the while never-treated individuals just multiply throughout time.\nThis control group inflation can be undesireable in practice. It increases demand of compute resources, both in terms of memory and runtime, while not necessarily contributing enough information to justify it. Statistical power is determined by the smallest group, anyway; and, while I don’t know how to formally extend this argument into a repetaed-measurement setting, I have an intuition that dozens and dozens of duplicate (and near-duplicate) records hit the diminishing returns curve somewhat quickly.\nThere are several valid strategies to deal with settings where eligibility criteria is met multiple times per person and lead to this cambrian-like explosion of controls records.\nFirst, we can just randomly subsample a fixed sample (e.g., 10%) from all the controls records from our dataset (Dickerman et al. (2019)). Since this blindly regards person and time information, it can have some unintended consequences like discarding entire time steps or individuals by some unfortunate random chance. This can reduce variability in the dataset and hurt the infromation content of it.\nFortunately, this can be easily solvable by forcing some structure on the discarding, like stratified sampling by time-steps and/or person IDs.\nWe can take this structural subsampling a step forward and force a single entry per-person, i.e., reduce it to a single eligibile record per person. When doing so, it is important to reduce it without taking strict advantage of “future infromation”, namely by the treatment and outcome status of people’s timeline. Two common ways are taking the first eligibility record per-person, or taking one randomly (Hernán and Robins (2016)). Both are valid, non-biasing approaches, but they suffer from similar disadvantages presented above when randomly subsampling controls. Namely, they can discard more useful infromation and keep more redundant infromation instead. Specifically, they can blindly discard treatment records, which can be very precious. In reasonable cases where a treated person-time is not treated at first eligibile time, or in cases where it simply been randomly discarded. As said above, treated person-time records are few to begin with, and discarding them reduces the size of the treatment group, which decrease precious statistical power unnecessarily.\nA forth option that maximizes the size of the (smaller) treatment group is a combined approach: first, gather all the treated person-time records; and second, take first (or random) eligibility time per-person (Garcı́a-Albéniz, Hsu, and Hernán (2017)). This is basically a different view on the first approach. Both first fix (or take) all the treated person-times and then subsample just the control person-times. Only here, the subsampling is further structured to keep one record per person (including individuals who are at some point treated) by determinstic subsampling (taking records at first eligibilty time) or random subsampling (taking a random record from all eligible times of an individual).\n\n\n\nIllustration of the four strategies for dealing with the inflation of control person-times in multiple eligibility settings. It describes 3 people: A, B, and C and their treatment status over 5 time steps. First, using all person-times6 from which we can derive the rest. Second, first eligibilty only, where B1, A1, and C2 participate (note C1 is ineligible in this example). Third, a random eligibility time per person. Note that in both cases we lose A3 as a treated unit. Forth, a mixed approach where we take all treated person-times and 1st eligibilty time. Lastly, a fifth strategy taking all treated person-times and randomly sampling a fixed proportions from the controls is not showed.\n\n\nIn the mixed approaches, it is important to choose control person-times while ignoring treatment indication in a person’s timeline. For instance, in the example above, it might be tempting to discard control trials of person A, because person A is eventually treated. Or, put differently, after selecting all the treated person-times, select a subsample of person-times controls while excluding people chosen in the treated person-times. However, this can bias the analysis as it will create a control group that is, by construction, never-treated; and never-treated might not be a comparable control for incidence users we care about (Garcı́a-Albéniz, Hsu, and Hernán (2017)).\n\nContemporary controls vs. self controls\nAs mention above, using all person-times can create computational strains and inefficiencies. All while in single-eligibility designs we might discard precious information. Mixed approaches where we first select all treated person-times is a good middle ground solution, but there is no free lunch.\nWe originally perform a trial at each time step. Looking at each trial independently, we adjust for time confounding as each treated person at that time has corresponding contemporary controls. However, when we dillute the controls (and the mixed approach of all-treated plus 1st eligibilty time is the most extreme approach), we reduce the weight of those contemporary controls.\nInstead, vieweing the dataset as a whole, we increase the weight of self controls. This is because for each person treated not at first eligibilty time we will have, by construction, two instances - one as treated and one as a control. When we discard records of other people from that time step, because this is not their first eligibility time, we reduce the relative weight (number) of contemporary controls and, by definition, increase the relative weight of the first eligible control person-times of those later treated.\nTo simplify with the example above, in the all-person-time approach, A3 is controlled by both its past self (A1) and a contemporary record (C3). When we go to the mixed approach, we drop C3, making A1 the sole control for A3. Therefore, we reduced the relative weight of contemrporary controls (at time 3) and increased the relative weight of self-controls.\nThis can create bias if person \\(i\\) at time \\(t\\) is not exchangeable with itself (person \\(i\\)) at time \\(t^*\\).\nThis is quite reasonable to assume, as people’s condition might trend over time, like disease progression status. Furthermore, if this progression is not captured by the covariates, it can create confounding bias (time confounding). Therefore, another cosideration should be to check for such cases, and, following footnote 7, keep trial ID in mind when adjusting and make sure every time-step can act as a quasi-independent trial."
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#discussion",
    "href": "blog/2023/03-sequential_trial_design/index.html#discussion",
    "title": "Sequential trial design for causal inference",
    "section": "Discussion",
    "text": "Discussion\n\nImmortal (bias) Combat\nImmortal time bias is when participants of a cohort cannot, by construction, experience the outcome during some period of the follow-up time8. It usually happens when the researcher fails to align the cohort entry time (eligibility time) with the time of getting the treatment (time zero). If the researcher classifies that individual as treated, and start counting the outcome from cohort entry - even though they are not treated at the start of eligibility - it necessarily means that that person did not (and couldn’t have) experience the outcome in the time period between eligibility and time of actual being treated. If you are not familiar with immortal time bias, I suggest going over its entry in the Catalog of Bias website.\nOne approach to overcome immortal time bias is to do a time-dependent analysis. Namely, we need to somehow associate the time between eligibility and treatment with the control group, and only the time between treatment and outcome (or end of follow-up) with the treatment group. It can be done in several ways, depending on the analytical approach, like using an offset in a Poisson risk-rate regression. But one generic solution is to work in person-time data. Basically, allow the person-time records between eligibility and treatment to be classified to the control group, and the person-time records between treatment onwards to hte treatment group.\nThis general solution is essentially what happens in the sequential trials design. Although the proposed solution is person-centric – assigning the person’s time to separate treatment groups – and sequential trial is cohort-centric – assigning a person to its appropriate group at each time-step. However, the resulting view is basically the same, as the end product is a table where that same person is classified to the control group in the period starting at its eligibility time until they are assigned a treatement and move to the treatment group.\nThe only nuance is in how we construct the outcome in the follow-up time. For sequential trials to be absolutely equivalent to the person-time analysis, the control person-times should be censored at the time that person receive treatment. This ensures the outcome contributed to the control group is solely for the period that person is considered untreated. This censor-at-change is also known as a per-protocol analysis, where we censor individuals who deviate from their originally assigned protocol (controls who eventually get teated).\n\n\nMeta-analysis: pooling estimates vs. pooling records\nIn the sequential trial approach we essentially create multiple studies. Meta-analysis is the science of synthesizing evidence across multiple similar studies. There are two main approaches for doing meta analyses. The classic approach would use aggregate summaries from different studies, like the estimated effect and confidence interval, and combine them into a single estimate. However, advances in data management and digital communication made it increasingly feasible to share the original data on individual participants and give rise to a second type of meta analysis: one that combines individual-level data, basically consolidating datasets (Riley, Lambert, and Abo-Zaid (2010), Lin and Zeng (2010)).\nSequential trial design results in a bunch of trials, and, therefore, these two approaches are applicable to analyzing sequential trials. Either estimate the effect from each trial separately and then synthesize the estimated effects. Or just consolidate the extracted person-time records over the separate trials to obtain one big dataset.\nThere seems to be some benefits for doing individual-level meta-analysis (Riley, Lambert, and Abo-Zaid (2010)), but I’m not expert enough to judge. However, since we extract all the trials from the same database, using the same study design with the same covariates, treatment and outcome defintions, it seems even more appropriate (or natural) to consolidate the raw data. This post even hints to this approach, as this, to the best of my knowledge, the only way sequential trials have been analyzied in the epidemiological literature. However, it is also worth putting the finger on the fact that this is essentially becomes a meta-analysis, and as such, other approaches are applicable.\nThe individual-level analysis should respect the person-time structure of the combined trials. They might apply a multilevel (hierarchical) model, partially pooling estimates across time (trials). They should defintely use robust variance estimation, either by sandwich (HC) estimates or bootstrap, to estimate standard errors since the records in the consolidated dataset are no longer independent but are repeated measures of the same pool of individuals.\n\n\nLimitations\nI have already mentioned the additional computational strain caused by the inflation of the dataset moving from person to person-time format; how control-dilution can hurt exchangeability; and the additional considerations that are required in computing standard errors as the rows of this dataset are no longer independent.\nHowever, in my opinion the biggest limitation of this method is that it requires the analysts to discretize time from the get go. The granularity of the time-steps is not just predefined, but is baked into the earliest point in the analytic pipeline - the data extraction from the database. As such, it means a sensitivity analysis on this time-step parameter is laborious as it requires applying the entire pipeline from data extraction to effect estimation.\nThis is problematic as the time granularity has both computational and practicical implications. Ideally, the time-step size should not be too small to avoid creating too redundant repeatitions. Too small and the size of the expanded person-time dataset will be too large and incur heavy computational costs (but may also cause floating-point issues as some survival models multiply probabilities [fractions] over time-steps, so too many time-steps may become numerically unstable). On the other hand, too large time-steps can bunch together too much information (like treatment assignment and outcome) and reduce information content of real treatment-outcome associations. The size of the time-step should be clinically (or practically) meaningful, but it’s ok to find a more proper value around it that also optimizes computational aspects. This is computationally harder in sequential trials since the size of the time-steps is baked deep into the construction of the analytical datasets.\n\n\nSummary\nSequential trials design is a generic approach that can solve many time-related biases (Hernán et al. (2016)). This was a high-level introduction, which I think is missing from the methodological literature, but I also hope it entails some interesting point of views."
  },
  {
    "objectID": "blog/2023/03-sequential_trial_design/index.html#footnotes",
    "href": "blog/2023/03-sequential_trial_design/index.html#footnotes",
    "title": "Sequential trial design for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA similar phenomenon was also observed in Hernán et al. (2008), finding similar hazard ratios (in terms of point estimation and confidence intervals) between adjusted and unadjusted estimates, suggesting the cause of discrepency between previous observational studies (even using the same data as they did) and RCTs was not confounding bias but rather the time-zero-related biases that are the subject of this blog post.↩︎\nFurthermore, I believe this discrepancy in how to define treatment groups, namely, how to define the \\(1\\) and \\(0\\) in \\(Y^1\\) and \\(Y^0\\), boils down to the causal consistency assumptions. Defining a poor treatment mechanism in the study design means the outcome one observes in their data (\\(E[Y|A=a]\\)) will not equal the hypothetical outcome they care about (\\(E[Y^a]\\)).↩︎\nThis also illustrates why sequential trials may be sometimes called “nested” trials. This is because, in some common and simple settings, the trial at time \\(t\\) (eligible people at time \\(t\\)) are nested within the controls of time \\(t-1\\). However I think “nested trials” has a different meaning in the RCT landscape, so “sequential trials” is both more general and non-taken.↩︎\nIn time-to-event settings, the follow-up window is often as long as can be possibly observed, avoiding explicitly stating a follow-up timeframe. In such scenarios, we can see the time to outcome (🚑) decreases as time-zero progresses. ↩︎\nFirst, note that they might also be inelgible because they don’t have sufficient follow-up. Second, that person may also be ineligible at time 10 as they are no longer outcome-naive, too.↩︎\nNote time 2 (Jan 2nd) may be redundant as it containd only control units. If time has some effect — i.e., people at time \\(t\\) might not be fully exchangeable with people at time \\(t^*\\) — then not having any comparable treated units at time 2 might violate a positiity (overlap) requirement. Thus time steps with all-controls may be further discarded in the preprocess step (drop the rows) or in the analytic step (e.g., apply exact matching on the trial ID [time step] and consequently discard such records).↩︎\nNote time 2 (Jan 2nd) may be redundant as it containd only control units. If time has some effect — i.e., people at time \\(t\\) might not be fully exchangeable with people at time \\(t^*\\) — then not having any comparable treated units at time 2 might violate a positiity (overlap) requirement. Thus time steps with all-controls may be further discarded in the preprocess step (drop the rows) or in the analytic step (e.g., apply exact matching on the trial ID [time step] and consequently discard such records).↩︎\nIt is called “immortal” since often that outcome is death. So if one cannot experience death for a period of time, they are basically immortal.↩︎"
  },
  {
    "objectID": "blog/2023/01-causal-inference-is-a-mindset/index.html",
    "href": "blog/2023/01-causal-inference-is-a-mindset/index.html",
    "title": "Causal inference is a mindset",
    "section": "",
    "text": "I’m a causal inference enthusiast, practitioner, and advocate (borderline missionary if we’re completely honest). I believe people are causal thinkers who ask causal questions and expect causal answers; that we understand the world through causal relationships and causal mechanisms. But when people come to me for help or education, they usually end up disappointed. They come for a solution and discover more questions. They come for a code library to import and end up with a study design to implement.\nThis is because causal inference is first and foremost a mindset, not a particular tool. There is no neural network architecture that will just spit out causal effects. On the contrary, if you organize your data properly, causal inference can be a mere average or a simple linear regression at most.\n\nExcellent study designs make simple analytics.\n\nThis is why causal inference is a bait and switch scheme. We talk a lot about causality but then provide associational statistical tools. And once we estimate a statistical association, claiming it is causal is actually a leap of faith. Of course we have mathematical theory to ground on, but its assumptions are untestable. So at the end of the day, causal claims from observational data are an educated leap of (rational) faith.\n\nCausation = Association + Logic\n\nCausation consists of identification and estimation. Deep learning driven thinking has somewhat sidelined practitioners from thinking of identification. We rarely consider whether the question is even solvable, or is it solvable with the data we have; and if so — what approach or tools can solve it. We easily throw it to a kitchen sink of neural networks to get an answer— we focus exclusively on estimation, forgetting some problems cannot be solved by a single prediction model. But estimation can get you no further than association, and without identification you can’t get causation. Causality requires embracing the beyond-the-data logic that transcends an association into causation, but it’s a hard pill to swallow— and even harder to convince others to join — in the current Big-Data♡AI climate.\nPeople expect a magic library to import in R or to conjure up a model in PyTorch like some causal alchemists. But then I shoot up my slides and start blubbering about causal roadmaps, target trial emulation, careful time-zero considerations and variable selection. People are so used to post-AlexNet machine learning approaches, they are baffled that I have so much to say about study design and DAGs.\nThis is not to say that causal estimation models are bad, on the contrary, they are great (there’s a good reason why I champion making flexible causal models approachable with causallib); they make the counterfactual prediction explicit and clear. This also doesn’t mean causal estimation is easy, far from it — I’m a strong proponent for sequeling The Book of Why with The Book of How — causal learning suffers from the same complexities machine learning does and then some. Also, my intention is not to bash deep-learning, which I myself sometimes use for causal effect estimation. But overcoming confounding bias given observed confounders is just one stair in the staircase towards claiming causation.\nIn the trenches, causality doesn’t always stand up to its hype because it is different from machine learning. The truth is that causal inference is more of a mindset, than a particular tool. And this truth is a tougher sell."
  },
  {
    "objectID": "blog/2021/02-why-balance-covariates/index.html",
    "href": "blog/2021/02-why-balance-covariates/index.html",
    "title": "Why we care for covariate balancing in comparative studies",
    "section": "",
    "text": "Comparative analysis is the scientific method for discovering effectiveness. Whether it is in medicine, economics, or elsewhere, we perform studies that collect information and compares two groups in order to see which is better and if we can learn any means to improve. These studies can be experiments where we control for the group assignment via randomization, or observational studies where participants self-assign.\nOne of the first tasks in analyzing data from such studies will be to compare the characteristics of the two groups. It isn’t called table one for nothing. In which, we summarize the distribution of variables between the two groups and might even perform a statistical test on how the groups differ. There’s a long-lasting debate on whether that’s a good practice or not. In general, biostatisticians tend to avoid it, while economists tend to prefer it. The recent Data Colada analysis of Dan Arielly fraudulent data — which part of the evidence was big baseline differences albeit the randomization — reopened this Pandora box, pitting academics against one another on Twitter.\nIn this post is my theory on why we compare balancing in baseline characteristics between groups.\n\nHow we infer causality\nFormalizing causality is a task so hard, it left philosophers baffled to this day. In the Rubin’s causal model, we have two unobserved quantities: \\(Y^1\\) is the hypothetical outcome a person would have had — had he been assigned a treatment, and \\(Y^0\\) is the hypothetical outcome that that same person would have had had he not been treated. Once we have these two hypothetical trajectories, two parallel universes in which everything is equal except the treatment assignment of that person — we can isolate the impact of the treatment. If we observe the outcome in both worlds, we can see the causal effect of the treatment — the changes in outcomes that were caused only due to the treatment.\nHowever, in reality — outside the Marvel Cinematic Universe of Rick and Morty — we don’t have access to this multiverse. People either get treated or they don’t.\nAnd so, because we can’t both give and not-give a treatment to the same person, the next best thing is to give a group of people the treatment and another group a placebo and compare the two groups. And this is where we start regarding group balancing. It is only but intuitive that we will want the groups to be identical in all regards, except to the treatment assignment, so we can isolate the contribution of the treatment, and cancel out changes in outcomes that may be due to differences in baseline characteristics.\n\n\nComparing factors is theoretically redundant\nHere, I said it. There are no theoretical justifications to compare the distribution of covariates between groups.\nStatistical theory of causal inference lays out three conditions in order to obtain causal effects from data: positivity, consistency, and exchangeability/ignorability. The crux of the argument considers the latter.\nIntuitively, the exchangeability assumption captures the notion that there shouldn’t be any systematic differences between the groups.\nMathematically, the exchangeability assumption regards the independency of the potential outcomes from the treatment assignment: \\(Y^0, Y^1 \\perp \\!\\!\\! \\perp A\\) .\nYou see what’s in there? This means thatit is the distribution of potential outcomes that should be balanced between groups, not the baseline factors!\n\n\nWhy we still do it\nIn case you missed it, the necessary exchangeability condition above regards potential outcomes — a hypothetical quantity we can't really observe.\nAnd if we can't observe it, we can't test to validate whether it holds in any particular case.\nHowever, we do have some prior understanding of how the world works. For example, clinicians may know that the risk for stroke is dependent on one's age, sex, and history of blood pressure; economists may know that one's salary depends on their past salaries and education level. Therefore, we are able to, at-least mentally, construct some function from baseline factors to the outcome. In turn, this means that if the variables \\(X\\) are balanced between groups then \\(f(X)\\) (which is some projection of them onto a scalar) is also balanced between groups, which suggests the potential outcomes are balanced between groups.\n\n\n\nA mental model of how we move from the available balancing of covariates to the desired balancing of potential outcomes. Because the outcome is a function of the covariates, we are willing to hypothesize that balanced covariates are likely to result in balanced potential outcomes.\n\n\nTesting for covariate balancing is an educated mental shortcut we do. A sort of an ambiguous control test — good balancing is reassuring but not definitive, bad balancing is alarming but not devastating. We just grasp on whatever data we can observe (covariates) and try to use our expertise, common sense, and logic to project it onto what we can't observe (potential outcomes).\n\n\nConclusions\nCovariates will be balanced due to randomization, but it is not the main reason why we randomize. Balanced covariates are just a convenient side effect while we try to balance the distribution of potential outcomes.\nTherefore, it is important to understand that testing for covariate balancing is not the end goal. It is, at best, a proxy for what we really want to know but cannot have. And being the humans that we are — instead of doing nothing about it, we try doing our best.\n\n\n\nAppendix: what can we benefit from covariate balancing tests and at what cost?\nI hope this explained why a debate exists regarding balancing tests. There's no factual right or wrong, only a debate on whether that mental leap from covariates to potential outcomes is uncalled for or not.\nIn my opinion the criticism against it is valid: First and foremost, variables must not be balanced — it the potential outcomes that should. Second, even if they are not — this is what statistical adjustment is for. Employing statistical tests in randomized data is even more baseless — there are no two population, just groups randomized from the same population — the null hypothesis is known to be true. Besides, for a large enough sample even meaningless small differences can become statistically significant, but it doesn't mean they are of practical importance. And for large enough number of covariates, some may pass the statistically significant threshold simply by chance of sampling (a false-positive, or type-1 error), rather than inherent difference in the characteristics between groups. Therefore, having statistical tests (and p-values) in a table one may be considered redundant or even counterproductive — causing confusion by focusing readers — not all of whom are statistical experts — on unessential information.\nHowever, the criticism for it is also valid: randomization might not be that simple to pull off (especially blindly, across multiple facilities, etc.) and humans do tend to err whenever they have the chance to. What guarantees do we have randomization was applied correctly? Do we blindly believe the process? Shouldn't we validate it? Unfortunately, verifying random assignment is near-impossible due to, well, the randomness inherent to the process. There are explicit tests for randomness but checking for balancing is a simple one to employ. Again, it can only hint a problem exist, flagging out more attention is deserved, but it cannot be definitive.\nTherefore, in my opinion, when working on randomized data, it's ok to test for balancing as a sanity check, but don't show it off (and if you do, bury it in an appendix where no one will read it anyway. Just like this section. Consider commenting with a penguin if you do?). It can be misleading at worse or be an avenue for misguided criticism at best (don't feed the trolls)."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#background",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#background",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Background",
    "text": "Background\n\nThe fundamental problem of causal inference\nEvaluating causal inference models is literary impossible. Few scientific concepts are so pompously named — yet accurately describe the gravity of an issue — as the notorious “fundamental problem of causal inference”.\nBriefly, the prediction task in causal inference is different than that of supervised machine learning (ML). While in ML we interpolate the target to new unseen samples, in causal inference we extrapolate the target from units in one group to units in the other group. Because in any given time a unit can only be in one group and not the other (e.g., you either have received a drug or you haven’t), we lack the ground-truth labels to compare against our predictions. Counterfactual outcome prediction cannot be derived like regular supervised prediction, nor can it be evaluated as one.\n\n\nCausal models as meta-learners\nMost causal inference algorithms usually have some machine learning core — a statistical model that predicts the outcome or treatment assignment. Once a mapping between features to targets is obtained, causal models can then have various ways to indirectly apply those statistical predictions to obtain a causal estimate.\nFor example, inverse probability weighting (IPW) is a causal model that estimates the causal effect by first modelling the treatment assignment. It takes any machine learning classifier that can also output a continuous score between 0 and 1 and assume it to model the probability of being treated: \\(\\hat{p}=\\Pr[T|X]\\). It regresses the binary treatment assignment (\\(T\\)) against the features (\\(X\\)), then takes the inverse of that predicted scores and use them to create a weighted average of the outcome.\nHaving this machine-learning backbone allows us to interrogate it using commonly known metrics from machine learning; and just like IPW adjusts a binary classifier to obtain a causal estimate, we can adjust these ML metrics to obtain a causal-inference-relevant view.\nThis post is a an effort to breakdown a larger manuscript into bite-size chunks, and will focus on what ROC curves can tell us about propensity models.\n\n\nROC curves recap\nClassifications models can be evaluated for their calibration — how well they behave as probability models — and for their discrimination — how well they separate positive from negative examples. AUC is a metric for discrimination. A more in-depth overview is slightly out of scope for this article, but I do want you to keep in mind two ways for generating ROC curves from a list of prediction scores and labels.\nFirst view is the naïve one. For each possible threshold we will calculate the true-positive and false-positive rates, plotting that point in ROC space. Note that the TPR and FPR can be affected by the weight each unit contributes to the classification, which is not necessarily 1.\nSecond view is more computationally efficient. It involves sorting the scores and traversing the list such that each positive unit moves you one step up and each negative unit moves you one step right. The size of the step is correspondingly determined by the fraction of positive and negative units, but we can weigh each unit so that the step size changes arbitrarily.\n\n\n\n\nObtaining an ROC curve from scores. On the left (a) an explicit view of threshold (taken from Dariya Sydykova). One the right (b) a computationally efficient view (taken from ML wiki)\n\n\n\nIn our case, the prediction scores are the propensity estimations (probability to be in the treatment group) and the labels are the actual treatment assignment. Moreover, controlling the ROC space through sample-weighting is the basis for the additional ROC curves to be presented."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Classification metrics for propensity models — overfit, underfit, and positivity violations",
    "text": "Classification metrics for propensity models — overfit, underfit, and positivity violations\nComing from machine learning, this can be somewhat counterintuitive, so let’s get done with it right out of the gate: good prediction performance usually suggests a bad propensity model and a bad causal model downstream. Propensity scores should not be able to discriminate well between the treatment and control units.\nIf you’re lucky, your good prediction performance is due to good-old overfit. You can use your ML knowledge to solve for that. Causal inference models are prone to all the same pitfalls in statistics, they are simply blessed with a few additional ones.\nIf you’re not lucky, your good discrimination ability may hint you have a positivity violation in your data. Positivity is an essential assumption if wanting to extrapolate outcomes across treatment groups, as in causal inference. It states that the treated should have some chance (i.e. positive probability) to be in the control group and vice versa. In other words, the groups should have some common support — in each subspace of features we should have both treated and control units, so both groups have their covariates overlap. Otherwise, how could you generalize the predicted outcome from the treated to the control if all treated units are males and all control units are females? Perfect discrimination between treated and controls suggests the groups occupy mutually exclusive regions in feature-space violating a necessary assumption for causal inference.\nConversely, bad discrimination performance is not necessarily bad. It might simply suggest the treatment groups are well mixed — an encouraging step towards the validity of a causal analysis. However, it might also be due to underfit. The response surface of treatment assignment might be a complex one to model. Therefore, you should experiment in iteratively increase the expressiveness of your model to the point you overfit just to verify it is indeed the data that is balanced and not the model that is under-specified.\nSolving for lack of overlap is possible, but out of scope for this post. Just to namedrop a few strategies: you should revise the inclusion criteria of your data, rethink your confounder selection, stratify your analysis on highly predictive features, or use domain knowledge to thoughtfully help you extrapolate through mechanism rather than data.\n\nROC curves for propensity models\nFocusing on propensity-based causal models, we have three relevant ROC curves: the regular one based on propensity scores and two novel curves created by reweighting the scores. They all work in tandem, and I’ll present each one: how to obtain them and how to interpret them.\n\nVanilla ROC curve\nHow: This is the regular ROC curve simply obtained by taking the propensity scores against the binary treatment assignment.\nroc_auc_score(t, p_hat)\nInterpretation: We already discussed the issue that the AUC should not be too high as it suggests good discrimination, which is bad for causal inference. The ROC curve allows us to detect such regions of perfect discrimination. Ideally, there should not be long vertical or horizontal segments in the curve. A sharp long vertical contour suggests there’s a bunch of data points for which we only get true positives (upward movement) without paying any false negatives (rightward movement). That is, the treated units are very separable from the untreated — they are not well-mixed. Reiterating the above: this can hint that we have a positivity violation in the feature subspace that is mapped into this region of scores (thresholds) causing the vertical line.\n\n\nIP-weighted ROC curve\nHow: in this curve we weight the contribution of each unit’s propensity score to the ROC curve by the corresponding inverse-probability weight of that unit.\nip_weights = (t / p_hat) + ((1 - t)) / ((1 - p_hat))\nroc_auc_score(t, p_hat, sample_weight=ip_weights)\nInterpretation: Ideally, like every post-weight discrimination metric, we should expect a random-like performance. Namely, a ROC curve that aligns with the diagonal and an AUC around 0.5.\nIntuition: This curve shows how well the weights balance the groups. IPW creates a pseudo-population in which the treated and control have similar characteristics — it weighs the sample so that in each region in the feature-space we should have similar amount of (weighted) units. If we were to apply a classifier in this weighted population, it would be difficult to discriminate the treated from the controls. For example, if we have the same amount of males and females we can’t use sex as a predictive feature, and if we have the same amount of young and adults we can’t use age, etc. Therefore, poor discrimination post-weighting is welcomed.\n\n\nExpected ROC curve\nHow: We obtain this curve by weighing the scores such that each unit contributes its propensity score to the positive label (treatment group) and its complementary score (1 minus propensity) to the negative label (control group)\nweights = concatenate([p_hat, 1 - p_hat])\nt = concatenate([ones_like(p_hat), zeros_like(p_hat)])\np_hat = concatenate([p_hat, p_hat])\nroc_auc_score(t, p_hat, sample_weight=weights)\nInterpretation: Ideally, we would want the expected propensity to align with the vanilla (unweighted) propensity curve (and have same AUC).\nIntuition: The propensity-to-be-treated is never observed, we only see one instantiation of it in the form of treatment assignment. However, we can model the average propensity of units with similar features. If we assume the statistical model represents the true propensity, then we move from a binary classification task to a smoother calibration-like task where units with high confidence (extreme propensity) contribute almost like they would in the vanilla ROC curve, and low-confidence units (propensity around 0.5) contribute a segment parallel to the diagonal.\n\n\n\nA view of the propensity ROC curves. Blue: the unweighted propensity score. Orange: the inverse-probability weighted curve of the propensity. Green the Expected propensity curve ."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Connection to propensity distribution plots",
    "text": "Connection to propensity distribution plots\nTraditionally, practitioners will plot the propensity distribution, colored by the treatment and control groups, and look for overlap. ROC curves are another view of that propensity distribution.\nThere is a direct transformation from scores distribution to ROC curves, as seen in the figure below taken from Janssens and Martens.\n\n\n\n\nTransforming a distribution of (propensity) scores into an ROC curve [figure from Janssens and Martens].\n\n\n\nAnd the gif below from Dariya Sydykova show how separability of scores affect how sharp the curves are.\n\n\n\n\nThe effect of separability of (propensity) scores on the sharpness (i.e. amount of long vertical/horizontal segments) of the ROC curve [figure by Dariya Sydykova].\n\n\n\nFollowing this perspective, the propensity histogram weighted by the inverse propensity serves the same purpose. The bar heights are no longer determined by the number of individuals in each bin, but by their accumulated weights. In the weighted scheme (right), the bars corresponding to the same propensity bucket (i.e. x-axis bin) have the same height in the treatment and control groups, relative to the unweighted version (left) in which the heights of the same bins differ.\n\n\n\nInverse-probability-weighted propensity histogram (right) has corresponding bars slightly more similar in height then the regular (unweighted) propensity histogram (left).\n\n\nHowever, I would argue that viewing this in ROC space provides an easier interpretation, since we can convert the fuzzy notion of “distribution overlap” to a concrete AUC score."
  },
  {
    "objectID": "blog/2020/03-ml-metrics-for-causal-inference/index.html#summary",
    "href": "blog/2020/03-ml-metrics-for-causal-inference/index.html#summary",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Summary",
    "text": "Summary\nWe have seen how to interpret pre-weighting classification metrics (good performance is bad) and post-weighting classification metrics (bad performance is good).\nI focused on ROC curves for propensity models, presented two novel curves and discussed how to interpret them. \nHere are three take-aways for three curves:\n\nRegular ROC curves should not have sharp, long vertical segments.\nInverse-probability weighted AUC should be around 0.5.\nExpected AUC should be close to the regular AUC.\n\nThese presents an off-the-shelf intuitive measure to verify a causal model is not omitting complete nonsense. Using such simple AUC-based criteria can be implemented to automatically select causal inference models that perform better than others through cross-validation, similar to how we apply model selection in machine learning.\nI believe that deploying a propensity model and examining its behavior is beneficial in any causal inference analysis. Even if you end up modeling the response surface directly without using the propensity scores, it can still provide meaningful insights into the structure of the data and the assumption needed for a valid causal conclusion.\nFor additional thoughts and evaluations, see our preprint: https://arxiv.org/abs/1906.00442."
  },
  {
    "objectID": "blog/2020/01-simpsons-paradox-ipw/index.html",
    "href": "blog/2020/01-simpsons-paradox-ipw/index.html",
    "title": "Solving Simpson’s Paradox with Inverse Probability Weighting",
    "section": "",
    "text": "Originally published on  Medium.\n\nStatisticians love using the word “paradox” to describe simply unintuitive results, regardless of how much it upsets their fellow logicians. To get back at them, we’ll apply causality to solve one of their most famous paradoxes — Simpson’s Paradox.\nIn this post, I will briefly introduce what IPW is in the context of causal inference and present a simple intuition to how it works. I will then provide a popular example of the paradox from the medical domain and we’ll see, visually, how IPW solves it.\nWant to skip the details? Scroll to the end of the article.\n\nCausal Inference with IPW\nIPW, short for Inverse Probability (sometimes Propensity) Weighting, is a popular method for estimating causal effects from data. It’s a simple yet powerful tool to eliminate confounding or selection bias. To keep it casual (ha!), let’s introduce it via a simple hypothetical example of estimating the causal effect of drug \\(D\\) on the risk of stroke.\nFor that, we’ll make a quick detour through randomized control trials.\n\nRandomized Control Trials\nUsually, to estimate the causal effect of a drug, we would construct a Randomized Control Trial (RCT). This means, we would recruit people and flip a coin to assign them either to take drug \\(D\\) or to take placebo (control). Then we would measure the rate of stroke in each group, compare them, and conclude whether \\(D\\) increased or decreased the risk of illness.\nWe know that there are multiple contributing factors to stroke. For example, sex (vascular systems can be differentiated between males and females) and age (veins tend to clog with time). The reason we could simply compare the two groups and disregard those other factors has everything to do with the randomization we applied.\nThe randomization creates a similar distribution of age and sex between the two groups (on average). Consequently, when comparing the groups, the contributions of those variables cancel themselves out. The only parameter consistently different between the groups is whether they took \\(D\\) or placebo, and therefore, the differences we observed in the risk can be contributed only to \\(D\\), making them the causal effect of \\(D\\).\nWe can decompose the risk of stroke in our simple example into a slightly more concise mathematical notation:\n\\[\n\\begin{array}{c}\n\\text{risk stroke in treated}=\\text{risk due to age} + \\text{risk due to sex} + \\text{risk due to }D \\\\\n- \\\\\n\\text{risk stroke in control}=\\text{risk due to age} + \\text{risk due to sex} + \\underbrace{\\text{risk due to placebo}}_{=0} \\\\\n= \\\\\n\\text{risk due to }D\n\\end{array}\n\\]\nWe compare the risk between between our groups by taking the difference.\nSince age and sex are distributed similarly between groups, they contribute the same risk in both groups and so they cancel themselves out. Since placebo is a small sugar pill, its contribution is zero. Hence, we are left only with \\(\\text{risk due to }D\\).\nVoilà! The causal effect of \\(D\\) on stroke.\n\n\nCausal Effect from Non-Experimental Data\nHowever, performing a randomized control trial costs money and takes a long time (among other disadvantages). Still paying our student-loan, we don’t have the resources to conduct such an experiment. What we do have is data, because data is becoming ever cheaper and HMOs collect them easily.\nThe problem with such observational data from HMOs is that it no longer comes from our nice experimental distribution. Unfortunately for us (but really luckily for us), physicians are not random. They assign treatments based on our characteristics (say, age and sex). Therefore, there might be an overall tendency to prescribe certain groups with one drug and not the other. In this case, if we were to simply compare those who did take D with those who did not, the distribution of those factors will not necessarily be the same and so their contribution will no longer cancel out.\nConsequently, the “effect” we’ll observe will no longer be the causal effect of D, but rather a quantity entangling both causal and non-causal impacts, essentially contaminating the causal effect of D with the contribution of those other factors.\nTo estimate the true causal effect, we’ll first need to make the two groups comparable, and the way to make them comparable is where causal inference, and specifically IPW, comes into play.\n\n\nInverse Probability/Propensity Weighting\nNow that we have set the scene, we can finally present what IPW is. As we said, IPW stands for Inverse Propensity Weighting. It’s a method to balance groups by giving each data-point a weight, so that the weighted-distribution of features in first group is similar to the weighted-distribution of the second one.\nWe mentioned that physicians don’t prescribe drugs randomly, but rather base it on the features of their patients. Therefore, each patient will have a different likelihood to be prescribed to D, based on their characteristics. This likelihood is referred to the propensity to be treated. To put it mathematically, if we mark our patient features as X, the propensity is the probability of patients getting or not getting the treatment: \\(\\Pr[D|X]\\). Once we estimated this probability to treat, the weight we assign is simply its inverse: \\(1 / \\Pr[D|X]\\).\nWhen we have a large number of features, we will need some machine learning model to crunch all those high dimensional data into one probability scalar. But in order to see why this process even results in a balanced population, let’s take the simple example with one feature, say being an adult male.\n\nIPW by a Simple Example\n\n\n\nOur original population is unbalanced because we have more untreated adult males than treated ones. If we were to compare the groups, we wouldn’t be able disentangle the contributions of drug and sex, and tell whether the observed effect is due to being treated or due to being male.\n\n\nExamining the distribution in the above figure, we see that our groups are imbalanced with regard to males. Therefore, if we were to simply calculate an average risk in each group, we would not be able to say whether the difference we see is due to being treated or simply because of being a male.\nOur first step is to calculate the probability of each individual to be in the group they are actually assigned to. We have 5 men in total, 1 treated and 4 that are not. Hence, we can make a simple estimate that the probability for males to get the drug is ⅕ and the probability for males to not get the drug is ⅘.\nOur second step is to inverse those probabilities and assign them to each individual. Homer, therefore, getting a weight of 5, while Moe, Barney, Groundskeeper Willie, and Principal Skinner each get a weight of 5/4. We basically create a pseudo population where we have 5 Homers, and we have 5/4 of each of the untreated — meaning we have one Moe, one Barney, one Willie, and one Skinner, and another hybrid-person which is ¼ of each. Making that a total of 5 treated and 5 controls.\n\n\n\nOur pseudo-population includes a similar number of adult males (note the duplicated Homer and the hybrid untreated), so when comparing the groups - the effect of adult-males will cancel it-self and we’ll be left only with the effect of the treatment.\n\n\nSee, we were able to create a population in which males are evenly distributed between groups.\nIn real life, of course, there will be more than one feature to handle, and that’s why we’ll need a model to estimate \\(\\Pr[D|X]\\).\nTL;DR\n\n\n\nIPW takes an unbalanced population and creates a balanced pseudo-population (Simpsons components from Wikipedia).\n\n\n\n\n\n\nSimpson’s Paradox\nBy now you might have a hunch how we can use IPW to solve Simpson’s paradox, but before we do, let’s briefly introduce what this paradox is all about.\nSimpson’s Paradox, a term coined by Blyte¹, is named after Edward Simpson, the first statistician to explicitly point to this problem. In short, the paradox happens when an overall average trend in the population is reversed or canceled-out when examining its composing sub-groups.\nThe intuition in the continuous case is very clear, as suggested by this GIF:\n\n\n\nA visual intuition on how a trend in the overall population can reverse itself in the composing sub-populations.\n\n\nTo get a better understanding of the phenomena, let’s examine a real-world example:\nAccording to the paper, we have two ways to treat kidney-stones. Either with an open-surgery (A) or with a new non-invasive method using shock-waves (B). We gather medical records from 700 people, 350 from each treatment-group and compare their success rates.\nWishing to conclude which method is better, we compare the success rates among group A and B and see they are 78% and 82.5%, respectively. Naively, we want to deduce B is better (by 4.5%), but we remember we read somewhere (where?) that physicians are not random. We suspect there’s probably some reason as to why patients got a certain treatment but not the other, and that it probably has to do with their prior medical condition.\nLo and behold, when we split the patients based on stone size — we see a different trend. Suddenly it is treatment A that is better for both small stones and large stones — 93.1 and 73% respectively.\nThe averaging process probably masks some vital information, so let’s look at the raw numbers.\nTreatment A has 81/87 (93.1%) success rate for small stones and 192/263 (73%) for large stones. Meanwhile, treatment B has 234/270 (86.67%) success rate for small stones and 55/80 (68.75%) for large stones.\n\nB has better success rate overall, but A is better in both small and large kidney stones. This happens because B got the majority of easy cases (small stones), while A got the majority of hard cases.\n\n\n\n\n\n\n\nStone size \\ Treatment\nA\nB\n\n\n\n\nBoth sizes\n273/350 =\n78.0%\n289/350 =\n82.5%\n\n\nSmall\n81/87 =\n93.1%\n234/270 =\n86.67%\n\n\nLarge\n192/263 =\n73.0%\n55/80 =\n68.75%\n\n\n\nDo you see what the denominator is hiding? The game is rigged. B got the majority of easy cases (small stones), while A got the majority of hard cases. Severity of the patients is not similarly distributed across treatment groups. Since larger stones also have a lower chance of success — because they are more difficult to handle, we find ourselves in a biased situation where comparison of treatments is not fair. This is common in day-to-day scenarios, because when physicians encounter tougher cases (larger stones), they will tend to use bigger guns (open surgery).\nGenerally speaking, a variable that affects both the treatment assignment and the outcome is called a confounding variable. In our case, the severity of patients, expressed by the size of their kidney stones, is a confounder since it increases both the likelihood to be assigned to a harsher medical procedure (a surgery more likely to be effective), and increases the risk of failing (stones not being completely removed).\nIn our final act of this post, we’ll fix this discrepancy in features (severity through stone sizes) between the treated and controls using IPW.\n\n\nUsing IPW to Solve Simpson’s Paradox\nWe have an imbalanced situation whereas the probability to get treatment A if you have small stones is 87/(87+270)=24.4% and if you have large stones it’s 263/(263+80)=76.67%. Similarly, for treatment B it’s 270/(270+87)=75.6% and 80/(80+263)=23.33%.\nTherefore, we can easily calculate the weight of each individual by taking the inverted fraction of people from each group (since we look only at binary stone size, our individuals are basically identical within their groups, so we can use group-level weights). To compute an unbiased success risk, we simply calculate the average of the success risk weighted by these weights:\n\\[\n\\begin{array}{c}\n\\text{For treatment } A: \\left( \\frac{357}{87} \\cdot \\frac{81}{87} + \\frac{343}{263} \\cdot \\frac{192}{263} \\right) /  \\left(\\frac{357}{87} + \\frac{343}{263} \\right) = 88.26\\% \\\\\n\\text{For treatment } B: \\left( \\frac{357}{270} \\cdot \\frac{234}{270} + \\frac{343}{80} \\cdot \\frac{55}{80} \\right) /  \\left(\\frac{357}{270} + \\frac{343}{80} \\right) = 72.97\\%\n\\end{array}\n\\]\nAnd we see that now, even if we aggregate the size of stones, treatment A is better than B (by ~15%). This is consistent with A also being better in every sub-group, and so the reversal we saw before no longer exists.\nWe have solved Simpson's Paradox.\nTL;DR \nin case you haven't read a word, here's the entire post summed up as a GIF:\n\n\n\nBreaking down Simpson’s Paradox visually and solving it by creating an average risk weighted by IP weights. Visualization, by the author, is based on a similar one by John Burn-Murdoch I remember seeing on Twitter"
  },
  {
    "objectID": "blog/2018/01-deep-learning-genetic-prediction/index.html",
    "href": "blog/2018/01-deep-learning-genetic-prediction/index.html",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2018/01-deep-learning-genetic-prediction/index.html#footnotes",
    "href": "blog/2018/01-deep-learning-genetic-prediction/index.html#footnotes",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nstill looking at this cute little ImageNet feature space.↩︎\nWhat is science if not baby-stepping all the way to the moon.↩︎\nP-value-based variable selection strategies are bad, see Sun, Shook, and Kay.↩︎\n*It may be interesting to evaluate the model’s performance when different chromosomes will share some of the deeper convolutions. Namely, replacing the DNN box in the next figure with another CNN.↩︎"
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html",
    "href": "blog/2020/02-against-agile-research/index.html",
    "title": "The case against Agile research",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#agile-methodologies",
    "href": "blog/2020/02-against-agile-research/index.html#agile-methodologies",
    "title": "The case against Agile research",
    "section": "Agile methodologies",
    "text": "Agile methodologies\nAgile methodologies are project management approaches becoming increasingly popular in software development. At its core, Agile software development is iterative and incremental. It promises frequent product delivery that can accommodate for the fast pace demand for changes from users. The main idea is to break down the software into independent self-contained components and implement them cyclically, ever improving the product.\nThis bottom-up approach has stemmed as a countermeasure to the more traditional top-down approach, known as Waterfall. In the Waterfall method, the entire system's architecture is first designed, down to the smallest components, and then coded from start to end. This has made software much slower to improve, which is the main reason why the method's popularity is decreasing in our fast-pace computing world."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#research-and-data-science-pipeline",
    "href": "blog/2020/02-against-agile-research/index.html#research-and-data-science-pipeline",
    "title": "The case against Agile research",
    "section": "Research and data science pipeline",
    "text": "Research and data science pipeline\nAgile has become so popular that it had percolated outside the realm of traditional software development. Specifically, it had trickled down to research, and more specifically, to data-driven research. The heavier the computational infrastructure of the research, the more likely it is to adapt Agile methodology.\nIn the data-science version of Agile, results are iteratively refined. For example, data definitions are constantly refined, models are iteratively tuned, and reports are continually generated.\nFrom my personal experience, this is especially the case in industry research. As opposed to academia, companies usually subject you to a greater corporate hierarchy. You have several lines of management, each requires periodically updates on the project, usually in the form of a report or a presentation. This demand requires you to set an initial pipeline quickly, just to obtain some results, and then iteratively improve it (refine the data, add analyses, prettify graphs, etc.) until the next meeting or report.\nThis article would like to argue that iteratively reporting should be avoided because it can creep in unconscious bias."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#unconscious-bias",
    "href": "blog/2020/02-against-agile-research/index.html#unconscious-bias",
    "title": "The case against Agile research",
    "section": "Unconscious bias",
    "text": "Unconscious bias\nUnconscious (or implicit) bias is a term used to depict one's preference towards prior beliefs on the expense of evidence at hand. It has been affecting science since ever and is the reason why researchers nowadays prefer methods such as blind peer reviews or randomized control experiments. In the former, reviewers aren't affected by the identity of the author, and in the latter, participants don't know whether they get treatment or placebo. Both neutralize a psychological effect that can bias one's response in the process.\nScientists are no exception and subconscious bias can also affect the research protocol and analysis itself. Even in presumably \"pure\" tasks, like measuring physical constants, it has been observed that new measurements tend to cluster around previous measurements, rather than what we nowadays know to be the true value.\nWe can mitigate such unconscious bias by adapting the same trickery we use for blind peer reviews and blind medical trials: blind analysis.\nIn his excellent book Statistics Done Wrong, Alex Reinhart gives a beautiful example of how blind analysis was applied in experimental physics: when Frank Dunnington wanted to measure the ratio between an electron's charge to its mass, it required him to build a dedicated machinery. Dunnington built the entire apparatus but left the detector in a slightly off angle. As the exact measurements are worthless, Dunnington could construct the entire experimental protocol without being exposed to the value he was interested in. Thus avoiding overfitting the protocol to his prior beliefs. Only upon finalizing the analysis plan, had he corrected the off-angled detector and correctly measured the desired ratio."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#blind-statistical-analysis",
    "href": "blog/2020/02-against-agile-research/index.html#blind-statistical-analysis",
    "title": "The case against Agile research",
    "section": "Blind statistical analysis",
    "text": "Blind statistical analysis\nTo be honest, there's no straightforward equivalent to a dislocated detector in the data science pipeline. One might use mock data, or a very (very) small random subset of the data they can later discard. The data will be used just to verify the code runs end to end, outputs are correctly saved, errors are correctly logged, graphs look as intended, etc. Only when the pipeline is finalized will one run their code on the entirety of the data, generating results.\nTo quote Alex: \"no analysis plan survives contact with the data\", and I will add that technical issues should be debugged and solved as data blind as possible."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#avoiding-agile-research",
    "href": "blog/2020/02-against-agile-research/index.html#avoiding-agile-research",
    "title": "The case against Agile research",
    "section": "Avoiding Agile research",
    "text": "Avoiding Agile research\nIn research, our product is usually some kind of a report. Therefore, in Agile-like research, we continuously generate periodic reports and therefore constantly re-estimating our estimand of interest. The more rounds we go, the greater the chances to be affected by the ever-lurking subconscious bias toward desired results.\nFurthermore, changing analysis plan as we go, especially when based on half-baked analyses, can wildly increase false positive discoveries. For example, if we decide to change the primary outcome we measure in the middle of the project, just because we saw (or any management reading your monthly report suggested) there's no effect, it will direct us into analyzing our data, rather than analyzing the phenomena the data measure.\nData are like a perishable resource. Each time you glance at it, the quality of the story it tells deteriorates. Soon you'll be reading whatever you want to read, rather than what the data wants to tell.\nSimilarly, Results are just a set of transformations applied to the data and glancing at them is like glancing at the data via proxy. Namely, that too drains out our data resource, making it less reliable."
  },
  {
    "objectID": "blog/2020/02-against-agile-research/index.html#summary",
    "href": "blog/2020/02-against-agile-research/index.html#summary",
    "title": "The case against Agile research",
    "section": "Summary",
    "text": "Summary\nAs short-cycle iterative management methodology become increasingly popular in research, so does our susceptibility to subconscious bias. This is not to say Agile-ish methodologies are never to be used in research or data science, only that they require some adaptations. Adaptation like more careful data management, avoiding peeping at the results just like we avoid peeking at the data, and pre-registering the analysis plan.\nEventually, the scientists themselves also adapt to the evidence they work with, just like any other statistical model adapts to the data. Therefore, scientists should limit their own access to data and results, just as they do to the models they apply, to avoid overfitting their own analysis to their desires or prior beliefs"
  },
  {
    "objectID": "blog/2021/01-visualizing-micro-macro-averages/index.html",
    "href": "blog/2021/01-visualizing-micro-macro-averages/index.html",
    "title": "A visual way to think of macro and micro averages in classification metrics",
    "section": "",
    "text": "For completeness, this article spends most its words explaining what a confusion matrix is. If you already familiar, you can probably understand the article by just scrolling down through the pretty pictures.\n\nConfusion matrix\nAlmost every classification metric can be defined by a confusion matrix. Those that cannot — can be defined by several. This makes the confusion matrix the most basic way to evaluate classification models.\nEvaluating a binary classification, we can either be right or wrong. But we can cross-tabulate the four possible combinations of predicted labels and true labels into a contingency table. On the diagonal, we count the correct predictions, and off the diagonal we count our mistakes.\nFirst, we can be correct on the positive label, these predictions are truly positive (TP). Second, we can be correct on the negative label, making these predictions truly negative (TN). But correct predictions are all alike; every wrong prediction is wrong in its own way. The first type of error is type-1 error — positive predictions that actually belong to the negative class — these are falsely positive predictions. Lastly, the fourth combination and the second type of error, we can wrongly predict the positive label to be negative — these predictions are falsely negative.\n\n\n\nConfusion matrix is a contingency table enumerating the combinations of predictions and true labels. It defines four important building blocks for most classification metrics: TPs, FPs, FNs, TNs.\n\n\nThis four-cell matrix can define a plethora of metrics. For example, accuracy can be defined by summing the TP and TN and dividing by the sum of the matrix; sensitivity is the TP divided by actual positive observations; specificity is similar for the negatives: dividing TN by the actual negative observations; and there are much, much more. Especially when we can also use the metrics themselves to create compound metrics, the sky is the limit.\n\n\nMulticlass confusion matrix\nConfusion matrices can be naturally expended into multi-class classification. Instead of a 2-by-2 table, we'll have a k-by-k table enumerating the different combinations of predictions and labels. However, the metrics in this multi-class setting are not always well defined. This is because we no longer have a single false positive or false negative count, but rather several ones — one for each pair of misclassified classes.\nTo redefine these metrics for multiple classes, we must first convert our single k-by-k table to k 2-by-2 tables. We simply aggregate the multiple false-positives, false negatives, and true negatives into one of each. This is the same as thinking of our multiclass classifier as multiple one-vs.-rest binary classifiers.\n\n\n\nMulticlass confusion matrix can be reformulated as multiple one-vs-rest 2-by-2 matrices. This will help us redefine what false positives and false negatives are in a multiple\n\n\nWe now have a bunch of 2-by-2 confusion matrices, let's stack them up. Think of it as a volume, or a 3D tensor. In each simple confusion matrix, the metrics — like precision, specificity, or recall — are well defined. However, we need to extract a single metric from them. This is when averaging — micro and macro — come into play.\n\n\n\nWe’ll benefit from thinking of these multiple one-vs-rest confusion matrices as a 3D stack volume of confusion matrices. Then macro- and micro-averages are just the order of axes reduced.\n\n\n\n\nMacro-averaging\nIn macro-averaging, we first reduce each of the k confusion matrices into a desired metric, and then average out the k scores into a single score.\n\n\n\nIn macro-average, we first calculate a metric from each confusion matrix, and then average out the scores of these metrics.\n\n\n\n\nMicro-averaging\nIn micro-averaging, we will first sum the TPs, TNs, FPs, and FNs across the different confusion matrices, say we denote them as ΣTP, ΣTN, ΣFP, and ΣFN. We then use these aggregated measures as if they form a confusion matrix of their own and use them to calculate the desired metrics.\n\n\n\nIn micro-average, we first reduce the confusion matrices into one summed up confusion matrix, and then calculate the metric from aggregated table.\n\n\n\n\nSummary\nMulticlass confusion matrices can be expanded to stacked one-vs.-rest confusion matrix. Under this formulation, micro- and macro-averages differ by which axis is reduced first. Macro-average first reduces the confusion matrices into scores and then averages the scores. Micro-average first reduces the multiple confusion matrices into a single confusion matrix, and then calculates the score."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html",
    "title": "Causal Inference with Continuous Treatments",
    "section": "",
    "text": "Causal inference, the science of estimating causal effects from non-randomized observational data, is usually presented using binary treatment; we either treat or don’t treat; we give drug A or drug B. There’s a good reason for that, as causality is already complex as it is. However, not all interventions are binary (or discrete).\nSometimes, the interventions we care about are continuous. “Taking a drug”, for example, is fairly vague — drugs have active ingredients and those can come in different dosages. Too little and the drug might not seem effective, too much and the drug might be harmful. Therefore, we might be interested in the effect of different dosages of the same drug. This is often called dose-response modeling.\nContinuous exposures are all around us. From drug dosages to number of daily cigarettes smoked or air pollution levels, from how much time you watched an ad before skipping it to how much red a the “unsubscribe” button is on a newsletter, from the interest rate increased by the central bank to the amount of money in a lottery winning. We can’t limit ourselves to studying binary exposures just because the introduction book didn’t cover the other ones.\nIn this post I will introduce a generalized version of inverse probability weighting for continuous treatment. I’ll show different estimation methods and discuss the required assumptions and its limitations. I will assume you are familiar with causal inference and IPW for binary treatment, but if you are not — I got you covered in this IPW explainer."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "title": "Causal Inference with Continuous Treatments",
    "section": "From binary to continuous treatment",
    "text": "From binary to continuous treatment\nRecall that in the binary treatment setting, a common way to estimate causal effects is by using inverse probability weighting (sometimes called inverse propensity weighting, but I’ll just use IPW). Given individual \\(i\\) with treatment assignment \\(a_i\\) and characteristics \\(x_i\\), its inverse propensity weight is defined as: \\(w_i=1/\\Pr[A=a_i|X=x_i]\\). Namely, the inverse probability of \\(i\\) to be assigned to their treatment, given their characteristics.\nHowever, when treatment (or any random variable for that matter) is continuous, the notion probability mass fails and we need to speak in terms of probability density. This is because the probability of a single point, say \\(a_i\\), is basically 0, while it may still have density associated with it since density is defined as the derivative of the cumulative probability function. This is a fundamental theoretical difference, but we can capture it in a small notation change, instead of \\(\\Pr \\left[A=a_i|X=x_i \\right]\\) we will use \\(f \\left(a_i|x_i \\right)\\).\n\n\n\nGradually approximating a discrete binomial distribution with a continuous Gaussian one."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#modelling",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#modelling",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Modelling",
    "text": "Modelling\nRecall that estimating the treatment effect with IPW is comprised of two main steps. First, model the treatment and obtain IP-weights. Second, model the outcome using those weights. In the binary case, once we have the weights, the simplest way to estimate the potential outcomes is to simply take the weighted average in the treated and untreated (often called the Horvitz-Thompson estimator). However, an equivalent way is to use a simple univariable regression: regress the outcome against the treatment (and an intercept) weighted by the IP-weights. Then, the average treatment effect is simply defined by the coefficient corresponding to the treatment variable. This is often called a marginal structural model in the epidemiology literature.\nNote that in the continuous treatment case, the first option is not applicable. Often, there will be many unique treatment values and it will be rare to have enough samples with the exact same continuous treatment value, for all treatment values. Binning them will solve it, but we’re here for continuous treatment modeling. Therefore, we will need to use the latter option and create an additional (parametric) model between the outcome and the treatment. This will be our dose-response function.\nLet’s examine those two steps in more details.\n\nStep 1: modeling the treatment\nWith categorical treatments, we needed to model the probability of getting treated. We could have done that by regressing the treatment assignment against the covariates, basically using any “classifier” that outputs predictions in the 0–1 interval which we can then interpret as probabilities. Logistic regression, for example is a generalized linear model that is defined by the binomial distribution — a discrete probability function. With continuous treatment, however, we will need a regression model instead. For example, in generalized linear models, a linear regression model is defined by the Gaussian distribution. And as the animation shows above — the more categories a binomial distribution has the better it is approximated by a normal distribution.\nOnce we fitted a model, we can obtain the conditional expectation \\(E \\left[A|X \\right]\\). But unlike the binomial case, in the continuous case, this is not sufficient to generate densities. For simplicity, let’s assume the common Gaussian distribution, which is parameterized by a mean and variance. The conditional mean of that distribution will be the estimated conditional expectations (the predictions); the variance will be constant and will be set to be the variance of the residuals between the treatment and the predictions. Once we defined the distribution, we take the density of the observed treatment values with respect to this distribution. The generalized IP-weights are the inverse of these densities.\nTo summarize step 1:\n\nFit a function \\(g(x)\\), regressing the treatment \\(A\\) on covariates \\(X\\).\n\\(A=g(X)+\\epsilon=\\alpha_0+\\alpha_1 X+\\epsilon\\)\nDefine the conditional distribution \\(D_i \\sim \\text{Normal}(g(x_i), \\text{Var} \\left(a_i-g(x_i) \\right)\\).\n\nThe conditional mean of each sample is its prediction.\nThe variance is fixed and is the variance of the prediction residuals.\n\nDefine the density \\(d_i\\) as the value of \\(a_i\\) from \\(D_i\\).\nDefine the weight \\(w_i\\) to be the inverse of the density: \\(1/d_i\\) .\n\n\n\nStep 2: modeling the outcome\nOnce we obtained the balancing weights w, we can model the counterfactual outcomes using the observed outcomes and treatments. To do that, we regress the outcome against the treatment, weighted by the IP-weights obtained from step 1. However, unlike the binary treatment case, the functional form of the continuous treatment should be flexible enough to avoid bias due to misspecification. For example, we will add a quadratic term of the treatment or model it using a spline, etc.\nWhen we have non-linear transformations of the main treatment variable, we can no longer interpret the treatment effect as the coefficient of the treatment covariate. Instead, to make counterfactual outcome predictions, we will set some treatment value and run it through our model to get the predicted outcome, and average it out across the units to obtain the average outcome had everyone been assigned that specific treatment value.\nWe can repeat that for two different treatment values. Then the causal effect will be the difference (or ratio) between these two potential outcome predictions. Alternatively, we can repeat that for every treatment value in a range we care about and obtain a dose-response curve — see how the counterfactual outcome prediction changes as a function of assigning different dosages.\n\n\n\n\nMarginal Structural Model - regress the outcome on the treatment weighted by the generalized IP-weights. As proposed by Robins, Hernan, and Brumback1.\n\n\n\n\nCode\nBelow is a Python code demonstrating the estimation process described above.\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom causallib.datasets import load_nhefs\n\n\ndef conditional_densities(data, use_confounders=True):\n    formula = \"smkintensity82_71 ~ 1\"\n    if use_confounders:\n        formula += \"\"\" + \n        C(sex) + C(race) + age + I(age**2) + \n        smokeintensity + I(smokeintensity**2) +\n        smokeyrs + I(smokeyrs**2) + \n        C(exercise) + C(active) + C(education) + \n        wt71  + I(wt71**2)\n        \"\"\"\n    model = sm.formula.ols(formula, data=data).fit()\n    density = stats.norm(\n        loc=model.fittedvalues,\n        scale=model.resid.std(),\n    )\n    densities = density.pdf(data[\"smkintensity82_71\"])\n    densities = pd.Series(densities, index=model.fittedvalues.index)\n    return densities\n\n\ndata = load_nhefs(raw=True)[0]\ndata = data.loc[data[\"smokeintensity\"] &lt;= 25]  # Above 25 intensity is sparser\n\ndenominator = conditional_densities(data, use_confounders=True)\nnumerator = conditional_densities(data, use_confounders=False)\ngeneralized_ipw = numerator / denominator\n\nmsm = sm.formula.wls(  # Using GEE instead will lead to better (more conservative) uncertainty intervals\n    \"wt82_71 ~ 1 + smkintensity82_71 + I(smkintensity82_71**2)\",\n    data=data,\n    weights=generalized_ipw,\n).fit()\n\ndosage = list(range(26))\ndosage = pd.DataFrame(\n    data={\"smkintensity82_71\": dosage, \"I(smkintensity82_71**2)\": dosage},\n    index=dosage,\n)\nresponse = msm.predict(dosage)\nax = response.plot(\n    kind=\"line\",\n    xlabel=\"Increase in cigarettes per day\",\n    ylabel=\"Change in weight after 10 years [kg]\",\n    title=\"Smoking more cigarettes led to smaller weight increase\"\n)\n\n# Example adjusted from Hernan and Robins' What If book\n\n\n\nThe dose response curve resulting from the snipped code above.\n\n\n\n\nExtensions\nThe above describes on simple flavor of estimation. It could, however, be extended in multiple parts. Below are a few such extensions. Feel free to skip if you had enough.\n\nStabilized weights\nIn IPW for binary treatment, we commonly calculate the weights as 1 over the probabilities. This results in pseudo population twice the size of our sample — since the weighting result in each treatment group being the size of our original sample.\nStabilized weights are a version in which the numerator is not 1, but the treatment prevalence (average of binary treatment). This shrinks the weights so that the overall pseudo-population size is the size of the original sample, not twice the size.\nThis stabilization is also applicable to the continuous treatment setting. Instead of setting the numerator to be 1, we can take the numerator to be the density of treatment values under the average treatment value (or, more generally, the prediction of an intercept-only model. Under this formulation we can also stabilize on effect modifiers, but this is for a different post). The code above shows a stabilized version that is more recommended.\n\n\nReplacing weighted regression with a clever covariate\nIn the second step, when modeling the outcomes based on the treatments, we incorporated the generalized propensity scores as weights in a weighted regression. This is usually referred to as marginal structural models as described by Robins, Hernan, and Brumback. However, similar to the different flavors of TMLE, we can also incorporate the generalized propensity scores as an additional covariate in the second-step outcome regression, rather than as weights. This, in fact, what Hirano and Imbens suggested.\nIn this version, we add the densities (not their inverse) as an additional feature. However, since it is another continuous measure, prone to misspecification, we will add it flexibly. Usually by also adding a squared term and an interaction with the treatment variable (or a spline).\n\n\n\nOutcome model with the generalized IP-weights as a predictor. As proposed by Hirano and Imbens.\n\n\nOne small but important detail to note is that during prediction, when we set the treatment value of interest for all individuals, we will now first need to calculate the density for that specific value and then insert these densities as the predictors to the outcome model we apply.\n\n\nHeteroskedastic density and other distributions\nIn item (2) of step 1, we estimated the density with a fixed variance for all individuals. This assumption, called homoskedasticity, is reasonable (and can be empirically tested by examining the residuals) but can be relaxed. Similar to how the mean of the density function was conditioned on covariates (i.e., a prediction), the variance can also be a function that changes with covariates. Or in other ways like density-weighted regression.\nAdditionally, we could parameterize the density function using other distributions, like t-distribution, truncated normal, etc. Alternatively, it can further be de-parametrized by using kernel density estimation, but there ain’t no such thing as free lunch — and this will require much denser data for a reliable estimation.\n\n\n\nPositivity assumptions under continuous treatment\nSo far, we have discussed how to obtain statistical associations between treatment and outcome. However, to convert them into causal claims, we will need to make additional assumptions. These assumptions are necessary no matter how sophisticated the statistical estimation is. It is up to us to apply additional logic on top of it to justify these associational differences are indeed causal effects.\n\nCausation = Association + Logic\n\nRecall that we have three main assumptions: consistency, exchangeability, and positivity. Consistency is an assumption on the treatment mechanism and is therefore the same as in the categorical treatment setting. Exchangeability assumes there are no unmeasured confounding, and that each potential outcome is independent of the observed treatment assignment (no bias) given the covariates — this is also the same. Positivity is the assumption requiring some adjustments.\nRecall that in the categorical case, positivity assumes each unit has some chance (positive probability) to be assigned to every treatment. This means the treatment groups share a common support, and their characteristics overlap. It is formally defined as \\(\\Pr[A=a|X]&gt;0\\) for all treatment values a across the entire space covariate space X.\nBut in the continuous case, we need to replace the probability with density. However, the rest remains the same. We will require \\(f(A=a|X)&gt;0\\) for all treatment values a across the entire covariate space X. Namely, we need positive density for all available combination of treatment and covariate levels. Luckily, this can be empirically tested (like regular positivity) by examining density of different treatment values, especially those we are most interested in, under the density model we obtained (the one whose means are our regression predictions).\n\n\n\n\n\n\n\n\n\nPlotting the histogram of conditional densities of the smoking example to assess overlap. Here testing for two treatment values: 0 = no change in smoking intensity, and 1 = an increase of a cigarette a day. This is the equivalent of plotting the probability to treat in the binary treatment case to assess overlap between groups.\n\n\n\n\n\n\n\nOr more broadly - A ridge plot examining the conditional density distribution of all possible dosage increases. We see the further the dosages are from each other - the smaller the overlap. Therefore we might trust the effect of local changes (say, increase of 5 vs. no change) than global changes (say, increase of 25 vs. no change)."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#limitations",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#limitations",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Limitations",
    "text": "Limitations\nIncreasing the number of treatment values does not come without a cost. There are several limitations we should be aware of when modeling continuous treatment.\nFirst, theoretical assumptions are harder to conforms to. It is harder to achieve exchangeability since we now have more treatment values for which we want to achieve unconfoundedness. It is also harder to achieve positivity, for both the same reason and the fact that conditional density might be sparser due to its continuous nature.\nSecond, continuous variables are harder to model. They are more prone to misspecification. We might partially solve it by using flexible estimators (like additive trees) or flexible data transformation (like splines in GAMs), but it can come at a cost — requiring more data or introducing some bias due to bias-variance trade-off. Additionally, densities are notoriously hard to estimate, and our generalized IP-weights can be sensitive to different choices of density estimators.\nThird, often times some continuous measures are actually ordinal. Treatment on the ordinal scale might be approximated as continuous, especially when the number of categories and their ranges increase. But continuous approximation of ordinal variables might also introduce some bias due to misspecification. There are generalizations of IPW to the ordinal scale, which require ordinal regression (similar to how we required linear regression, and regular IPW requires logistic regression), but these are beyond the scope of this post. Just so you're aware of that.\nLastly, to end on a brighter note, throughout this post I had in mind a case of continuous treatment and continuous outcome. However, this is also applicable to other outcomes. Namely, the second-step outcome model can correspond to arbitrary type of the outcome. Most commonly, if we have binary outcome, we can apply a logistic regression (or any other \"classifier\")."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#conclusions",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#conclusions",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post I introduced causal inference with continuous treatments. I presented their importance, described how to model them, how to adjust the required causal assumptions, and their limitations. I hope you find it useful."
  },
  {
    "objectID": "blog/2022/01-continuous-treatment-causal-inference/index.html#footnotes",
    "href": "blog/2022/01-continuous-treatment-causal-inference/index.html#footnotes",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\n\n\nMarginal Structural Model - regress the outcome on the treatment weighted by the generalized IP-weights. As proposed by Robins, Hernan, and Brumback.\n\n\n↩︎"
  },
  {
    "objectID": "blog/2023/02-hello_world/index.html",
    "href": "blog/2023/02-hello_world/index.html",
    "title": "Hello Quarto!",
    "section": "",
    "text": "I made a personal website."
  },
  {
    "objectID": "blog/2023/02-hello_world/index.html#footnotes",
    "href": "blog/2023/02-hello_world/index.html#footnotes",
    "title": "Hello Quarto!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince I already mentioned Jinja above, it seems like its template language combines both pandoc’s and EJS capabilities together?↩︎"
  },
  {
    "objectID": "blog/2024/01-oui-love-plots/index.html",
    "href": "blog/2024/01-oui-love-plots/index.html",
    "title": "Oui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference",
    "section": "",
    "text": "Complete manuscript at: https://ehud.co/oui-love-plots/  User survey at: https://forms.gle/cb8AqGWjbBZqbjbSA"
  },
  {
    "objectID": "blog/2024/01-oui-love-plots/index.html#motivation",
    "href": "blog/2024/01-oui-love-plots/index.html#motivation",
    "title": "Oui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference",
    "section": "Motivation",
    "text": "Motivation\nAssessing balance between exposure groups by visualizing the absolute standardized mean differences (ASMD) in a Love plot is a common approach to diagnose models like propensity score weighting or matching. However, the ASMD only captures covariate-exposure associations and neglects to integrate information about the covariate-outcome associations. This can mislead researchers, especially in fields like epidemiology where adjustment sets are determined based on domain experts before looking at the data. In such analysis approach, researchers will prefer to err on including a variable rather than excluding it. This strategy can sneak in non-confounding variables that the researcher will treat as confounders and therefore try to balance their distributions across exposure groups. But since non-confounding variables should not be adjusted for (instruments can amplify z-bias) and balanced (prognostic should capture heterogeneity), they can lead astray researchers that will try to force them to be balanced.\nIn other cases, structural confounders selected a-priori may not necessarily be statistical confounders. Namely, they may not be associated with both the exposure and the outcome and therefore not necessarily bias treatment effect estimation in the data at hand.\nTo overcome this issue, I developed the Oui-Love plot: an OUtcome-Informed Love plot. I will first introduce a new score combining the known standardized difference with covariate-outcome importance measures (“variable importance”). Second, since we love plots (wink wink), this additional score will be visualized through several graphical channels to augment the standard Love plot and help researchers focus on actual statistical confounders in the data."
  },
  {
    "objectID": "blog/2024/01-oui-love-plots/index.html#outcome-informed-asmd-score",
    "href": "blog/2024/01-oui-love-plots/index.html#outcome-informed-asmd-score",
    "title": "Oui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference",
    "section": "Outcome-informed ASMD score",
    "text": "Outcome-informed ASMD score\nThe standardized mean difference is a commonly used metric to diagnose treatment-first causal inference models (like matching or propensity score weighting). It is defined as the difference in covariate averages between two exposure groups divided by the pooled standard errors. Since it is only dependent on covariates and exposure, it measures covariate-exposure association alone. This can be an insufficient metric for diagnosing models because inability to balance instruments or prognostic variables is not an issue.\nCovariate-outcome importance metrics measure how each covariate contributes to the prediction accuracy of the outcome. There are many approaches to calculate importance, but the model-agnostic ones often work through covariate “exclusion”, where a covariate is either excluded or have its values shuffled between observations. Then we can measure the prediction error of the full model (including all covariates) against the model missing that covariate (and including all the rest, as well as the exposure). Importance is then a non-negative score where low scores correspond to small increase in prediction error and high scores correspond to large increase in prediction error, relative to the full model.\nGive an ASMD score and an covariate-outcome importance score for each covariate, the natural way to combine them is simply to multiply them. These are two orthogonal measures, so they can either cancel out each other or amplify each other. Figure 1 shows the instrument \\(X_A\\) with high ASMD (panel A) together with low covariate-outcome importance (panel B) is ultimately canceled out by the multiplication (panel C). Similarly, the prognostic factor \\(X_Y\\) that has very high outcome importance (higher than confounder \\(X_{AY}\\)) and small ASMD is dimished in the combined measure. Only the true confounder \\(X_{AY}\\) is is kept at relatively high importance throughout.\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "blog/2024/01-oui-love-plots/index.html#outcome-informed-love-plot",
    "href": "blog/2024/01-oui-love-plots/index.html#outcome-informed-love-plot",
    "title": "Oui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference",
    "section": "Outcome-informed Love plot",
    "text": "Outcome-informed Love plot\nOnce we have an additional covariate-outcome importance score, we can visualize it. When we visualize data we essentially map between dimensions oh the data to different graphical channels.\nA traditional Love plot map the covariates to the y-axis, the ASMD to the x-axis, and the types of the model (e.g., weighted/unweighted) to the color of the marker All in all, we mapped three data dimensions: covariates, their ASMD, and adjustment model, to three graphical channels: y-axis, x-axis, and color (and possibly a fourth channel of marker shape for emphasis).\nIn an outcome-informed Love plot, we have an additional data dimension that is the outcome-informed ASMD. In the manuscript, I suggest to map to (up to) three different graphical channels Figure 2 that can be combined arbitrarily Figure 3:\n\nThe opacity channel. Marks corresponding to more important covariates are more opaque, while less important marks are more transparent.\nThe size channel. Marks corresponding to more important covariate are larger, while less important marks are smaller.\nThe order of the y-axis. Covariates are ranked by their importance with more important covariates appearing on top.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nThe common property for options (1) and (2) is that less important covariates appear less prominent, either being smaller or more transparent. The argument being that if they do not influence the outcome, they will not bias the estimation, and therefore are not important and less interesting to examine. If they are less interesting to examine, there is less need for them to stand out and can therefore be salient. This will reduce clutter and allow the viewer to focus on the more important (and thus visually prominent) covariates. Meanwhile, option (3) clusters more important covariates to specific regions of the plot, but breaks the standard of ordering covariates by the unadjusted ASMD that may be familiar to practitioners. All options achieve a similar objective of differential attention onto more important covariates, either by differential prominence (transparency and size) or by differential spatial location (order).\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "blog/2024/01-oui-love-plots/index.html#summary",
    "href": "blog/2024/01-oui-love-plots/index.html#summary",
    "title": "Oui-Love Plots: Outcome-informed Love plots for covariate balance in causal inference",
    "section": "Summary",
    "text": "Summary\nWe have introduced an augmentation to the Love plot by incorporating additional information about covariate-outcome importance. Love plot is a common graphical diagnostic for group balancing methods in causal inference, visualizing the (Absolute) Standardized Mean Difference (ASMD) for each covariate before and after adjustment. ASMD alone, however, can be misleading if the covariates under investigation are not true confounding variables that influence both the exposure and the outcome. Therefore, the outcome-informed Love plot can help paint a fuller picture, teasing out covariates that are both imbalanced and drive change in the outcome - and therefore actually bias the estimation.\nThis is a modular, extendable, and easy-to-implement idea that I hope causal inference practitioners will find useful.\nFor more details, see the full manuscript draft at: https://ehud.co/oui-love-plots/"
  },
  {
    "objectID": "blog/2024/03-crossfit-crossval/index.html",
    "href": "blog/2024/03-crossfit-crossval/index.html",
    "title": "Visualizing (double) cross-fitting for causal inference",
    "section": "",
    "text": "The integration of complex machine learning estimators with causal inference estimators is beneficial but not trivial.\nIt is beneficial because once we have complex high-dimensional data, where we can’t just summarize the outcome in each “cell” of the data (i.e., every combination of covariate levels), identification of the causal effect no longer solely relies on whether we where able to measure all confounders but also on whether we were able to capture the correct functional form between confounders, treatment and outcome (i.e., correct model specification). Machine learning (ML) techniques can therefore really broaden the range of functional forms, and therefore strengthen our belief that we were able to correctly specify the model and remove confounding bias.\nHowever, applying ML estimators opens a new front of modeling considerations like bias in effect estimations due to overfitting. Therefore, plugging complex ML estimators into causal estimators is not trivial and requires adaptations. One such adaptation is the need to model both the treatment and the outcomes separately (like in TMLE or double/debias ML). Another adaptation is out-of-sample estimation, which comes in different forms: cross-validation, cross-fitting, and double cross-fitting. Cross validation, familiar to most ML practitioners is the same data partitioning scheme as cross-fitting. Double cross-fitting introduces an additional 50:50 split within each fit-fold. This post will try to make sense of these out-of-sample techniques visually.\nThroughout this post I will use \\(X\\) to denote covariates/confounders, \\(A\\) for a treatment assignment, and \\(Y\\) for the outcome. The treatment will be modeled with a function \\(\\pi(X)\\), and the outcome with the function \\(m(X)\\) (or \\(m(X,A)\\) in the case of AIPW and TMLE). Both will have a subscript \\(-k\\) to specify the out-of-fit fold on which they predict on (with \\(K\\) being 5 in total in this post). While often in the causal inference literature, each such test fold will be used for an effect estimation that will later be aggregated across folds, the overarching goal for us in this post will be to generate out-of-sample predictions for each observation in the dataset that will later be used estimate an effect once1."
  },
  {
    "objectID": "blog/2024/03-crossfit-crossval/index.html#footnotes",
    "href": "blog/2024/03-crossfit-crossval/index.html#footnotes",
    "title": "Visualizing (double) cross-fitting for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is similar to how CV-TMLE (Zheng and Van Der Laan 2010) estimates an effect using TMLE within each test data partition, while there is an equivalent version in which TMLE is simply fed with test-data partition predictions (and the TMLE procedure is ran just once) (Levy 2018).↩︎\nThis figure is highly inspired by Figure 1 in Zivich and Breskin (2021). However, there they depict a 3-fold double cross-fitting, which is a somewhat degenerate case, since there is no non-trivial complementary folds (it is basically the equivalent of a two-fold (single) cross-fitting). In contrast, Figure 2 shows a 5-fold double cross-fitting, that properly illustrates how to take the complement training-folds and split them into two separate subsamples for the two nuisance functions.↩︎\nThis is a benefit of having an odd number of folds. For a 10-fold split, one of the training folds would have had to be further split into two, half for the treatment nuisance function and half for the outcome.↩︎\nThis implementation does not follow Figure 2 exactly, because it does not respect the neat separation into the predefined folds, although an exact implementation of the schematics in the Figure is achievable with a bit more coding.↩︎\nThe only problem I personally have with RepeatedKFold is that it flattens the results and losses the nested (hierarchical) structure of when a repeat starts and ends outside each internal K-fold. Keeping track of this nested structure can be beneficial, especially when the average treatment effect is estimated within each test-fold. Because then within-repetition ATEs should be aggregated differently than between-repetitions ATEs (Zivich and Breskin (2021)).↩︎\nI, for instance, was under the impression that cross-fitting is not equal to cross-validation because it used independent disjoint sets, rather than the complementary folds. Namely, It predicted on fold \\(k\\) using a model that was fitted only on fold (say) \\(k+1\\) (rather than all the rest of the data other than \\(k\\)). Which immediately begged the question why use anything other than 2-fold cross-fitting (and to which I then thought the &gt;2 number of folds in the literature might refer to random repetitions…). So you can see how my understanding could have spiraled, so going back over these materials and formalizing it in sketches really made it sing for me.↩︎"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Ehud Karavani",
    "section": "",
    "text": "Email  LinkedIn  Scholar  Medium  GitHub"
  },
  {
    "objectID": "cv/index.html#highlights",
    "href": "cv/index.html#highlights",
    "title": "Ehud Karavani",
    "section": " Highlights",
    "text": "Highlights\n\nHealthcare machine learning researcher with 8+ years of experience\n1st author Cell paper, 2019\nCreator of causallib - an open-source Python package for causal inference.\n700+ stars and 100+ forks on Github.\nReceived an IBM Research Accomplishment award (2023)\nCo-inventor on 3 US patents\nPyData conference speaker and podcast interviewee\nCausal inference, machine learning, deep learning, statistics, data viz, Python"
  },
  {
    "objectID": "cv/index.html#about-me",
    "href": "cv/index.html#about-me",
    "title": "Ehud Karavani",
    "section": " About me",
    "text": "About me\nHighly skilled in causal inference, machine learning, (Bayesian) statistics, and data visualization. An applied researcher and data scientist, I spend my time between building reusable tools for research and putting them into use. Advocating Clean Code for research code. Strong preference for eclectic, collaborative environments."
  },
  {
    "objectID": "cv/index.html#experience",
    "href": "cv/index.html#experience",
    "title": "Ehud Karavani",
    "section": " Experience",
    "text": "Experience\n\n\n\n2017 – present\n\nResearch Staff Member\nCausal Machine Learning for Healthcare and Life Science, IBM-Research, Israel\n\n\nCreator of causallib – a one-stop-shop open-source Python package for flexible causal inference modeling.\n\nReceived an IBM Research Accomplishment award (2023)\n\nIndividual Contributor (IC)\nCausal inference consultant for Research labs globally\nProject leader: drug repurposing by applying high-throughput causal inference to observational healthcare data\n\nManaging a team of 5 researchers.\nLeading the scientific pipeline, system design, and visualization app\nGenerating 100s of hypotheses in minutes\nServed several external engagements with pharma clients, bringing millions in revenue\n\nOrganized the 2018 Atlantic Causal Inference Conference Data Challenge\nMentored 10+ students and interns\nOnboarding lead, onboarding 10+ researchers\nPublished 10+ papers and issued 3 patents\n\n\n\n2022\n\nApplied Statistician\nLaboratory for Gait & Neurodynamics, Ichilov Hospital\n\n\nBayesian hierarchical/multilevel models and causal inference for gait analysis in multiple sclerosis patients\n\n\n\n2016 – 2017\n\nTeaching Assitant\nThe School of Computer Science, Hebrew University\n\n\nIntroduction to Data Science\nWorkshop in Computational Bioskills\n\n\n\n2015 – 2016\n\nResearch Associate / Computational Biologist\nInstitue for Medical Research Israel-Canada, Hebrew University, Faculty of Medicine\n\n\nLarge-scale RNA analysis for finding high-resolution protein-RNA interactions"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Ehud Karavani",
    "section": " Education",
    "text": "Education\n\n\n\n2016 – 2019\n\nM.Sc. in Computer Science and Computational Biology\nFaculty of Science, the Hebrew University of Jerusalem, Israel\n\nThesis: quantifying the utility of embryo selection using genomic prediction of traits\npublished in Cell\n\n\n2013 – 2016\n\nB.Sc. in Computer Science and Computational Biology\nFaculty of Science, the Hebrew University of Jerusalem, Israel\n\n\nDean’s List of Academic Excellence (2016)\n\nBachelor’s thesis published in Nucleic Acids Research"
  },
  {
    "objectID": "cv/index.html#community",
    "href": "cv/index.html#community",
    "title": "Ehud Karavani",
    "section": " Community",
    "text": "Community\n\nPyData speaker\nCausal Bandits podcast interviewee\nDataNights causality series lecturer\nRecurring DataHack mentor and judge"
  },
  {
    "objectID": "cv/index.html#skills",
    "href": "cv/index.html#skills",
    "title": "Ehud Karavani",
    "section": " Skills",
    "text": "Skills\n\n\n\n\n\n\n\nProgramming skills\n\nPython scientific stack (fluent)\n\nScikit-Learn, Pandas, Statsmodels, Seaborn (objects), Matplotlib, Altair, Streamlit, Pytorch, Keras, cvxpy, PyMC, Bambi, Arviz…\n\nR (when needed)\nGit + GitHub\nContinuous integration (Travis, GitHub Actions)\nLinux and remote development (Cloud/AWS + Jupyter lab / VS Code)\n\n\n\nLanguages\n\nFluent English\nNative Hebrew\n\n\n\nGeneral\n\nData enthusiast\nMusician 🎸, hiker / backpacker 🏔️ | | | | | |\nFriendly 🙂"
  },
  {
    "objectID": "cv/index.html#awards",
    "href": "cv/index.html#awards",
    "title": "Ehud Karavani",
    "section": " Awards",
    "text": "Awards\n\n\n\n\n\n\n\n2023\nIBM-Research Accomplishment\nFor my work on causallib and research engagement with the Cleveland Clinic Foundation.\n\n\n2019\nBest of RSNA\nFor the paper Predicting Breast Cancer by Applying Deep Learning to Linked Health Records and Mammograms, published in Radiology.\n\n\n2019\nBest Talk: Israeli Population Genetics Meeting\nFor the paper Screening Human Embryos for Polygenic Traits has Limited Utility.\n\n\n2019\nFeatured Theory of the issue (Cell)\nFor the paper Screening Human Embryos for Polygenic Traits has Limited Utility.\n\n\n2016\nDean’s list of academic excellence"
  },
  {
    "objectID": "cv/index.html#publications",
    "href": "cv/index.html#publications",
    "title": "Ehud Karavani",
    "section": " Publications",
    "text": "Publications\n\n\n\n\n\nDate\n\n\nTitle\n\n\nVenue\n\n\nDOI\n\n\n\n\n\n\n2024\n\n\nSingle-microglia transcriptomic transition network-based prediction and real-world patient data validation identifies ketorolac as a repurposable drug for Alzheimer’s disease\n\n\nAlzheimer’s & Dementia \n\n\nhttps://doi.org/10.1002/alz.14373\n\n\n\n\n2024\n\n\nUsing Causal Inference to Investigate Contraceptive Discontinuation in Sub-Saharan Africa\n\n\nInternational Joint Conference on Artificial Intelligence (IJCAI)\n\n\nhttps://doi.org/10.24963/ijcai.2024/792\n\n\n\n\n2024\n\n\nImproving Inverse Probability Weighting by Post-calibrating Its Propensity Scores\n\n\nEpidemiology\n\n\nhttps://doi.org/10.1097/ede.0000000000001733\n\n\n\n\n2024\n\n\nHierarchical Bias-Driven Stratification for Interpretable Causal Effect Estimation\n\n\nArxiv\n\n\nhttps://doi.org/10.48550/arXiv.2401.17737\n\n\n\n\n2023\n\n\nCausalvis: Visualizations for Causal Inference\n\n\nCHI: Conference on Human Factors in Computing Systems\n\n\nhttps://doi.org/10.1145/3544548.3581236\n\n\n\n\n2023\n\n\nFairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization\n\n\nPacific Symposium on Biocomputing\n\n\nhttps://doi.org/10.1142/9789811270611_0019\n\n\n\n\n2021\n\n\nTrends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2\n\n\nMedRxiv\n\n\nhttps://doi.org/10.1101/2021.03.24.21251900\n\n\n\n\n2019\n\n\nScreening human embryos for polygenic traits has limited utility\n\n\nCell\n\n\nhttps://doi.org/10.1016/j.cell.2019.10.033\n\n\n\n\n2019\n\n\nA discriminative approach for finding and characterizing positivity violations using decision trees\n\n\nArxiv\n\n\nhttps://doi.org/10.48550/arXiv.1907.08127\n\n\n\n\n2019\n\n\nPredicting breast cancer by applying deep learning to linked health records and mammograms\n\n\nRadiology\n\n\nhttps://doi.org/10.1148/radiol.2019182622\n\n\n\n\n2019\n\n\nAn evaluation toolkit to guide model selection and cohort definition in causal inference\n\n\nArxiv\n\n\nhttps://doi.org/10.48550/arXiv.1906.00442\n\n\n\n\n2019\n\n\nComment: causal inference competitions: where should we aim?\n\n\nStatistical Science\n\n\nhttps://doi.org/10.1214/18-STS679\n\n\n\n\n2018\n\n\nIn vivo cleavage rules and target repertoire of RNase III in Escherichia coli\n\n\nNucleic Acids Research\n\n\nhttps://doi.org/10.1093/nar/gky684\n\n\n\n\n2018\n\n\nBenchmarking Framework for Performance-Evaluation of Causal Inference Analysis\n\n\nArxiv\n\n\nhttps://doi.org/10.48550/arXiv.1802.05046\n\n\n\n\n\nNo matching items\n\n\nMay go out of date. Please see my Google Scholar page for the most up-to-date information."
  },
  {
    "objectID": "materials/causal-inference/causalitea/index.html",
    "href": "materials/causal-inference/causalitea/index.html",
    "title": "CausaliTea",
    "section": "",
    "text": "CausaliTea is a reading club on causality [source ]\n\n\n\nCausaliTea’s logo"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html",
    "href": "materials/causal-inference/methods/index.html",
    "title": "Causal Inference - Methods",
    "section": "",
    "text": "An overview of many causal inference methods."
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#zoo-of-causal-methods",
    "href": "materials/causal-inference/methods/index.html#zoo-of-causal-methods",
    "title": "Causal Inference - Methods",
    "section": "",
    "text": "An overview of many causal inference methods."
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#the-simpsons",
    "href": "materials/causal-inference/methods/index.html#the-simpsons",
    "title": "Causal Inference - Methods",
    "section": "The Simpsons ",
    "text": "The Simpsons \n Matching and Inverse Probability Weighting with The Simpsons.\n(This is before the time of Powerpoint morph; this was painstaking.)"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#causal-inference-with-deep-learning",
    "href": "materials/causal-inference/methods/index.html#causal-inference-with-deep-learning",
    "title": "Causal Inference - Methods",
    "section": "Causal inference with deep learning ",
    "text": "Causal inference with deep learning \nRelevant up to ~2020"
  },
  {
    "objectID": "materials/causal-inference/methods/index.html#interrupted-time-series",
    "href": "materials/causal-inference/methods/index.html#interrupted-time-series",
    "title": "Causal Inference - Methods",
    "section": "Interrupted time series ",
    "text": "Interrupted time series \nFrom difference-in-differences to synthetic controls."
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Materials",
    "section": "",
    "text": "Please properly attribute if using.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCausaliTea\n\n\nA casual reading club on causality.\n\n\n\ncausal inference\n\n\n\nA casual reading club on causality.\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference - Motivation\n\n\n\n\n\n\ncausal inference\n\n\n\nMotivating causal inference\n\n\n\n\n\nMay 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference - Concepts\n\n\n\n\n\n\ncausal inference\n\n\n\nBroad overview of causal inference concepts\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference - Methods\n\n\n\n\n\n\ncausal inference\n\n\n\nStatistical methods for causal inference\n\n\n\n\n\nNov 18, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/2018-rnase3-cleavage-rules/index.html",
    "href": "publications/2018-rnase3-cleavage-rules/index.html",
    "title": "In vivo cleavage rules and target repertoire of RNase III in Escherichia coli",
    "section": "",
    "text": "@article{altuvia2018vivo,\n  title={In vivo cleavage rules and target repertoire of RNase III in Escherichia coli},\n  author={Altuvia, Yael and Bar, Amir and Reiss, Niv and Karavani, Ehud and Argaman, Liron and Margalit, Hanah},\n  journal={Nucleic acids research},\n  volume={46},\n  number={19},\n  pages={10380--10394},\n  year={2018},\n  publisher={Oxford University Press}\n}"
  },
  {
    "objectID": "publications/2018-rnase3-cleavage-rules/index.html#citation",
    "href": "publications/2018-rnase3-cleavage-rules/index.html#citation",
    "title": "In vivo cleavage rules and target repertoire of RNase III in Escherichia coli",
    "section": "",
    "text": "@article{altuvia2018vivo,\n  title={In vivo cleavage rules and target repertoire of RNase III in Escherichia coli},\n  author={Altuvia, Yael and Bar, Amir and Reiss, Niv and Karavani, Ehud and Argaman, Liron and Margalit, Hanah},\n  journal={Nucleic acids research},\n  volume={46},\n  number={19},\n  pages={10380--10394},\n  year={2018},\n  publisher={Oxford University Press}\n}"
  },
  {
    "objectID": "publications/2019-causal-inference-evaluation-toolkit/index.html",
    "href": "publications/2019-causal-inference-evaluation-toolkit/index.html",
    "title": "An evaluation toolkit to guide model selection and cohort definition in causal inference",
    "section": "",
    "text": "This manuscript combines the evaluation suite with a specific use-case. A readers-digest version focusing on ROC curves for propensity score models is also available here"
  },
  {
    "objectID": "publications/2019-causal-inference-evaluation-toolkit/index.html#citation",
    "href": "publications/2019-causal-inference-evaluation-toolkit/index.html#citation",
    "title": "An evaluation toolkit to guide model selection and cohort definition in causal inference",
    "section": "Citation",
    "text": "Citation\n@article{shimoni2019evaluation,\n  title={An evaluation toolkit to guide model selection and cohort definition in causal inference},\n  author={Shimoni, Yishai and Karavani, Ehud and Ravid, Sivan and Bak, Peter and Ng, Tan Hung and Alford, Sharon Hensley and Meade, Denise and Goldschmidt, Yaara},\n  journal={arXiv preprint arXiv:1906.00442},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2019-positivitree/index.html",
    "href": "publications/2019-positivitree/index.html",
    "title": "A discriminative approach for finding and characterizing positivity violations using decision trees",
    "section": "",
    "text": "A tree-based approach for finding easily-characterizable positivity violations, commonly referred to as PositiviTree."
  },
  {
    "objectID": "publications/2019-positivitree/index.html#citation",
    "href": "publications/2019-positivitree/index.html#citation",
    "title": "A discriminative approach for finding and characterizing positivity violations using decision trees",
    "section": "Citation",
    "text": "Citation\n@article{karavani2019discriminative,\n  title={A discriminative approach for finding and characterizing positivity violations using decision trees},\n  author={Karavani, Ehud and Bak, Peter and Shimoni, Yishai},\n  journal={arXiv preprint arXiv:1907.08127},\n  year={2019}\n}"
  },
  {
    "objectID": "publications/2021-non-respiratory-covid19/index.html",
    "href": "publications/2021-non-respiratory-covid19/index.html",
    "title": "Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2",
    "section": "",
    "text": "Association of SARS-CoV-2 infection with several severe outcomes from 425,988 admitted individuals."
  },
  {
    "objectID": "publications/2021-non-respiratory-covid19/index.html#citation",
    "href": "publications/2021-non-respiratory-covid19/index.html#citation",
    "title": "Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2",
    "section": "Citation",
    "text": "Citation\n@article{el2021trends,\n  title={Trends in clinical characteristics and associations of severe non-respiratory events related to SARS-CoV-2},\n  author={El-Hay, Tal and Karavani, Ehud and Peretz, Asaf and Ninio, Matan and Ravid, Sivan and Chorev, Michal and Rosen-Zvi, Michal and Patalon, Tal and Shimoni, Yishai and Jain, Anil},\n  journal={medRxiv},\n  pages={2021--03},\n  year={2021},\n  publisher={Cold Spring Harbor Laboratory Press}\n}"
  },
  {
    "objectID": "publications/2023-prs-plus-irm/index.html",
    "href": "publications/2023-prs-plus-irm/index.html",
    "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization",
    "section": "",
    "text": "Model overview"
  },
  {
    "objectID": "publications/2023-prs-plus-irm/index.html#citation",
    "href": "publications/2023-prs-plus-irm/index.html#citation",
    "title": "FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization",
    "section": "Citation",
    "text": "Citation\n@inproceedings{reyes2023fairprs,\n  title={FairPRS: adjusting for admixed populations in polygenic risk scores using invariant risk minimization},\n  author={Reyes, Diego Machado and Bose, Aritra and Karavani, Ehud and Parida, Laxmi},\n  booktitle={Pacific Symposium on Biocomputing},\n  year={2023}\n}"
  },
  {
    "objectID": "publications/2024-contraceptives-discontinuation/index.html",
    "href": "publications/2024-contraceptives-discontinuation/index.html",
    "title": "Using Causal Inference to Investigate Contraceptive Discontinuation in Sub-Saharan Africa",
    "section": "",
    "text": "IJCAI 2024: Special Track on AI for Good"
  },
  {
    "objectID": "publications/2024-contraceptives-discontinuation/index.html#citation",
    "href": "publications/2024-contraceptives-discontinuation/index.html#citation",
    "title": "Using Causal Inference to Investigate Contraceptive Discontinuation in Sub-Saharan Africa",
    "section": "Citation",
    "text": "Citation\n@inproceedings{akinwande2024using,\n  title={Using Causal Inference to Investigate Contraceptive Discontinuation in Sub-Saharan Africa},\n  author={Akinwande, Victor and MacGregor, Megan and Cintas, Celia and Karavani, Ehud and Wei, Dennis and Varshney, Kush and Nepomnaschy, Pablo},\n  booktitle={International Joint Conference on Artificial Intelligence},\n  year={2024}\n}"
  },
  {
    "objectID": "publications/2024-propensity-score-calibration/index.html",
    "href": "publications/2024-propensity-score-calibration/index.html",
    "title": "Improving Inverse Probability Weighting by Post-calibrating Its Propensity Scores",
    "section": "",
    "text": "Honorable mention: post-calibration procedures are now available in the renowned R package WeightIt v.1.0.0."
  },
  {
    "objectID": "publications/2024-propensity-score-calibration/index.html#citation",
    "href": "publications/2024-propensity-score-calibration/index.html#citation",
    "title": "Improving Inverse Probability Weighting by Post-calibrating Its Propensity Scores",
    "section": "Citation",
    "text": "Citation\n@article{gutman2024improving,\n  title={Improving Inverse Probability Weighting by Post-calibrating Its Propensity Scores},\n  author={Gutman, Rom and Karavani, Ehud and Shimoni, Yishai},\n  journal={Epidemiology},\n  pages={10--1097},\n  year={2024},\n  publisher={LWW}\n}"
  }
]