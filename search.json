[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ehud Karavani",
    "section": "",
    "text": "Hi there!\nI’m a researcher and data scientist, specilizing in causal inference, machine learning, (Bayesian) statistics, and data visualization.\nI’m currently a Research Staff Member at IBM Research, Israel in the Causal Machine Learning for Healthcare & Life Sciences group. There, I focus on high-throughput causal inference for finding new indications for existing drugs using electornic health records and insurance claims data. I’m also the creator and maintainer of causallib, a one-stop shop open-source Python package for flexible causal inference modeling.\n\nEducation\nI have a BSc and MSc in Computer Science and Computational Biology from the Hebrew University. I did my Master’s thesis with Dr. Shai Carmi studying prediction of traits using DNA and its potential effect on selecting embryos for implantation in IVF (aka “designer babies”) .\nPrevious to that I was an undergraduate research associate in Prof. Hanah Marglit’s lab, developing methods for finding novel protein-RNA interactions using RNAseq data ."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "List of blog posts\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHello World\n\n\nThat’s the subtitle\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nCausal inference is a mindset\n\n\nCausal inference from observational data is a mindset, not a set of tools.\n\n\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2023\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCausal Inference with Continuous Treatments\n\n\nGeneralizing inverse probability weights for non-categorical treatments.\n\n\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy we care for covariate balancing in comparative studies\n\n\nBalancing variables in statistical comparative analysis is a proxy, not a goal.\n\n\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nA visual way to think of macro and micro averages in classification metrics\n\n\nExplaining what macro-average and micro-average metrics are.\n\n\n\n\nmachine learning\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2021\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nUsing machine learning metrics to evaluate causal inference models\n\n\nReinterpreting known machine learning evaluations from a causal inference perspective, focusing on ROC curves for propensity models.\n\n\n\n\ncausal inference\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nThe case against Agile research\n\n\nEver popular iterative development approaches can sneak in unconscious-bias that can be harmful to the scientific process.\n\n\n\n\nmeta-science\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2020\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nSolving Simpson’s Paradox with Inverse Probability Weighting\n\n\nA visual intuition on how the most popular method in causal-inference works, and how it solves one of the most popular paradoxes in statistics.\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2020\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nApplying Deep Learning to Genetic Prediction\n\n\nWhat classical methods for obtaining polygenic (risk) scores lack, and how deep learning might help mitigated these shortcomings.\n\n\n\n\ngenetics\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2018\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023/01/04-causal-inference-is-a-mindset/index.html",
    "href": "blog/2023/01/04-causal-inference-is-a-mindset/index.html",
    "title": "Causal inference is a mindset",
    "section": "",
    "text": "I’m a causal inference enthusiast, practitioner, and advocate (borderline missionary if we’re completely honest). I believe people are causal thinkers who ask causal questions and expect causal answers; that we understand the world through causal relationships and causal mechanisms. But when people come to me for help or education, they usually end up disappointed. They come for a solution and discover more questions. They come for a code library to import and end up with a study design to implement.\nThis is because causal inference is first and foremost a mindset, not a particular tool. There is no neural network architecture that will just spit out causal effects. On the contrary, if you organize your data properly, causal inference can be a mere average or a simple linear regression at most.\n\nExcellent study designs make simple analytics.\n\nThis is why causal inference is a bait and switch scheme. We talk a lot about causality but then provide associational statistical tools. And once we estimate a statistical association, claiming it is causal is actually a leap of faith. Of course we have mathematical theory to ground on, but its assumptions are untestable. So at the end of the day, causal claims from observational data are an educated leap of (rational) faith.\n\nCausation = Association + Logic\n\nCausation consists of identification and estimation. Deep learning driven thinking has somewhat sidelined practitioners from thinking of identification. We rarely consider whether the question is even solvable, or is it solvable with the data we have; and if so — what approach or tools can solve it. We easily throw it to a kitchen sink of neural networks to get an answer— we focus exclusively on estimation, forgetting some problems cannot be solved by a single prediction model. But estimation can get you no further than association, and without identification you can’t get causation. Causality requires embracing the beyond-the-data logic that transcends an association into causation, but it’s a hard pill to swallow— and even harder to convince others to join — in the current Big-Data♡AI climate.\nPeople expect a magic library to import in R or to conjure up a model in PyTorch like some causal alchemists. But then I shoot up my slides and start blubbering about causal roadmaps, target trial emulation, careful time-zero considerations and variable selection. People are so used to post-AlexNet machine learning approaches, they are baffled that I have so much to say about study design and DAGs.\nThis is not to say that causal estimation models are bad, on the contrary, they are great (there’s a good reason why I champion making flexible causal models approachable with causallib); they make the counterfactual prediction explicit and clear. This also doesn’t mean causal estimation is easy, far from it — I’m a strong proponent for sequeling The Book of Why with The Book of How — causal learning suffers from the same complexities machine learning does and then some. Also, my intention is not to bash deep-learning, which I myself sometimes use for causal effect estimation. But overcoming confounding bias given observed confounders is just one stair in the staircase towards claiming causation.\nIn the trenches, causality doesn’t always stand up to its hype because it is different from machine learning. The truth is that causal inference is more of a mindset, than a particular tool. And this truth is a tougher sell."
  },
  {
    "objectID": "blog/2021/11/20-why-balance-covariates/index.html",
    "href": "blog/2021/11/20-why-balance-covariates/index.html",
    "title": "Why we care for covariate balancing in comparative studies",
    "section": "",
    "text": "Comparative analysis is the scientific method for discovering effectiveness. Whether it is in medicine, economics, or elsewhere, we perform studies that collect information and compares two groups in order to see which is better and if we can learn any means to improve. These studies can be experiments where we control for the group assignment via randomization, or observational studies where participants self-assign.\nOne of the first tasks in analyzing data from such studies will be to compare the characteristics of the two groups. It isn’t called table one for nothing. In which, we summarize the distribution of variables between the two groups and might even perform a statistical test on how the groups differ. There’s a long-lasting debate on whether that’s a good practice or not. In general, biostatisticians tend to avoid it, while economists tend to prefer it. The recent Data Colada analysis of Dan Arielly fraudulent data — which part of the evidence was big baseline differences albeit the randomization — reopened this Pandora box, pitting academics against one another on Twitter.\nIn this post is my theory on why we compare balancing in baseline characteristics between groups.\n\nHow we infer causality\nFormalizing causality is a task so hard, it left philosophers baffled to this day. In the Rubin’s causal model, we have two unobserved quantities: \\(Y^1\\) is the hypothetical outcome a person would have had — had he been assigned a treatment, and \\(Y^0\\) is the hypothetical outcome that that same person would have had had he not been treated. Once we have these two hypothetical trajectories, two parallel universes in which everything is equal except the treatment assignment of that person — we can isolate the impact of the treatment. If we observe the outcome in both worlds, we can see the causal effect of the treatment — the changes in outcomes that were caused only due to the treatment.\nHowever, in reality — outside the Marvel Cinematic Universe of Rick and Morty — we don’t have access to this multiverse. People either get treated or they don’t.\nAnd so, because we can’t both give and not-give a treatment to the same person, the next best thing is to give a group of people the treatment and another group a placebo and compare the two groups. And this is where we start regarding group balancing. It is only but intuitive that we will want the groups to be identical in all regards, except to the treatment assignment, so we can isolate the contribution of the treatment, and cancel out changes in outcomes that may be due to differences in baseline characteristics.\n\n\nComparing factors is theoretically redundant\nHere, I said it. There are no theoretical justifications to compare the distribution of covariates between groups.\nStatistical theory of causal inference lays out three conditions in order to obtain causal effects from data: positivity, consistency, and exchangeability/ignorability. The crux of the argument considers the latter.\nIntuitively, the exchangeability assumption captures the notion that there shouldn’t be any systematic differences between the groups.\nMathematically, the exchangeability assumption regards the independency of the potential outcomes from the treatment assignment: \\(Y^0, Y^1 \\perp \\!\\!\\! \\perp A\\) .\nYou see what’s in there? This means thatit is the distribution of potential outcomes that should be balanced between groups, not the baseline factors!\n\n\nWhy we still do it\nIn case you missed it, the necessary exchangeability condition above regards potential outcomes — a hypothetical quantity we can't really observe.\nAnd if we can't observe it, we can't test to validate whether it holds in any particular case.\nHowever, we do have some prior understanding of how the world works. For example, clinicians may know that the risk for stroke is dependent on one's age, sex, and history of blood pressure; economists may know that one's salary depends on their past salaries and education level. Therefore, we are able to, at-least mentally, construct some function from baseline factors to the outcome. In turn, this means that if the variables \\(X\\) are balanced between groups then \\(f(X)\\) (which is some projection of them onto a scalar) is also balanced between groups, which suggests the potential outcomes are balanced between groups.\n\n\n\nA mental model of how we move from the available balancing of covariates to the desired balancing of potential outcomes. Because the outcome is a function of the covariates, we are willing to hypothesize that balanced covariates are likely to result in balanced potential outcomes.\n\n\nTesting for covariate balancing is an educated mental shortcut we do. A sort of an ambiguous control test — good balancing is reassuring but not definitive, bad balancing is alarming but not devastating. We just grasp on whatever data we can observe (covariates) and try to use our expertise, common sense, and logic to project it onto what we can't observe (potential outcomes).\n\n\nConclusions\nCovariates will be balanced due to randomization, but it is not the main reason why we randomize. Balanced covariates are just a convenient side effect while we try to balance the distribution of potential outcomes.\nTherefore, it is important to understand that testing for covariate balancing is not the end goal. It is, at best, a proxy for what we really want to know but cannot have. And being the humans that we are — instead of doing nothing about it, we try doing our best.\n\n\n\nAppendix: what can we benefit from covariate balancing tests and at what cost?\nI hope this explained why a debate exists regarding balancing tests. There's no factual right or wrong, only a debate on whether that mental leap from covariates to potential outcomes is uncalled for or not.\nIn my opinion the criticism against it is valid: First and foremost, variables must not be balanced — it the potential outcomes that should. Second, even if they are not — this is what statistical adjustment is for. Employing statistical tests in randomized data is even more baseless — there are no two population, just groups randomized from the same population — the null hypothesis is known to be true. Besides, for a large enough sample even meaningless small differences can become statistically significant, but it doesn't mean they are of practical importance. And for large enough number of covariates, some may pass the statistically significant threshold simply by chance of sampling (a false-positive, or type-1 error), rather than inherent difference in the characteristics between groups. Therefore, having statistical tests (and p-values) in a table one may be considered redundant or even counterproductive — causing confusion by focusing readers — not all of whom are statistical experts — on unessential information.\nHowever, the criticism for it is also valid: randomization might not be that simple to pull off (especially blindly, across multiple facilities, etc.) and humans do tend to err whenever they have the chance to. What guarantees do we have randomization was applied correctly? Do we blindly believe the process? Shouldn't we validate it? Unfortunately, verifying random assignment is near-impossible due to, well, the randomness inherent to the process. There are explicit tests for randomness but checking for balancing is a simple one to employ. Again, it can only hint a problem exist, flagging out more attention is deserved, but it cannot be definitive.\nTherefore, in my opinion, when working on randomized data, it's ok to test for balancing as a sanity check, but don't show it off (and if you do, bury it in an appendix where no one will read it anyway. Just like this section. Consider commenting with a penguin if you do?). It can be misleading at worse or be an avenue for misguided criticism at best (don't feed the trolls)."
  },
  {
    "objectID": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html",
    "href": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#background",
    "href": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#background",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Background",
    "text": "Background\n\nThe fundamental problem of causal inference\nEvaluating causal inference models is literary impossible. Few scientific concepts are so pompously named — yet accurately describe the gravity of an issue — as the notorious “fundamental problem of causal inference”.\nBriefly, the prediction task in causal inference is different than that of supervised machine learning (ML). While in ML we interpolate the target to new unseen samples, in causal inference we extrapolate the target from units in one group to units in the other group. Because in any given time a unit can only be in one group and not the other (e.g., you either have received a drug or you haven’t), we lack the ground-truth labels to compare against our predictions. Counterfactual outcome prediction cannot be derived like regular supervised prediction, nor can it be evaluated as one.\n\n\nCausal models as meta-learners\nMost causal inference algorithms usually have some machine learning core — a statistical model that predicts the outcome or treatment assignment. Once a mapping between features to targets is obtained, causal models can then have various ways to indirectly apply those statistical predictions to obtain a causal estimate.\nFor example, inverse probability weighting (IPW) is a causal model that estimates the causal effect by first modelling the treatment assignment. It takes any machine learning classifier that can also output a continuous score between 0 and 1 and assume it to model the probability of being treated: \\(\\hat{p}=\\Pr[T|X]\\). It regresses the binary treatment assignment (\\(T\\)) against the features (\\(X\\)), then takes the inverse of that predicted scores and use them to create a weighted average of the outcome.\nHaving this machine-learning backbone allows us to interrogate it using commonly known metrics from machine learning; and just like IPW adjusts a binary classifier to obtain a causal estimate, we can adjust these ML metrics to obtain a causal-inference-relevant view.\nThis post is a an effort to breakdown a larger manuscript into bite-size chunks, and will focus on what ROC curves can tell us about propensity models.\n\n\nROC curves recap\nClassifications models can be evaluated for their calibration — how well they behave as probability models — and for their discrimination — how well they separate positive from negative examples. AUC is a metric for discrimination. A more in-depth overview is slightly out of scope for this article, but I do want you to keep in mind two ways for generating ROC curves from a list of prediction scores and labels.\nFirst view is the naïve one. For each possible threshold we will calculate the true-positive and false-positive rates, plotting that point in ROC space. Note that the TPR and FPR can be affected by the weight each unit contributes to the classification, which is not necessarily 1.\nSecond view is more computationally efficient. It involves sorting the scores and traversing the list such that each positive unit moves you one step up and each negative unit moves you one step right. The size of the step is correspondingly determined by the fraction of positive and negative units, but we can weigh each unit so that the step size changes arbitrarily.\n\n\n\n\nObtaining an ROC curve from scores. On the left (a) an explicit view of threshold (taken from Dariya Sydykova). One the right (b) a computationally efficient view (taken from ML wiki)\n\n\n\nIn our case, the prediction scores are the propensity estimations (probability to be in the treatment group) and the labels are the actual treatment assignment. Moreover, controlling the ROC space through sample-weighting is the basis for the additional ROC curves to be presented."
  },
  {
    "objectID": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "href": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#classification-metrics-for-propensity-models-overfit-underfit-and-positivity-violations",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Classification metrics for propensity models — overfit, underfit, and positivity violations",
    "text": "Classification metrics for propensity models — overfit, underfit, and positivity violations\nComing from machine learning, this can be somewhat counterintuitive, so let’s get done with it right out of the gate: good prediction performance usually suggests a bad propensity model and a bad causal model downstream. Propensity scores should not be able to discriminate well between the treatment and control units.\nIf you’re lucky, your good prediction performance is due to good-old overfit. You can use your ML knowledge to solve for that. Causal inference models are prone to all the same pitfalls in statistics, they are simply blessed with a few additional ones.\nIf you’re not lucky, your good discrimination ability may hint you have a positivity violation in your data. Positivity is an essential assumption if wanting to extrapolate outcomes across treatment groups, as in causal inference. It states that the treated should have some chance (i.e. positive probability) to be in the control group and vice versa. In other words, the groups should have some common support — in each subspace of features we should have both treated and control units, so both groups have their covariates overlap. Otherwise, how could you generalize the predicted outcome from the treated to the control if all treated units are males and all control units are females? Perfect discrimination between treated and controls suggests the groups occupy mutually exclusive regions in feature-space violating a necessary assumption for causal inference.\nConversely, bad discrimination performance is not necessarily bad. It might simply suggest the treatment groups are well mixed — an encouraging step towards the validity of a causal analysis. However, it might also be due to underfit. The response surface of treatment assignment might be a complex one to model. Therefore, you should experiment in iteratively increase the expressiveness of your model to the point you overfit just to verify it is indeed the data that is balanced and not the model that is under-specified.\nSolving for lack of overlap is possible, but out of scope for this post. Just to namedrop a few strategies: you should revise the inclusion criteria of your data, rethink your confounder selection, stratify your analysis on highly predictive features, or use domain knowledge to thoughtfully help you extrapolate through mechanism rather than data.\n\nROC curves for propensity models\nFocusing on propensity-based causal models, we have three relevant ROC curves: the regular one based on propensity scores and two novel curves created by reweighting the scores. They all work in tandem, and I’ll present each one: how to obtain them and how to interpret them.\n\nVanilla ROC curve\nHow: This is the regular ROC curve simply obtained by taking the propensity scores against the binary treatment assignment.\nroc_auc_score(t, p_hat)\nInterpretation: We already discussed the issue that the AUC should not be too high as it suggests good discrimination, which is bad for causal inference. The ROC curve allows us to detect such regions of perfect discrimination. Ideally, there should not be long vertical or horizontal segments in the curve. A sharp long vertical contour suggests there’s a bunch of data points for which we only get true positives (upward movement) without paying any false negatives (rightward movement). That is, the treated units are very separable from the untreated — they are not well-mixed. Reiterating the above: this can hint that we have a positivity violation in the feature subspace that is mapped into this region of scores (thresholds) causing the vertical line.\n\n\nIP-weighted ROC curve\nHow: in this curve we weight the contribution of each unit’s propensity score to the ROC curve by the corresponding inverse-probability weight of that unit.\nip_weights = (t / p_hat) + ((1 - t)) / ((1 - p_hat))\nroc_auc_score(t, p_hat, sample_weight=ip_weights)\nInterpretation: Ideally, like every post-weight discrimination metric, we should expect a random-like performance. Namely, a ROC curve that aligns with the diagonal and an AUC around 0.5.\nIntuition: This curve shows how well the weights balance the groups. IPW creates a pseudo-population in which the treated and control have similar characteristics — it weighs the sample so that in each region in the feature-space we should have similar amount of (weighted) units. If we were to apply a classifier in this weighted population, it would be difficult to discriminate the treated from the controls. For example, if we have the same amount of males and females we can’t use sex as a predictive feature, and if we have the same amount of young and adults we can’t use age, etc. Therefore, poor discrimination post-weighting is welcomed.\n\n\nExpected ROC curve\nHow: We obtain this curve by weighing the scores such that each unit contributes its propensity score to the positive label (treatment group) and its complementary score (1 minus propensity) to the negative label (control group)\nweights = concatenate([p_hat, 1 - p_hat])\nt = concatenate([ones_like(p_hat), zeros_like(p_hat)])\np_hat = concatenate([p_hat, p_hat])\nroc_auc_score(t, p_hat, sample_weight=weights)\nInterpretation: Ideally, we would want the expected propensity to align with the vanilla (unweighted) propensity curve (and have same AUC).\nIntuition: The propensity-to-be-treated is never observed, we only see one instantiation of it in the form of treatment assignment. However, we can model the average propensity of units with similar features. If we assume the statistical model represents the true propensity, then we move from a binary classification task to a smoother calibration-like task where units with high confidence (extreme propensity) contribute almost like they would in the vanilla ROC curve, and low-confidence units (propensity around 0.5) contribute a segment parallel to the diagonal.\n\n\n\nA view of the propensity ROC curves. Blue: the unweighted propensity score. Orange: the inverse-probability weighted curve of the propensity. Green the Expected propensity curve ."
  },
  {
    "objectID": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "href": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#connection-to-propensity-distribution-plots",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Connection to propensity distribution plots",
    "text": "Connection to propensity distribution plots\nTraditionally, practitioners will plot the propensity distribution, colored by the treatment and control groups, and look for overlap. ROC curves are another view of that propensity distribution.\nThere is a direct transformation from scores distribution to ROC curves, as seen in the figure below taken from Janssens and Martens.\n\n\n\n\nTransforming a distribution of (propensity) scores into an ROC curve [figure from Janssens and Martens].\n\n\n\nAnd the gif below from Dariya Sydykova show how separability of scores affect how sharp the curves are.\n\n\n\n\nThe effect of separability of (propensity) scores on the sharpness (i.e. amount of long vertical/horizontal segments) of the ROC curve [figure by Dariya Sydykova].\n\n\n\nFollowing this perspective, the propensity histogram weighted by the inverse propensity serves the same purpose. The bar heights are no longer determined by the number of individuals in each bin, but by their accumulated weights. In the weighted scheme (right), the bars corresponding to the same propensity bucket (i.e. x-axis bin) have the same height in the treatment and control groups, relative to the unweighted version (left) in which the heights of the same bins differ.\n\n\n\nInverse-probability-weighted propensity histogram (right) has corresponding bars slightly more similar in height then the regular (unweighted) propensity histogram (left).\n\n\nHowever, I would argue that viewing this in ROC space provides an easier interpretation, since we can convert the fuzzy notion of “distribution overlap” to a concrete AUC score."
  },
  {
    "objectID": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#summary",
    "href": "blog/2020/12/28-ml-metrics-for-causal-inference/index.html#summary",
    "title": "Using machine learning metrics to evaluate causal inference models",
    "section": "Summary",
    "text": "Summary\nWe have seen how to interpret pre-weighting classification metrics (good performance is bad) and post-weighting classification metrics (bad performance is good).\nI focused on ROC curves for propensity models, presented two novel curves and discussed how to interpret them. \nHere are three take-aways for three curves:\n\nRegular ROC curves should not have sharp, long vertical segments.\nInverse-probability weighted AUC should be around 0.5.\nExpected AUC should be close to the regular AUC.\n\nThese presents an off-the-shelf intuitive measure to verify a causal model is not omitting complete nonsense. Using such simple AUC-based criteria can be implemented to automatically select causal inference models that perform better than others through cross-validation, similar to how we apply model selection in machine learning.\nI believe that deploying a propensity model and examining its behavior is beneficial in any causal inference analysis. Even if you end up modeling the response surface directly without using the propensity scores, it can still provide meaningful insights into the structure of the data and the assumption needed for a valid causal conclusion.\nFor additional thoughts and evaluations, see our preprint: https://arxiv.org/abs/1906.00442."
  },
  {
    "objectID": "blog/2020/02/22-simpsons-paradox-ipw/index.html",
    "href": "blog/2020/02/22-simpsons-paradox-ipw/index.html",
    "title": "Solving Simpson’s Paradox with Inverse Probability Weighting",
    "section": "",
    "text": "Originally published on  Medium.\n\nStatisticians love using the word “paradox” to describe simply unintuitive results, regardless of how much it upsets their fellow logicians. To get back at them, we’ll apply causality to solve one of their most famous paradoxes — Simpson’s Paradox.\nIn this post, I will briefly introduce what IPW is in the context of causal inference and present a simple intuition to how it works. I will then provide a popular example of the paradox from the medical domain and we’ll see, visually, how IPW solves it.\nWant to skip the details? Scroll to the end of the article.\n\nCausal Inference with IPW\nIPW, short for Inverse Probability (sometimes Propensity) Weighting, is a popular method for estimating causal effects from data. It’s a simple yet powerful tool to eliminate confounding or selection bias. To keep it casual (ha!), let’s introduce it via a simple hypothetical example of estimating the causal effect of drug \\(D\\) on the risk of stroke.\nFor that, we’ll make a quick detour through randomized control trials.\n\nRandomized Control Trials\nUsually, to estimate the causal effect of a drug, we would construct a Randomized Control Trial (RCT). This means, we would recruit people and flip a coin to assign them either to take drug \\(D\\) or to take placebo (control). Then we would measure the rate of stroke in each group, compare them, and conclude whether \\(D\\) increased or decreased the risk of illness.\nWe know that there are multiple contributing factors to stroke. For example, sex (vascular systems can be differentiated between males and females) and age (veins tend to clog with time). The reason we could simply compare the two groups and disregard those other factors has everything to do with the randomization we applied.\nThe randomization creates a similar distribution of age and sex between the two groups (on average). Consequently, when comparing the groups, the contributions of those variables cancel themselves out. The only parameter consistently different between the groups is whether they took \\(D\\) or placebo, and therefore, the differences we observed in the risk can be contributed only to \\(D\\), making them the causal effect of \\(D\\).\nWe can decompose the risk of stroke in our simple example into a slightly more concise mathematical notation:\n\\[\n\\begin{array}{c}\n\\text{risk stroke in treated}=\\text{risk due to age} + \\text{risk due to sex} + \\text{risk due to }D \\\\\n- \\\\\n\\text{risk stroke in control}=\\text{risk due to age} + \\text{risk due to sex} + \\underbrace{\\text{risk due to placebo}}_{=0} \\\\\n= \\\\\n\\text{risk due to }D\n\\end{array}\n\\]\nWe compare the risk between between our groups by taking the difference.\nSince age and sex are distributed similarly between groups, they contribute the same risk in both groups and so they cancel themselves out. Since placebo is a small sugar pill, its contribution is zero. Hence, we are left only with \\(\\text{risk due to }D\\).\nVoilà! The causal effect of \\(D\\) on stroke.\n\n\nCausal Effect from Non-Experimental Data\nHowever, performing a randomized control trial costs money and takes a long time (among other disadvantages). Still paying our student-loan, we don’t have the resources to conduct such an experiment. What we do have is data, because data is becoming ever cheaper and HMOs collect them easily.\nThe problem with such observational data from HMOs is that it no longer comes from our nice experimental distribution. Unfortunately for us (but really luckily for us), physicians are not random. They assign treatments based on our characteristics (say, age and sex). Therefore, there might be an overall tendency to prescribe certain groups with one drug and not the other. In this case, if we were to simply compare those who did take D with those who did not, the distribution of those factors will not necessarily be the same and so their contribution will no longer cancel out.\nConsequently, the “effect” we’ll observe will no longer be the causal effect of D, but rather a quantity entangling both causal and non-causal impacts, essentially contaminating the causal effect of D with the contribution of those other factors.\nTo estimate the true causal effect, we’ll first need to make the two groups comparable, and the way to make them comparable is where causal inference, and specifically IPW, comes into play.\n\n\nInverse Probability/Propensity Weighting\nNow that we have set the scene, we can finally present what IPW is. As we said, IPW stands for Inverse Propensity Weighting. It’s a method to balance groups by giving each data-point a weight, so that the weighted-distribution of features in first group is similar to the weighted-distribution of the second one.\nWe mentioned that physicians don’t prescribe drugs randomly, but rather base it on the features of their patients. Therefore, each patient will have a different likelihood to be prescribed to D, based on their characteristics. This likelihood is referred to the propensity to be treated. To put it mathematically, if we mark our patient features as X, the propensity is the probability of patients getting or not getting the treatment: \\(\\Pr[D|X]\\). Once we estimated this probability to treat, the weight we assign is simply its inverse: \\(1 / \\Pr[D|X]\\).\nWhen we have a large number of features, we will need some machine learning model to crunch all those high dimensional data into one probability scalar. But in order to see why this process even results in a balanced population, let’s take the simple example with one feature, say being an adult male.\n\nIPW by a Simple Example\n\n\n\nOur original population is unbalanced because we have more untreated adult males than treated ones. If we were to compare the groups, we wouldn’t be able disentangle the contributions of drug and sex, and tell whether the observed effect is due to being treated or due to being male.\n\n\nExamining the distribution in the above figure, we see that our groups are imbalanced with regard to males. Therefore, if we were to simply calculate an average risk in each group, we would not be able to say whether the difference we see is due to being treated or simply because of being a male.\nOur first step is to calculate the probability of each individual to be in the group they are actually assigned to. We have 5 men in total, 1 treated and 4 that are not. Hence, we can make a simple estimate that the probability for males to get the drug is ⅕ and the probability for males to not get the drug is ⅘.\nOur second step is to inverse those probabilities and assign them to each individual. Homer, therefore, getting a weight of 5, while Moe, Barney, Groundskeeper Willie, and Principal Skinner each get a weight of 5/4. We basically create a pseudo population where we have 5 Homers, and we have 5/4 of each of the untreated — meaning we have one Moe, one Barney, one Willie, and one Skinner, and another hybrid-person which is ¼ of each. Making that a total of 5 treated and 5 controls.\n\n\n\nOur pseudo-population includes a similar number of adult males (note the duplicated Homer and the hybrid untreated), so when comparing the groups - the effect of adult-males will cancel it-self and we’ll be left only with the effect of the treatment.\n\n\nSee, we were able to create a population in which males are evenly distributed between groups.\nIn real life, of course, there will be more than one feature to handle, and that’s why we’ll need a model to estimate \\(\\Pr[D|X]\\).\nTL;DR\n\n\n\nIPW takes an unbalanced population and creates a balanced pseudo-population (Simpsons components from Wikipedia).\n\n\n\n\n\n\nSimpson’s Paradox\nBy now you might have a hunch how we can use IPW to solve Simpson’s paradox, but before we do, let’s briefly introduce what this paradox is all about.\nSimpson’s Paradox, a term coined by Blyte¹, is named after Edward Simpson, the first statistician to explicitly point to this problem. In short, the paradox happens when an overall average trend in the population is reversed or canceled-out when examining its composing sub-groups.\nThe intuition in the continuous case is very clear, as suggested by this GIF:\n\n\n\nA visual intuition on how a trend in the overall population can reverse itself in the composing sub-populations.\n\n\nTo get a better understanding of the phenomena, let’s examine a real-world example:\nAccording to the paper, we have two ways to treat kidney-stones. Either with an open-surgery (A) or with a new non-invasive method using shock-waves (B). We gather medical records from 700 people, 350 from each treatment-group and compare their success rates.\nWishing to conclude which method is better, we compare the success rates among group A and B and see they are 78% and 82.5%, respectively. Naively, we want to deduce B is better (by 4.5%), but we remember we read somewhere (where?) that physicians are not random. We suspect there’s probably some reason as to why patients got a certain treatment but not the other, and that it probably has to do with their prior medical condition.\nLo and behold, when we split the patients based on stone size — we see a different trend. Suddenly it is treatment A that is better for both small stones and large stones — 93.1 and 73% respectively.\nThe averaging process probably masks some vital information, so let’s look at the raw numbers.\nTreatment A has 81/87 (93.1%) success rate for small stones and 192/263 (73%) for large stones. Meanwhile, treatment B has 234/270 (86.67%) success rate for small stones and 55/80 (68.75%) for large stones.\n\nB has better success rate overall, but A is better in both small and large kidney stones. This happens because B got the majority of easy cases (small stones), while A got the majority of hard cases.\n\n\n\n\n\n\n\nStone size \\ Treatment\nA\nB\n\n\n\n\nBoth sizes\n273/350 =\n78.0%\n289/350 =\n82.5%\n\n\nSmall\n81/87 =\n93.1%\n234/270 =\n86.67%\n\n\nLarge\n192/263 =\n73.0%\n55/80 =\n68.75%\n\n\n\nDo you see what the denominator is hiding? The game is rigged. B got the majority of easy cases (small stones), while A got the majority of hard cases. Severity of the patients is not similarly distributed across treatment groups. Since larger stones also have a lower chance of success — because they are more difficult to handle, we find ourselves in a biased situation where comparison of treatments is not fair. This is common in day-to-day scenarios, because when physicians encounter tougher cases (larger stones), they will tend to use bigger guns (open surgery).\nGenerally speaking, a variable that affects both the treatment assignment and the outcome is called a confounding variable. In our case, the severity of patients, expressed by the size of their kidney stones, is a confounder since it increases both the likelihood to be assigned to a harsher medical procedure (a surgery more likely to be effective), and increases the risk of failing (stones not being completely removed).\nIn our final act of this post, we’ll fix this discrepancy in features (severity through stone sizes) between the treated and controls using IPW.\n\n\nUsing IPW to Solve Simpson’s Paradox\nWe have an imbalanced situation whereas the probability to get treatment A if you have small stones is 87/(87+270)=24.4% and if you have large stones it’s 263/(263+80)=76.67%. Similarly, for treatment B it’s 270/(270+87)=75.6% and 80/(80+263)=23.33%.\nTherefore, we can easily calculate the weight of each individual by taking the inverted fraction of people from each group (since we look only at binary stone size, our individuals are basically identical within their groups, so we can use group-level weights). To compute an unbiased success risk, we simply calculate the average of the success risk weighted by these weights:\n\\[\n\\begin{array}{c}\n\\text{For treatment } A: \\left( \\frac{357}{87} \\cdot \\frac{81}{87} + \\frac{343}{263} \\cdot \\frac{192}{263} \\right) /  \\left(\\frac{357}{87} + \\frac{343}{263} \\right) = 88.26\\% \\\\\n\\text{For treatment } B: \\left( \\frac{357}{270} \\cdot \\frac{234}{270} + \\frac{343}{80} \\cdot \\frac{55}{80} \\right) /  \\left(\\frac{357}{270} + \\frac{343}{80} \\right) = 72.97\\%\n\\end{array}\n\\]\nAnd we see that now, even if we aggregate the size of stones, treatment A is better than B (by ~15%). This is consistent with A also being better in every sub-group, and so the reversal we saw before no longer exists.\nWe have solved Simpson's Paradox.\nTL;DR \nin case you haven't read a word, here's the entire post summed up as a GIF:\n\n\n\nBreaking down Simpson’s Paradox visually and solving it by creating an average risk weighted by IP weights. Visualization, by the author, is based on a similar one by John Burn-Murdoch I remember seeing on Twitter"
  },
  {
    "objectID": "blog/2018/05/05-deep-learning-genetic-prediction/index.html",
    "href": "blog/2018/05/05-deep-learning-genetic-prediction/index.html",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2018/05/05-deep-learning-genetic-prediction/index.html#footnotes",
    "href": "blog/2018/05/05-deep-learning-genetic-prediction/index.html#footnotes",
    "title": "Applying Deep Learning to Genetic Prediction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nstill looking at this cute little ImageNet feature space.↩︎\nWhat is science if not baby-stepping all the way to the moon.↩︎\nP-value-based variable selection strategies are bad, see Sun, Shook, and Kay.↩︎\nIt may be interesting to evaluate the model’s performance when different chromosomes will share some of the deeper convolutions. Namely, replacing the DNN box in the next figure with another CNN.↩︎"
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html",
    "href": "blog/2020/07/06-against-agile-research/index.html",
    "title": "The case against Agile research",
    "section": "",
    "text": "Originally published on  Medium."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#agile-methodologies",
    "href": "blog/2020/07/06-against-agile-research/index.html#agile-methodologies",
    "title": "The case against Agile research",
    "section": "Agile methodologies",
    "text": "Agile methodologies\nAgile methodologies are project management approaches becoming increasingly popular in software development. At its core, Agile software development is iterative and incremental. It promises frequent product delivery that can accommodate for the fast pace demand for changes from users. The main idea is to break down the software into independent self-contained components and implement them cyclically, ever improving the product.\nThis bottom-up approach has stemmed as a countermeasure to the more traditional top-down approach, known as Waterfall. In the Waterfall method, the entire system's architecture is first designed, down to the smallest components, and then coded from start to end. This has made software much slower to improve, which is the main reason why the method's popularity is decreasing in our fast-pace computing world."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#research-and-data-science-pipeline",
    "href": "blog/2020/07/06-against-agile-research/index.html#research-and-data-science-pipeline",
    "title": "The case against Agile research",
    "section": "Research and data science pipeline",
    "text": "Research and data science pipeline\nAgile has become so popular that it had percolated outside the realm of traditional software development. Specifically, it had trickled down to research, and more specifically, to data-driven research. The heavier the computational infrastructure of the research, the more likely it is to adapt Agile methodology.\nIn the data-science version of Agile, results are iteratively refined. For example, data definitions are constantly refined, models are iteratively tuned, and reports are continually generated.\nFrom my personal experience, this is especially the case in industry research. As opposed to academia, companies usually subject you to a greater corporate hierarchy. You have several lines of management, each requires periodically updates on the project, usually in the form of a report or a presentation. This demand requires you to set an initial pipeline quickly, just to obtain some results, and then iteratively improve it (refine the data, add analyses, prettify graphs, etc.) until the next meeting or report.\nThis article would like to argue that iteratively reporting should be avoided because it can creep in unconscious bias."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#unconscious-bias",
    "href": "blog/2020/07/06-against-agile-research/index.html#unconscious-bias",
    "title": "The case against Agile research",
    "section": "Unconscious bias",
    "text": "Unconscious bias\nUnconscious (or implicit) bias is a term used to depict one's preference towards prior beliefs on the expense of evidence at hand. It has been affecting science since ever and is the reason why researchers nowadays prefer methods such as blind peer reviews or randomized control experiments. In the former, reviewers aren't affected by the identity of the author, and in the latter, participants don't know whether they get treatment or placebo. Both neutralize a psychological effect that can bias one's response in the process.\nScientists are no exception and subconscious bias can also affect the research protocol and analysis itself. Even in presumably \"pure\" tasks, like measuring physical constants, it has been observed that new measurements tend to cluster around previous measurements, rather than what we nowadays know to be the true value.\nWe can mitigate such unconscious bias by adapting the same trickery we use for blind peer reviews and blind medical trials: blind analysis.\nIn his excellent book Statistics Done Wrong, Alex Reinhart gives a beautiful example of how blind analysis was applied in experimental physics: when Frank Dunnington wanted to measure the ratio between an electron's charge to its mass, it required him to build a dedicated machinery. Dunnington built the entire apparatus but left the detector in a slightly off angle. As the exact measurements are worthless, Dunnington could construct the entire experimental protocol without being exposed to the value he was interested in. Thus avoiding overfitting the protocol to his prior beliefs. Only upon finalizing the analysis plan, had he corrected the off-angled detector and correctly measured the desired ratio."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#blind-statistical-analysis",
    "href": "blog/2020/07/06-against-agile-research/index.html#blind-statistical-analysis",
    "title": "The case against Agile research",
    "section": "Blind statistical analysis",
    "text": "Blind statistical analysis\nTo be honest, there's no straightforward equivalent to a dislocated detector in the data science pipeline. One might use mock data, or a very (very) small random subset of the data they can later discard. The data will be used just to verify the code runs end to end, outputs are correctly saved, errors are correctly logged, graphs look as intended, etc. Only when the pipeline is finalized will one run their code on the entirety of the data, generating results.\nTo quote Alex: \"no analysis plan survives contact with the data\", and I will add that technical issues should be debugged and solved as data blind as possible."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#avoiding-agile-research",
    "href": "blog/2020/07/06-against-agile-research/index.html#avoiding-agile-research",
    "title": "The case against Agile research",
    "section": "Avoiding Agile research",
    "text": "Avoiding Agile research\nIn research, our product is usually some kind of a report. Therefore, in Agile-like research, we continuously generate periodic reports and therefore constantly re-estimating our estimand of interest. The more rounds we go, the greater the chances to be affected by the ever-lurking subconscious bias toward desired results.\nFurthermore, changing analysis plan as we go, especially when based on half-baked analyses, can wildly increase false positive discoveries. For example, if we decide to change the primary outcome we measure in the middle of the project, just because we saw (or any management reading your monthly report suggested) there's no effect, it will direct us into analyzing our data, rather than analyzing the phenomena the data measure.\nData are like a perishable resource. Each time you glance at it, the quality of the story it tells deteriorates. Soon you'll be reading whatever you want to read, rather than what the data wants to tell.\nSimilarly, Results are just a set of transformations applied to the data and glancing at them is like glancing at the data via proxy. Namely, that too drains out our data resource, making it less reliable."
  },
  {
    "objectID": "blog/2020/07/06-against-agile-research/index.html#summary",
    "href": "blog/2020/07/06-against-agile-research/index.html#summary",
    "title": "The case against Agile research",
    "section": "Summary",
    "text": "Summary\nAs short-cycle iterative management methodology become increasingly popular in research, so does our susceptibility to subconscious bias. This is not to say Agile-ish methodologies are never to be used in research or data science, only that they require some adaptations. Adaptation like more careful data management, avoiding peeping at the results just like we avoid peeking at the data, and pre-registering the analysis plan.\nEventually, the scientists themselves also adapt to the evidence they work with, just like any other statistical model adapts to the data. Therefore, scientists should limit their own access to data and results, just as they do to the models they apply, to avoid overfitting their own analysis to their desires or prior beliefs"
  },
  {
    "objectID": "blog/2021/09/04-visualizing-micro-macro-averages/index.html",
    "href": "blog/2021/09/04-visualizing-micro-macro-averages/index.html",
    "title": "A visual way to think of macro and micro averages in classification metrics",
    "section": "",
    "text": "For completeness, this article spends most its words explaining what a confusion matrix is. If you already familiar, you can probably understand the article by just scrolling down through the pretty pictures.\n\nConfusion matrix\nAlmost every classification metric can be defined by a confusion matrix. Those that cannot — can be defined by several. This makes the confusion matrix the most basic way to evaluate classification models.\nEvaluating a binary classification, we can either be right or wrong. But we can cross-tabulate the four possible combinations of predicted labels and true labels into a contingency table. On the diagonal, we count the correct predictions, and off the diagonal we count our mistakes.\nFirst, we can be correct on the positive label, these predictions are truly positive (TP). Second, we can be correct on the negative label, making these predictions truly negative (TN). But correct predictions are all alike; every wrong prediction is wrong in its own way. The first type of error is type-1 error — positive predictions that actually belong to the negative class — these are falsely positive predictions. Lastly, the fourth combination and the second type of error, we can wrongly predict the positive label to be negative — these predictions are falsely negative.\n\n\n\nConfusion matrix is a contingency table enumerating the combinations of predictions and true labels. It defines four important building blocks for most classification metrics: TPs, FPs, FNs, TNs.\n\n\nThis four-cell matrix can define a plethora of metrics. For example, accuracy can be defined by summing the TP and TN and dividing by the sum of the matrix; sensitivity is the TP divided by actual positive observations; specificity is similar for the negatives: dividing TN by the actual negative observations; and there are much, much more. Especially when we can also use the metrics themselves to create compound metrics, the sky is the limit.\n\n\nMulticlass confusion matrix\nConfusion matrices can be naturally expended into multi-class classification. Instead of a 2-by-2 table, we'll have a k-by-k table enumerating the different combinations of predictions and labels. However, the metrics in this multi-class setting are not always well defined. This is because we no longer have a single false positive or false negative count, but rather several ones — one for each pair of misclassified classes.\nTo redefine these metrics for multiple classes, we must first convert our single k-by-k table to k 2-by-2 tables. We simply aggregate the multiple false-positives, false negatives, and true negatives into one of each. This is the same as thinking of our multiclass classifier as multiple one-vs.-rest binary classifiers.\n\n\n\nMulticlass confusion matrix can be reformulated as multiple one-vs-rest 2-by-2 matrices. This will help us redefine what false positives and false negatives are in a multiple\n\n\nWe now have a bunch of 2-by-2 confusion matrices, let's stack them up. Think of it as a volume, or a 3D tensor. In each simple confusion matrix, the metrics — like precision, specificity, or recall — are well defined. However, we need to extract a single metric from them. This is when averaging — micro and macro — come into play.\n\n\n\nWe’ll benefit from thinking of these multiple one-vs-rest confusion matrices as a 3D stack volume of confusion matrices. Then macro- and micro-averages are just the order of axes reduced.\n\n\n\n\nMacro-averaging\nIn macro-averaging, we first reduce each of the k confusion matrices into a desired metric, and then average out the k scores into a single score.\n\n\n\nIn macro-average, we first calculate a metric from each confusion matrix, and then average out the scores of these metrics.\n\n\n\n\nMicro-averaging\nIn micro-averaging, we will first sum the TPs, TNs, FPs, and FNs across the different confusion matrices, say we denote them as ΣTP, ΣTN, ΣFP, and ΣFN. We then use these aggregated measures as if they form a confusion matrix of their own and use them to calculate the desired metrics.\n\n\n\nIn micro-average, we first reduce the confusion matrices into one summed up confusion matrix, and then calculate the metric from aggregated table.\n\n\n\n\nSummary\nMulticlass confusion matrices can be expanded to stacked one-vs.-rest confusion matrix. Under this formulation, micro- and macro-averages differ by which axis is reduced first. Macro-average first reduces the confusion matrices into scores and then averages the scores. Micro-average first reduces the multiple confusion matrices into a single confusion matrix, and then calculates the score."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html",
    "title": "Causal Inference with Continuous Treatments",
    "section": "",
    "text": "Causal inference, the science of estimating causal effects from non-randomized observational data, is usually presented using binary treatment; we either treat or don’t treat; we give drug A or drug B. There’s a good reason for that, as causality is already complex as it is. However, not all interventions are binary (or discrete).\nSometimes, the interventions we care about are continuous. “Taking a drug”, for example, is fairly vague — drugs have active ingredients and those can come in different dosages. Too little and the drug might not seem effective, too much and the drug might be harmful. Therefore, we might be interested in the effect of different dosages of the same drug. This is often called dose-response modeling.\nContinuous exposures are all around us. From drug dosages to number of daily cigarettes smoked or air pollution levels, from how much time you watched an ad before skipping it to how much red a the “unsubscribe” button is on a newsletter, from the interest rate increased by the central bank to the amount of money in a lottery winning. We can’t limit ourselves to studying binary exposures just because the introduction book didn’t cover the other ones.\nIn this post I will introduce a generalized version of inverse probability weighting for continuous treatment. I’ll show different estimation methods and discuss the required assumptions and its limitations. I will assume you are familiar with causal inference and IPW for binary treatment, but if you are not — I got you covered in this IPW explainer."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#from-binary-to-continuous-treatment",
    "title": "Causal Inference with Continuous Treatments",
    "section": "From binary to continuous treatment",
    "text": "From binary to continuous treatment\nRecall that in the binary treatment setting, a common way to estimate causal effects is by using inverse probability weighting (sometimes called inverse propensity weighting, but I’ll just use IPW). Given individual \\(i\\) with treatment assignment \\(a_i\\) and characteristics \\(x_i\\), its inverse propensity weight is defined as: \\(w_i=1/\\Pr[A=a_i|X=x_i]\\). Namely, the inverse probability of \\(i\\) to be assigned to their treatment, given their characteristics.\nHowever, when treatment (or any random variable for that matter) is continuous, the notion probability mass fails and we need to speak in terms of probability density. This is because the probability of a single point, say \\(a_i\\), is basically 0, while it may still have density associated with it since density is defined as the derivative of the cumulative probability function. This is a fundamental theoretical difference, but we can capture it in a small notation change, instead of \\(\\Pr \\left[A=a_i|X=x_i \\right]\\) we will use \\(f \\left(a_i|x_i \\right)\\).\n\n\n\nGradually approximating a discrete binomial distribution with a continuous Gaussian one."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#modelling",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#modelling",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Modelling",
    "text": "Modelling\nRecall that estimating the treatment effect with IPW is comprised of two main steps. First, model the treatment and obtain IP-weights. Second, model the outcome using those weights. In the binary case, once we have the weights, the simplest way to estimate the potential outcomes is to simply take the weighted average in the treated and untreated (often called the Horvitz-Thompson estimator). However, an equivalent way is to use a simple univariable regression: regress the outcome against the treatment (and an intercept) weighted by the IP-weights. Then, the average treatment effect is simply defined by the coefficient corresponding to the treatment variable. This is often called a marginal structural model in the epidemiology literature.\nNote that in the continuous treatment case, the first option is not applicable. Often, there will be many unique treatment values and it will be rare to have enough samples with the exact same continuous treatment value, for all treatment values. Binning them will solve it, but we’re here for continuous treatment modeling. Therefore, we will need to use the latter option and create an additional (parametric) model between the outcome and the treatment. This will be our dose-response function.\nLet’s examine those two steps in more details.\n\nStep 1: modeling the treatment\nWith categorical treatments, we needed to model the probability of getting treated. We could have done that by regressing the treatment assignment against the covariates, basically using any “classifier” that outputs predictions in the 0–1 interval which we can then interpret as probabilities. Logistic regression, for example is a generalized linear model that is defined by the binomial distribution — a discrete probability function. With continuous treatment, however, we will need a regression model instead. For example, in generalized linear models, a linear regression model is defined by the Gaussian distribution. And as the animation shows above — the more categories a binomial distribution has the better it is approximated by a normal distribution.\nOnce we fitted a model, we can obtain the conditional expectation \\(E \\left[A|X \\right]\\). But unlike the binomial case, in the continuous case, this is not sufficient to generate densities. For simplicity, let’s assume the common Gaussian distribution, which is parameterized by a mean and variance. The conditional mean of that distribution will be the estimated conditional expectations (the predictions); the variance will be constant and will be set to be the variance of the residuals between the treatment and the predictions. Once we defined the distribution, we take the density of the observed treatment values with respect to this distribution. The generalized IP-weights are the inverse of these densities.\nTo summarize step 1:\n\nFit a function \\(g(x)\\), regressing the treatment \\(A\\) on covariates \\(X\\).\n\\(A=g(X)+\\epsilon=\\alpha_0+\\alpha_1 X+\\epsilon\\)\nDefine the conditional distribution \\(D_i \\sim \\text{Normal}(g(x_i), \\text{Var} \\left(a_i-g(x_i) \\right)\\).\n\nThe conditional mean of each sample is its prediction.\nThe variance is fixed and is the variance of the prediction residuals.\n\nDefine the density \\(d_i\\) as the value of \\(a_i\\) from \\(D_i\\).\nDefine the weight \\(w_i\\) to be the inverse of the density: \\(1/d_i\\) .\n\n\n\nStep 2: modeling the outcome\nOnce we obtained the balancing weights w, we can model the counterfactual outcomes using the observed outcomes and treatments. To do that, we regress the outcome against the treatment, weighted by the IP-weights obtained from step 1. However, unlike the binary treatment case, the functional form of the continuous treatment should be flexible enough to avoid bias due to misspecification. For example, we will add a quadratic term of the treatment or model it using a spline, etc.\nWhen we have non-linear transformations of the main treatment variable, we can no longer interpret the treatment effect as the coefficient of the treatment covariate. Instead, to make counterfactual outcome predictions, we will set some treatment value and run it through our model to get the predicted outcome, and average it out across the units to obtain the average outcome had everyone been assigned that specific treatment value.\nWe can repeat that for two different treatment values. Then the causal effect will be the difference (or ratio) between these two potential outcome predictions. Alternatively, we can repeat that for every treatment value in a range we care about and obtain a dose-response curve — see how the counterfactual outcome prediction changes as a function of assigning different dosages.\n\n\n\n\nMarginal Structural Model - regress the outcome on the treatment weighted by the generalized IP-weights. As proposed by Robins, Hernan, and Brumback1.\n\n\n\n\nCode\nBelow is a Python code demonstrating the estimation process described above.\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom causallib.datasets import load_nhefs\n\n\ndef conditional_densities(data, use_confounders=True):\n    formula = \"smkintensity82_71 ~ 1\"\n    if use_confounders:\n        formula += \"\"\" + \n        C(sex) + C(race) + age + I(age**2) + \n        smokeintensity + I(smokeintensity**2) +\n        smokeyrs + I(smokeyrs**2) + \n        C(exercise) + C(active) + C(education) + \n        wt71  + I(wt71**2)\n        \"\"\"\n    model = sm.formula.ols(formula, data=data).fit()\n    density = stats.norm(\n        loc=model.fittedvalues,\n        scale=model.resid.std(),\n    )\n    densities = density.pdf(data[\"smkintensity82_71\"])\n    densities = pd.Series(densities, index=model.fittedvalues.index)\n    return densities\n\n\ndata = load_nhefs(raw=True)[0]\ndata = data.loc[data[\"smokeintensity\"] &lt;= 25]  # Above 25 intensity is sparser\n\ndenominator = conditional_densities(data, use_confounders=True)\nnumerator = conditional_densities(data, use_confounders=False)\ngeneralized_ipw = numerator / denominator\n\nmsm = sm.formula.wls(  # Using GEE instead will lead to better (more conservative) uncertainty intervals\n    \"wt82_71 ~ 1 + smkintensity82_71 + I(smkintensity82_71**2)\",\n    data=data,\n    weights=generalized_ipw,\n).fit()\n\ndosage = list(range(26))\ndosage = pd.DataFrame(\n    data={\"smkintensity82_71\": dosage, \"I(smkintensity82_71**2)\": dosage},\n    index=dosage,\n)\nresponse = msm.predict(dosage)\nax = response.plot(\n    kind=\"line\",\n    xlabel=\"Increase in cigarettes per day\",\n    ylabel=\"Change in weight after 10 years [kg]\",\n    title=\"Smoking more cigarettes led to smaller weight increase\"\n)\n\n# Example adjusted from Hernan and Robins' What If book\n\n\n\nThe dose response curve resulting from the snipped code above.\n\n\n\n\nExtensions\nThe above describes on simple flavor of estimation. It could, however, be extended in multiple parts. Below are a few such extensions. Feel free to skip if you had enough.\n\nStabilized weights\nIn IPW for binary treatment, we commonly calculate the weights as 1 over the probabilities. This results in pseudo population twice the size of our sample — since the weighting result in each treatment group being the size of our original sample.\nStabilized weights are a version in which the numerator is not 1, but the treatment prevalence (average of binary treatment). This shrinks the weights so that the overall pseudo-population size is the size of the original sample, not twice the size.\nThis stabilization is also applicable to the continuous treatment setting. Instead of setting the numerator to be 1, we can take the numerator to be the density of treatment values under the average treatment value (or, more generally, the prediction of an intercept-only model. Under this formulation we can also stabilize on effect modifiers, but this is for a different post). The code above shows a stabilized version that is more recommended.\n\n\nReplacing weighted regression with a clever covariate\nIn the second step, when modeling the outcomes based on the treatments, we incorporated the generalized propensity scores as weights in a weighted regression. This is usually referred to as marginal structural models as described by Robins, Hernan, and Brumback. However, similar to the different flavors of TMLE, we can also incorporate the generalized propensity scores as an additional covariate in the second-step outcome regression, rather than as weights. This, in fact, what Hirano and Imbens suggested.\nIn this version, we add the densities (not their inverse) as an additional feature. However, since it is another continuous measure, prone to misspecification, we will add it flexibly. Usually by also adding a squared term and an interaction with the treatment variable (or a spline).\n\n\n\nOutcome model with the generalized IP-weights as a predictor. As proposed by Hirano and Imbens.\n\n\nOne small but important detail to note is that during prediction, when we set the treatment value of interest for all individuals, we will now first need to calculate the density for that specific value and then insert these densities as the predictors to the outcome model we apply.\n\n\nHeteroskedastic density and other distributions\nIn item (2) of step 1, we estimated the density with a fixed variance for all individuals. This assumption, called homoskedasticity, is reasonable (and can be empirically tested by examining the residuals) but can be relaxed. Similar to how the mean of the density function was conditioned on covariates (i.e., a prediction), the variance can also be a function that changes with covariates. Or in other ways like density-weighted regression.\nAdditionally, we could parameterize the density function using other distributions, like t-distribution, truncated normal, etc. Alternatively, it can further be de-parametrized by using kernel density estimation, but there ain’t no such thing as free lunch — and this will require much denser data for a reliable estimation.\n\n\n\nPositivity assumptions under continuous treatment\nSo far, we have discussed how to obtain statistical associations between treatment and outcome. However, to convert them into causal claims, we will need to make additional assumptions. These assumptions are necessary no matter how sophisticated the statistical estimation is. It is up to us to apply additional logic on top of it to justify these associational differences are indeed causal effects.\n\nCausation = Association + Logic\n\nRecall that we have three main assumptions: consistency, exchangeability, and positivity. Consistency is an assumption on the treatment mechanism and is therefore the same as in the categorical treatment setting. Exchangeability assumes there are no unmeasured confounding, and that each potential outcome is independent of the observed treatment assignment (no bias) given the covariates — this is also the same. Positivity is the assumption requiring some adjustments.\nRecall that in the categorical case, positivity assumes each unit has some chance (positive probability) to be assigned to every treatment. This means the treatment groups share a common support, and their characteristics overlap. It is formally defined as \\(\\Pr[A=a|X]&gt;0\\) for all treatment values a across the entire space covariate space X.\nBut in the continuous case, we need to replace the probability with density. However, the rest remains the same. We will require \\(f(A=a|X)&gt;0\\) for all treatment values a across the entire covariate space X. Namely, we need positive density for all available combination of treatment and covariate levels. Luckily, this can be empirically tested (like regular positivity) by examining density of different treatment values, especially those we are most interested in, under the density model we obtained (the one whose means are our regression predictions).\n\n\n\n\n\n\nPlotting the histogram of conditional densities of the smoking example to assess overlap. Here testing for two treatment values: 0 = no change in smoking intensity, and 1 = an increase of a cigarette a day. This is the equivalent of plotting the probability to treat in the binary treatment case to assess overlap between groups.\n\n\n\n\n\n\n\nOr more broadly - A ridge plot examining the conditional density distribution of all possible dosage increases. We see the further the dosages are from each other - the smaller the overlap. Therefore we might trust the effect of local changes (say, increase of 5 vs. no change) than global changes (say, increase of 25 vs. no change)."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#limitations",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#limitations",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Limitations",
    "text": "Limitations\nIncreasing the number of treatment values does not come without a cost. There are several limitations we should be aware of when modeling continuous treatment.\nFirst, theoretical assumptions are harder to conforms to. It is harder to achieve exchangeability since we now have more treatment values for which we want to achieve unconfoundedness. It is also harder to achieve positivity, for both the same reason and the fact that conditional density might be sparser due to its continuous nature.\nSecond, continuous variables are harder to model. They are more prone to misspecification. We might partially solve it by using flexible estimators (like additive trees) or flexible data transformation (like splines in GAMs), but it can come at a cost — requiring more data or introducing some bias due to bias-variance trade-off. Additionally, densities are notoriously hard to estimate, and our generalized IP-weights can be sensitive to different choices of density estimators.\nThird, often times some continuous measures are actually ordinal. Treatment on the ordinal scale might be approximated as continuous, especially when the number of categories and their ranges increase. But continuous approximation of ordinal variables might also introduce some bias due to misspecification. There are generalizations of IPW to the ordinal scale, which require ordinal regression (similar to how we required linear regression, and regular IPW requires logistic regression), but these are beyond the scope of this post. Just so you're aware of that.\nLastly, to end on a brighter note, throughout this post I had in mind a case of continuous treatment and continuous outcome. However, this is also applicable to other outcomes. Namely, the second-step outcome model can correspond to arbitrary type of the outcome. Most commonly, if we have binary outcome, we can apply a logistic regression (or any other \"classifier\")."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#conclusions",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#conclusions",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post I introduced causal inference with continuous treatments. I presented their importance, described how to model them, how to adjust the required causal assumptions, and their limitations. I hope you find it useful."
  },
  {
    "objectID": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#footnotes",
    "href": "blog/2022/11/02-continuous-treatment-causal-inference/index.html#footnotes",
    "title": "Causal Inference with Continuous Treatments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n↩︎"
  },
  {
    "objectID": "blog/2023/07/15-hello_world/index.html",
    "href": "blog/2023/07/15-hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Hello world,"
  },
  {
    "objectID": "blog/2023/07/15-hello_world/index.html#introduction",
    "href": "blog/2023/07/15-hello_world/index.html#introduction",
    "title": "Hello World",
    "section": "",
    "text": "Hello world,"
  },
  {
    "objectID": "blog/2023/07/15-hello_world/index.html#methods",
    "href": "blog/2023/07/15-hello_world/index.html#methods",
    "title": "Hello World",
    "section": "Methods",
    "text": "Methods\nWaving hands sideways."
  },
  {
    "objectID": "blog/2023/07/15-hello_world/index.html#results",
    "href": "blog/2023/07/15-hello_world/index.html#results",
    "title": "Hello World",
    "section": "Results",
    "text": "Results\nProof was hand-waved and left as an excercise for the reader."
  },
  {
    "objectID": "blog/2023/07/15-hello_world/index.html#discussion",
    "href": "blog/2023/07/15-hello_world/index.html#discussion",
    "title": "Hello World",
    "section": "Discussion",
    "text": "Discussion\nGood bye"
  }
]