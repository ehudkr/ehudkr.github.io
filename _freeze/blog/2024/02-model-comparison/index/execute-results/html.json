{
  "hash": "e3b0995db00ba2f33957c13822577fce",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Uncertainty-aware model comparison\"\nsubtitle: > \n  Comparing model performance rigorously and meaningfully using statistics.\nauthor: \"Ehud Karavani\"\ndate: 2024/08/30\ndate-modified: last-modified\n# image: images/ouilove_plot.png\nbibliography: references.bib\ncategories: \n  - machine learning\n  - deep learning\n  - statistics\n---\n\n## Motivation\nI'm under the impression that machine learning models are not benchmarked properly, making it difficult to draw meaningful conclusions.\nToo often than not, models will be evaluated using a single number, with no uncertainty around that number.\nToo often than not, the performance of different models will not be assessed directly, and again, no uncertainty around that difference in performance[^1].\n\n[^1]: and we know the standard error of the difference between means is larger than the standard error of either mean. \n\nThis creates difficulty in understanding which models are comparable and which are superior. \nIs a point difference of 0.1% between models on the leaderboard meaningful or a fluke?\nAs the monetary (and environmental) costs of training these large language models (LLMs) skyrockets, we should be able to truly tell whether size matters and are larger models really are superior to their slimmer, more economical, versions.\n\nTo solve that, we'll need to know the range of errors compatible with the model; not just the average score, but the uncertainty around it. \nUnfortunately (non-Bayesian) machine learning and deep learning models don't lend themselves to uncertainty estimation easily. \nOftentimes, estimating the variability in model performance is done using the bootstrap (sampling data with replacement and refitting)[^5] or cross validation. \nBut fitting just a single model can exhaust unreasonable resources, so fitting is hundreds (or even thousands) of times just to obtain variance estimation is out of the question. \nLuckily, there are other ways to estimate the variance of model performance using the variability within the dataset or asymptotics (that may very well kick in as the datasets can be large).\nWe'll see how to utilize them.\n\n\n[^5]: One variant I've also seen in some settings is that the sample is kept fixed, and just evaluate difference random initializations of the model.\n\n### What's in it for you\nIn this blog post I'll explore an approach to rigorously compare the performance of two (or more!) models directly.\nIt will incorporate uncertainty that will allow us to assess whether a model is truly superior or inferior (or non-inferior) to others[^2].\nIt will be a generalizable framework, allowing for multiple models, multiple metrics, multiple datasets, even at once.\nMoreover, it is totally decoupled from the actual models - you'll only need predictions and ground-truth labels. No need for refitting or retrofitting, only a forward pass.\n\n\n[^2]: These are formal well-defined statistical concepts that I'll introduce later down the post. \n\n## Setup\nTo achieve all that I will utilize Statistics; which I personally feel were long overlooked as the field of machine (and then deep) learning became more and more dominated by engineers (or engineering mindset rather than by scientists more versed in the scientific method and its philosophy).\n\nHowever, I'm a statistics reformer kind of person. \nI will not trouble you with the zoo of statistical tests - it appears they are mostly just linear models [@lindelov2019common]. \nYou know, \"regression\". \nSometimes called \"ANOVAs\", but I know this jargon might intimidate some, so let's stick with \"linear models\" for now[^3].\nThe most important point is that I will use this machinery to compare between models.\nThe benefit using this framework is that it comes with 100+ years of statistics research focusing on uncertainty estimation - that, thanks to software available today, I could just apply off-the-shelf. \n\n[^3]: Actually, we'll need \"generalized linear (mixed) models\", but I will build it slowly and gradually, you won't even notice. And we will also turn Bayesian, but it will be as easy, don't worry. \n\nTo be completely honest, I don't think this approach is novel, at least not in its basic version. \nAlthough I haven't encountered it exactly as I'm going to present it, it might be the case I just haven't landed on the right keyword search on google.\nSo, as always, some people will find this post informative and some will just scoff. \n\n\n### Prerequisites\nBefore I start, I'll cover some of the basics first, for completeness.\nIf you're comfortable with statistical inference, regression, and model evaluation, then you can skip this (or at least skim it).\n\n#### Notations\nThroughout the post I'll use $X$ (`X`) to denote a feature matrix (covariates), $y$ (`y` or `y_true`) will be the ground truth labels coupled to $X$, and $\\hat{y}$ (`y_pred`) will be the predictions from an arbitrary model $f(X)$ (a linear regression or a deep neural network) aiming to be as closest to $y$ as possible.\n\nThe approach I'll present will only need $y$ and $\\hat{y}$, making it independent of $f$ and whatever software or hardware it is attached to. \n\n\n#### Individual-level errors\nPerformance metrics are a way to quantify the performance of a model, assessing how the model errs by comparing the ground truth labels ($y$) to the predictions ($\\hat{y}$).\nMost commonly known will be the mean squared error (MSE) if $y$ is continuous or log-loss (aka cross entropy) if $y$ is categorical. \nIf you are, like me, used to Scikit-learn's metrics API `metric(y_true, y-pred)` returning a single number - the average over the dataset, you might have forgotten that these errors can be calculated on an individual level. For instance $l_i = (y_i - \\hat{y}_i)^2$ is the individual level squared error (which is later averaged to obtain the _mean_ squared error).\n\nNote that not all evaluation metrics allow individual-level errors.\nFor instance, the ROC AUC (and many confusion matrix-based metrics) will often require aggregating over the sample, and individual-level errors might not be well-defined[^7].\n\n[^7]: There might be workarounds, but let's ignore those for the sake of this post, and focus on easily-accessible individual-level errors.\n\n#### Regression models are fancy average machines\nLinear models (those $y_i = \\alpha + \\beta X_i$, for example) end up estimating $E[Y|X]$. \nNamely, the expected value of $Y$ given some level of $X$. \nBasically, the average value of $Y$ in some setting.\nAnd if there is no $X$ (there is only an intercept, so $y_i = \\alpha$), then that's just the average overall, look:\n\n::: {#69d1107d .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(seed=0)\n\ny = rng.normal(loc=3, size=1000)\nols = smf.ols(\n    \"y ~ 1\", \n    data=pd.DataFrame({\"y\": y})\n).fit()\nprint(f\"Taking the mean of `y`: {y.mean():.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params[0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTaking the mean of `y`: 2.9520\nThe coefficient of the regression intercept: 2.9520\n```\n:::\n:::\n\n\nThe nice thing about the regression framework is that we also get inferential results (standard errors, confidence intervals, p-values...) for free, off-the-shelf.\nIn the frequentist (read, \"standard\") regression setting, this is done using analytical asymptotic properties (no resampling and refitting) \n\n::: {#8acd3d6f .cell execution_count=2}\n``` {.python .cell-code}\nprint(ols.summary(slim=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nNo. Observations:                1000   F-statistic:                       nan\nCovariance Type:            nonrobust   Prob (F-statistic):                nan\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9520      0.031     95.523      0.000       2.891       3.013\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n#### Averaging individual-level errors with regression\nTying the two subsections above together, given a ground truth vector $y$ and predictions $\\hat{y}$, we can calculate the mean squared error as an average or with a regression.\n\n::: {#8cc16cec .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n\ny = rng.normal(loc=3, size=1000)\ny_pred = y + rng.normal(loc=0, scale=0.5, size=1000)\n\nindividual_errors = (y - y_pred)**2\nols = smf.ols(\n    \"li ~ 1\", \n    data=pd.DataFrame({\"li\": individual_errors})\n).fit()\nprint(f\"Taking the mean of the squared errors with sklearn: {mean_squared_error(y, y_pred):.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params[0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTaking the mean of the squared errors with sklearn: 0.2415\nThe coefficient of the regression intercept: 0.2415\n```\n:::\n:::\n\n\n#### Generalization and statistical inference of model performance\nModel performance is a characteristic of the model. \nEvaluating it on a given dataset is not the core interest of the researcher.\nWe often care about generalization errors - figuring out what will be the performance when the model will be deployed publicly on unseen data. \n\nThis notion of generalization carries a very similar meaning to inferential statistics, \nwhere the average in a sample is of little importance relative to the mean of the entire underlying population from which the dataset was sampled from[^6].\nAnd since the leap from sample average to population mean comes with some uncertainty to it, we often bound these estimates with uncertainty intervals around them[^4].\n\n[^6]: In the most basic example, say we are interested in the average height of men in a certain country. We sample men from that country and measure their height, but the average height of the sample is of little interest. What's interesting is what that average tells about the average height of the people in that country, or, in reverse - what can we infer about the population from the sample.\n\n[^4]: In this setting, I allude to confidence intervals, of course. Strictly speaking, they are not really uncertainty intervals (more like [compatibility intervals](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01105-9)), but because I will also introduce Bayesian credible intervals later, I don't want to be sidetracked by the lingo, so I call them all uncertainty intervals.\n\nLike inferential statistics, in generalization we would like to infer what will the average error rate will be on unknown data out there in the world, using the sample of data we do have right now. \nWe don't  really care about the error in any given dataset, even if it is a test set that was not used for training, it is still just a sample from the possible population of data available in the world.\nThe leap from the average error rate in the dataset to the errors out there comes with some uncertainty to it, and we'd like to know the likely range of that average error rate in the world.\n\n## Uncertainty-aware model comparison via regression\nAs alluded to above, we'll use individual-level errors in a regression framework in order to infer the uncertainty around performance measures.\nKnowing whether all errors tightly clustered around zero or if their variance high will be useful information when we will take uncertainty-aware differences of performance between models.\n\n### Settings up data\nTo demonstrate the process, we will need data and we will need models.\nI'll use toy data of a regression, rather than classification, task (i.e., a continuous target) for simplicity as squared errors were already introduced, but this can work with any individual-level metric.\nI will split it train and test, so the comparison will be performed on unseen-data. \nAs for models, I'll compare a linear (regression) with a nonlinear (boosting trees) one, just so we can see the difference in performance.\n\n::: {#da444827 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = make_friedman1(n_samples=400, noise=1, random_state=rng.integers(100))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=rng.integers(100))\nlr = LinearRegression().fit(X_train, y_train)\nrf = GradientBoostingRegressor().fit(X_train, y_train)\n```\n:::\n\n\nNow let's calculate individual-level squared errors:\n\n::: {#0d00d161 .cell execution_count=5}\n``` {.python .cell-code}\nlr_res = (y_test - lr.predict(X_test))**2\nrf_res = (y_test - rf.predict(X_test))**2\n```\n:::\n\n\n### Comparing models\nTo compare the difference in performance between the models, I'll model it as a regression task setting the individual-level errors as the target and a dummy coding of the models as the feature (together with an intercept). \nThis merely requires concatenating the errors and indicating which model corresponds to them.\n\n::: {#d8b94528 .cell execution_count=6}\n``` {.python .cell-code}\nsqe = pd.concat({\"lr\": pd.Series(lr_res), \"rf\": pd.Series(rf_res)}, names=[\"model\", \"id\"]).rename(\"li\")\nsqe\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nmodel  id \nlr     0       1.672041\n       1      28.901371\n       2       7.072445\n       3       0.212794\n       4      23.561583\n                ...    \nrf     195     4.584243\n       196    12.403116\n       197    14.442194\n       198     0.002028\n       199     0.175701\nName: li, Length: 400, dtype: float64\n```\n:::\n:::\n\n\nAnd putting that data into the regression[^8]:\n\n::: {#6cc7f3f9 .cell execution_count=7}\n``` {.python .cell-code}\nsqe = sqe.reset_index()\nols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit()\nprint(ols.summary(slim=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     li   R-squared:                       0.031\nModel:                            OLS   Adj. R-squared:                  0.029\nNo. Observations:                 400   F-statistic:                     12.74\nCovariance Type:            nonrobust   Prob (F-statistic):           0.000402\n==================================================================================\n                     coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          7.4025      0.688     10.755      0.000       6.049       8.756\nC(model)[T.rf]    -3.4741      0.973     -3.569      0.000      -5.388      -1.561\n==================================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n[^8]: We use the factor transformation `C(model)` to convert the `model` column, which contains strings, into 0-1 coding suitable for regression\n\nWe can now follow a standard interpretation of a linear model.\nThe `C(model)[T.rf]` variable (i.e., setting `rf` to be encoded as 1 while `lr` was encoded as 0) quantifies the average difference in squared errors (variance) between the random forest and the linear regression models (-3.47).\nThis allows us to directly assess the difference in model performance, but since we get asymptotic inference too, we can further place uncertainty around that difference - enabling us to tell whether it is statistically significant or not (p-value 0.0004), or at least what is a likely range for that error to be in the \"population\" of data from which the dataset came from (95% confidence intervals (-5.4, -1.6)).\n\nThis direct comparison is the proper way to assess difference in performance.\nWhen deep learning model do get confidence intervals around them, they are often around the performance metrics itself. \nBut then, translating that to confidence intervals around the difference between two models (and their own confidence intervals) is not necessarily straightforward, and can we need to be extra cautious not to misinterpret the significance [@gelman2006difference].\n\nThis was relatively easy. \n\n## Going further\n\n### goind baysian\nregular anova often requires the df for a null hypothesis , but go figure what's the degrees of  freedom for ChatGPT. \nbayesian estimation \n\n### non-inferiority\n\n### extensions\nregression framework is easily extendable. \nextensions: why regression? \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}