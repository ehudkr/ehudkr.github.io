{
  "hash": "06d4410563e56fdea38c93e543780b64",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Uncertainty-aware model comparison\"\nsubtitle: > \n  Comparing model performance rigorously and meaningfully using statistics.\nauthor: \"Ehud Karavani\"\ndate: 2024/08/30\ndate-modified: last-modified\n# image: images/ouilove_plot.png\nbibliography: references.bib\ncategories: \n  - machine learning\n  - deep learning\n  - statistics\n---\n\n## Motivation\nI'm under the impression that machine learning models are not benchmarked properly, making it difficult to draw meaningful conclusions.\nToo often than not, models will be evaluated using a single number, with no uncertainty around that number.\nToo often than not, the performance of different models will not be assessed directly, and again, no uncertainty around that difference in performance[^1].\n\n[^1]: and we know the standard error of the difference between means is larger than the standard error of either mean. \n\nThis creates difficulty in understanding which models are comparable and which are superior. \nIs a point difference of 0.1% between models on the leaderboard meaningful or a fluke?\nAs the monetary (and environmental) costs of training these large language models (LLMs) skyrockets, we should be able to truly tell whether size matters and are larger models really are superior to their slimmer, more economical, versions.\n\nTo solve that, we'll need to know the range of errors compatible with the model; not just the average score, but the uncertainty around it. \nUnfortunately (non-Bayesian) machine learning and deep learning models don't lend themselves to uncertainty estimation easily. \nOftentimes, estimating the variability in model performance is done using the bootstrap (sampling data with replacement and refitting)[^5] or cross validation. \nBut fitting just a single model can exhaust unreasonable resources, so fitting is hundreds (or even thousands) of times just to obtain variance estimation is out of the question. \nLuckily, there are other ways to estimate the variance of model performance using the variability within the dataset or asymptotics (that may very well kick in as the datasets can be large).\nWe'll see how to utilize them.\n\n\n[^5]: One variant I've also seen in some settings is that the sample is kept fixed, and just evaluate difference random initializations of the model.\n\n### What's in it for you\nIn this blog post I'll explore an approach to rigorously compare the performance of two (or more!) models directly.\nIt will incorporate uncertainty that will allow us to assess whether a model is truly superior or inferior (or non-inferior) to others[^2].\nIt will be a generalizable framework, allowing for multiple models, multiple metrics, multiple datasets, even at once.\nMoreover, it is totally decoupled from the actual models - you'll only need predictions and ground-truth labels. No need for refitting or retrofitting, only a forward pass.\n\n\n[^2]: These are formal well-defined statistical concepts that I'll introduce later down the post. \n\n## Setup\nTo achieve all that I will utilize Statistics; which I personally feel were long overlooked as the field of machine (and then deep) learning became more and more dominated by engineers (or engineering mindset rather than by scientists more versed in the scientific method and its philosophy).\n\nHowever, I'm a statistics reformer kind of person. \nI will not trouble you with the zoo of statistical tests - it appears they are mostly just linear models [@lindelov2019common]. \nYou know, \"regression\". \nSometimes called \"ANOVAs\", but I know this jargon might intimidate some, so let's stick with \"linear models\" for now[^3].\nThe most important point is that I will use this machinery to compare between models.\nThe benefit using this framework is that it comes with 100+ years of statistics research focusing on uncertainty estimation - that, thanks to software available today, I could just apply off-the-shelf. \n\n[^3]: Actually, we'll need \"generalized linear (mixed) models\", but I will build it slowly and gradually, you won't even notice. And we will also turn Bayesian, but it will be as easy, don't worry. \n\nTo be completely honest, I don't think this approach is novel, at least not in its basic version. \nAlthough I haven't encountered it exactly as I'm going to present it, it might be the case I just haven't landed on the right keyword search on google.\nSo, as always, some people will find this post informative and some will just scoff. \n\n\n## warm up\nTODO: put in setup?\ndenote X, y, yhat. f(x)=y_hat. \nas we said our framework will need only yhat. \n\n### individual-level errors\nPerformance metrics are a way to quantify the performance of a model, assessing how the model errs by comparing the ground truth labels ($y$) to the predictions ($\\hat{y}$).\nMost commonly known will be the mean squared error (MSE) if $y$ is continuous or log-loss (aka cross entropy) if $y$ is categorical. \nIf you are, like me, used to Scikit-learn's metrics API `metric(y_true, y-pred)` returning a single number - the average over the dataset, you might have forgotten that these errors can be calculated on an individual level. For instance $l_i = (y_i - \\hat{y}_i)^2$ is the individual level squared error (which is later averaged to obtain the _mean_ squared error).\n\n### Regression models are fancy average machines\nLinear models (those $y_i = \\alpha + \\beta X_i$, for example) end up estimating $E[Y|X]$. \nNamely, the expected value of $Y$ given some level of $X$. \nBasically, the average value of $Y$ in some setting.\nAnd if there is no $X$ (there is only an intercept, so $y_i = \\alpha$), then that's just the average overall, look:\n\n::: {#78ac7e15 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(seed=0)\n\ny = rng.normal(loc=3, size=1000)\nols = smf.ols(\n    \"y ~ 1\", \n    data=pd.DataFrame({\"y\": y})\n).fit()\nprint(f\"Taking the mean of `y`: {y.mean():.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params[0]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTaking the mean of `y`: 2.9520\nThe coefficient of the regression intercept: 2.9520\n```\n:::\n:::\n\n\nThe nice thing about the regression framework is that we also get inferential results (standard errors, confidence intervals, p-values...) for free, off-the-shelf.\nIn the frequentist (read, \"standard\") regression setting, this is done using analytical asymptotic properties (no resampling and refitting) \n\n::: {#7127a6db .cell execution_count=2}\n``` {.python .cell-code}\nprint(ols.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                       nan\nDate:                Sat, 23 Nov 2024   Prob (F-statistic):                nan\nTime:                        21:40:05   Log-Likelihood:                -1395.4\nNo. Observations:                1000   AIC:                             2793.\nDf Residuals:                     999   BIC:                             2798.\nDf Model:                           0                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9520      0.031     95.523      0.000       2.891       3.013\n==============================================================================\nOmnibus:                        4.056   Durbin-Watson:                   1.988\nProb(Omnibus):                  0.132   Jarque-Bera (JB):                4.399\nSkew:                          -0.073   Prob(JB):                        0.111\nKurtosis:                       3.290   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n### inference performance\nthe performance is a charachetristics of the model. it is less relevant what it will be on a given dataset, as much as what it will be when deployed publically. \nThis is the same notion as inferential statistics, where the average in sample is of little importance relative to the mean of the entire underlying population from which the dataset was sampled from.\nAnd since the leap from sample to population comes with some uncertainty to it, we often bound these estimates with uncertainty intervals around them[^4].\nSimilarly, we don't care about the error from a given dataset, even though it is a test set that was not used for training, it is still just a sample from the possible population of data available in the world.\n\n[^4]: In this setting, I allude to confidence intervals, of course. Strictly speaking, they are not really uncertainty intervals (more like [compatibility intervals](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01105-9)), but because I will also introduce Bayesian credible intervals later, I don't want to be sidetracked by the lingo, so I call them all uncertainty intervals.\n\n\nextensions: why regression? \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}