{
  "hash": "bfb662e4c7508a5ecbe4110e9f900eeb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Uncertainty-aware model comparison\"\nsubtitle: > \n  Comparing model performance rigorously and meaningfully using statistics.\nauthor: \"Ehud Karavani\"\ndate: 2024/08/30\ndate-modified: last-modified\nimage: ./noninferiority_nejm.jpg\nbibliography: references.bib\ncategories: \n  - machine learning\n  - deep learning\n  - statistics\n  - meta-science\nexecute: \n  freeze: true\n---\n\n\n\n::: {.callout-note}\nThis post is still a work in process.\n:::\n\n\n## Motivation\nI'm under the impression that deep learning models are not benchmarked properly, making it difficult to draw meaningful conclusions.\nToo often than not, models will be evaluated using a single number, with no uncertainty around that number.\nToo often than not, the difference in performance of different models will not be assessed directly, and again, no uncertainty around that difference either[^1].\n\n[^1]: and we know the standard error of the difference between means is larger than the standard error of either mean. \n\nThis creates difficulty in understanding which models are comparable and which are superior. \nIs a point difference of 0.1% between models on the leaderboard meaningful or a fluke?\nAs the monetary (and environmental) costs of training these large language models (LLMs) skyrockets, we should be able to truly tell whether size matters and are larger models really are superior to their slimmer, more economical versions.\n\nTo solve that, we'll need to know the range of errors compatible with the model; not just the average score - but the uncertainty around it. \nUnfortunately (non-Bayesian) machine learning and deep learning models don't lend themselves to uncertainty estimation easily. \nOftentimes, estimating the variability in model performance requires the bootstrap (sampling data with replacement and refitting)[^5] or cross validation. \nBut fitting just a single model can exhaust unreasonable resources, so fitting it hundreds (or even thousands) of times just to estimate its variance is out of the question. \nLuckily, there are other ways to estimate the variance of model performance using the variability within the dataset or asymptotics (that may very well kick in as datasets can be large).\nIn this post I'll show how to utilize them.\n\n\n[^5]: One variant I've also seen in some settings is that the sample is kept fixed, and just iterates over different random initializations of the model.\n\n### What's in it for you\nIn this blog post I'll explore an approach to rigorously compare the performance of two (or more!) models directly.\nIt will incorporate uncertainty that will allow us to assess whether a model is truly superior or inferior (or non-inferior) to others[^2].\nIt will be a generalizable framework, allowing for multiple models, multiple metrics, multiple datasets, even at once.\nMoreover, it is totally decoupled from the actual models - you'll only need predictions and ground-truth labels. \nNo need for refitting or retrofitting, only a forward pass.\n\n\n[^2]: These are formal, well-defined statistical concepts that I'll introduce later down the post. \n\n## Setup\nTo achieve all that I will utilize Statistics; which I personally feel were long overlooked as the field of machine (and then deep) learning became more and more dominated by engineers (or engineering mindset rather than by scientists more versed in the scientific method and its philosophy).\nIt's not necessarily a bad thing! Engineers get things done, but the skill sets (or the mindset) may de-prioritize Research Methods. \n\nHowever, I'm a statistics reformer kind of person. \nSo I will not trouble you with the zoo of statistical tests - it appears they are mostly just linear models [@lindelov2019common]. \nYou know, \"regression\". \nSometimes called \"ANOVAs\", but I know this jargon might intimidate some, so let's stick with \"linear models\" for now[^3].\nThe most important point is that I will use this machinery to compare between models.\nThe benefit using this framework is that it comes with 100+ years of statistics research focusing on uncertainty estimation, that -- thanks to software available today -- we can just apply off-the-shelf. \n\n<!-- [^3]: Actually, we'll need \"generalized linear (mixed) models\", but I will build it slowly and gradually, you won't even notice. And we will also turn Bayesian, but it will be as easy, don't worry.  -->\n[^3]: Actually, we'll need (Bayesian) \"generalized linear (mixed) models\" to enjoy the full capabilities of the framework, but modern software makes it so easy you don't have to bother much about it. \n\nTo be completely honest, I don't think this approach is novel, at least not in its basic version. \nAlthough I haven't encountered it exactly as I'm going to present it, it might be the case I just haven't landed on the right keyword search on google.\nAnd I'll use Python, which I hope is more convenient to those who are versed in the field of deep learning.\nSo, as always, some people will find this post informative and some will just scoff. \n\n\n### Prerequisites\nBefore I start, I'll cover some of the basics first, for completeness.\nIf you're comfortable with statistical inference, regression, and model evaluation, then you can skip it (or at least skim it).\n\n#### Notations\nThroughout the post I'll use $X$ (`X`) to denote a feature matrix (covariates), $y$ (`y` or `y_true`) will be the ground truth labels coupled to $X$, and $\\hat{y}$ (`y_pred`) will be the predictions from an arbitrary model $f(X)$ (a linear regression or a deep neural network) aiming to be as closest to $y$ as possible.\n\nThe approach I'll present will only need $y$ and $\\hat{y}$, making it independent of $f$ and whatever software or hardware it is attached to. \n\n\n#### Individual-level errors\nPerformance metrics are a way to quantify the performance of a model, assessing how the model errs by comparing the ground truth labels ($y$) to the predictions ($\\hat{y}$).\nMost commonly known will be the mean squared error (MSE) if $y$ is continuous or log-loss (aka cross entropy) if $y$ is categorical. \nIf you are, like me, used to Scikit-learn's metrics API  with `metric(y_true, y_pred)` returning a single number - the average over the dataset, you might have forgotten that these errors can be calculated on an individual level. For instance, $l_i = (y_i - \\hat{y}_i)^2$ is the individual-level squared error of observation $i$ (which is later averaged to obtain the _mean_ squared error).\n\nNote that not all evaluation metrics allow individual-level errors.\nFor instance, the ROC AUC (and many confusion matrix-based metrics) will often require aggregating over the sample, and individual-level errors might not be well-defined[^7].\n\n[^7]: There might be workarounds, but let's ignore those for the sake of this post, and focus on easily-accessible individual-level errors.\n\n#### Regression models are fancy average machines\nLinear models (those $y_i = \\alpha + \\beta X_i$, for example) end up estimating $E[Y|X]$. \nNamely, the expected value of $Y$ given some level of $X$. \nBasically, the average value of $Y$ in some setting.\nAnd if there is no $X$ (i.e., there is only an intercept, so $y_i = \\alpha$), then that's just the average overall, look:\n\n::: {#c017d878 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\nrng = np.random.default_rng(seed=0)\n\ny = rng.normal(loc=3, size=1000)\nols = smf.ols(\n    \"y ~ 1\", \n    data=pd.DataFrame({\"y\": y})\n).fit()\nprint(f\"Taking the mean of `y`: {y.mean():.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params['Intercept']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTaking the mean of `y`: 2.9520\nThe coefficient of the regression intercept: 2.9520\n```\n:::\n:::\n\n\nThe nice thing about the regression framework is that we also get inferential results (standard errors, confidence intervals, p-values...) for free, off-the-shelf.\nIn the frequentist (read, \"standard\") regression setting, this is done using analytical asymptotic properties (no resampling and refitting) \n\n::: {#85bd0c8b .cell execution_count=2}\n``` {.python .cell-code}\nprint(ols.summary(slim=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nNo. Observations:                1000   F-statistic:                       nan\nCovariance Type:            nonrobust   Prob (F-statistic):                nan\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9520      0.031     95.523      0.000       2.891       3.013\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n#### Averaging individual-level errors with regression\nTying the two subsections above together, given a ground truth vector $y$ and predictions $\\hat{y}$, we can calculate the mean squared error as an average or with a regression.\n\n::: {#451a598e .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n\ny = rng.normal(loc=3, size=1000)\ny_pred = y + rng.normal(loc=0, scale=0.5, size=1000)\n\nindividual_errors = (y - y_pred)**2\nols = smf.ols(\n    \"li ~ 1\", \n    data=pd.DataFrame({\"li\": individual_errors})\n).fit()\nprint(f\"Taking the mean of the squared errors with sklearn: {mean_squared_error(y, y_pred):.4f}\")\nprint(f\"The coefficient of the regression intercept: {ols.params['Intercept']:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTaking the mean of the squared errors with sklearn: 0.2415\nThe coefficient of the regression intercept: 0.2415\n```\n:::\n:::\n\n\n#### Generalization and statistical inference of model performance\nModel performance is a characteristic of the model. \nEvaluating it on a given dataset is not the core interest of the researcher.\nWe often care about generalization errors - figuring out what will be the performance when the model will be deployed publicly on unseen data. \n\nThis notion of generalization carries a very similar meaning to inferential statistics, \nwhere the average in a sample is of little importance relative to the mean of the entire underlying population from which the dataset was sampled from[^6].\nAnd since the leap from sample average to population mean comes with some uncertainty to it, we often bound these estimates with uncertainty intervals around them[^4].\n\n[^6]: In the most basic example, say we are interested in the average height of men in a certain country. We sample men from that country and measure their height, but the average height of the sample is of little interest. What's interesting is what that average tells about the average height of the people in that country, or, in reverse - what can we infer about the population from the sample.\n\n[^4]: In this setting, I allude to confidence intervals, of course. Strictly speaking, they are not really uncertainty intervals (more like [compatibility intervals](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01105-9)), but because I will also introduce Bayesian credible intervals later, I don't want to be sidetracked by the lingo, so I call them all uncertainty intervals.\n\nLike inferential statistics, in generalization we would like to infer what will the average error rate will be on unknown data out there in the world, using the sample of data we do have right now. \nWe don't really care about the error in any given dataset, even if it is a test set that was not used for training, it is still just a sample from the possible population of data available in the world.\nThe leap from the average error rate in the dataset to the errors out there comes with some uncertainty to it, and we'd like to know the likely range of that average error rate in the actual world.\n\n## Uncertainty-aware model comparison via regression\nAs alluded to above, we'll use individual-level errors in a regression framework in order to infer the uncertainty around performance measures.\nKnowing whether all errors tightly clustered around zero or if their variance is high will be useful information when we will take uncertainty-aware differences of performance between models.\n\n### Settings up data\nTo demonstrate the process, we will need labeled data and we will need prediction models.\nI'll use toy data of a regression, rather than classification, task (i.e., a continuous target) for simplicity as squared errors were already introduced, but this can work with any individual-level metric.\nI will split it train and test, so the comparison will be performed on unseen-data. \nAs for models, I'll compare a linear (regression) with a nonlinear (boosting trees) one, just so we can see the difference in performance.\n\n::: {#deed2171 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = make_friedman1(n_samples=400, noise=1, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\nlr = LinearRegression().fit(X_train, y_train)\ngbt = GradientBoostingRegressor().fit(X_train, y_train)\n```\n:::\n\n\nNow let's calculate individual-level squared errors:\n\n::: {#c0d9bb37 .cell execution_count=5}\n``` {.python .cell-code}\nlr_res = (y_test - lr.predict(X_test))**2\ngbt_res = (y_test - gbt.predict(X_test))**2\n```\n:::\n\n\n### Comparing models\nTo compare the difference in performance between the models, I'll model it as a regression task setting the individual-level errors as the target and a dummy coding of the models as the feature (together with an intercept). \nThis merely requires concatenating the errors and indicating which model corresponds to them.\n\n::: {#86ec9660 .cell execution_count=6}\n``` {.python .cell-code}\nsqe = pd.concat({\"lr\": pd.Series(lr_res), \"gbt\": pd.Series(gbt_res)}, names=[\"model\", \"id\"]).rename(\"li\")\nsqe\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nmodel  id \nlr     0       0.089986\n       1       1.319738\n       2       6.044722\n       3       6.061478\n       4      13.876234\n                ...    \ngbt    195     0.518832\n       196     1.191123\n       197     0.002167\n       198     2.259212\n       199     3.439729\nName: li, Length: 400, dtype: float64\n```\n:::\n:::\n\n\nAnd putting that data into the regression[^8]:\n\n::: {#abd392de .cell execution_count=7}\n``` {.python .cell-code}\nsqe = sqe.reset_index()\nols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit()\nprint(ols.summary(slim=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     li   R-squared:                       0.019\nModel:                            OLS   Adj. R-squared:                  0.017\nNo. Observations:                 400   F-statistic:                     7.810\nCovariance Type:            nonrobust   Prob (F-statistic):            0.00545\n==================================================================================\n                     coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          4.2308      0.607      6.967      0.000       3.037       5.425\nC(model)[T.lr]     2.4000      0.859      2.795      0.005       0.712       4.088\n==================================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n[^8]: We use the factor transformation `C(model)` to convert the `model` column, which contains strings, into 0-1 coding suitable for regression\n\n#### Accounting for repeated measures\nOne important detail to notice is that under this \"long\" data format, we actually have repeated measure.\nThe same original datapoint appears twice - once as the residual from the linear regression model and once from the gradient boosting trees. \nWhen we stack those residuals, the rows in the dataset are no longer independent (their errors are correlated as the data point could've been easy/hard to predict in the first place) and ignoring this fact can lead us to incorrect inferences.[^14]\nLuckily, we can easily account for this structure by using clustered standard errors accounting for sample `id`s:[^15]\n\n::: {#f92ed38c .cell execution_count=8}\n``` {.python .cell-code}\n# # If you were to refit the model:\nrols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit(\n    cov_type=\"cluster\", cov_kwds={\"groups\":sqe[\"id\"]},\n)\n# # Or robustify the results of an already fitted model:\n# rols = ols.get_robustcov_results(\n#     cov_type=\"cluster\", groups=sqe[\"id\"],\n# )\nprint(rols.summary(slim=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     li   R-squared:                       0.019\nModel:                            OLS   Adj. R-squared:                  0.017\nNo. Observations:                 400   F-statistic:                     15.12\nCovariance Type:              cluster   Prob (F-statistic):           0.000138\n==================================================================================\n                     coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          4.2308      0.426      9.939      0.000       3.397       5.065\nC(model)[T.lr]     2.4000      0.617      3.888      0.000       1.190       3.610\n==================================================================================\n\nNotes:\n[1] Standard Errors are robust to cluster correlation (cluster)\n```\n:::\n:::\n\n\nYou can see the point estimate for `model` is the same as before, but the corresponding standard error has changed.\n\n[^14]: Oftentimes ignoring repeated measure can lead to overly optimistic inferences (unjustifiably narrow confidence intervals), but in our case we'll see that because our repeated measures are of the same data point, the error will shrink. Intuitively, because every observation is its own control, the baseline characteristics are the same and therefore an estimation is less noisy.  \nAnother point, is that Note that we could have formulate this task as a wide-data analysis, using one residuals to predict the other residuals and testing whether the slope goes downward or upward. \nHowever, one, I'm not sure how to interpret the resulting coefficient other than its direction, and second, stacking will allow to generalize more easily when we have more than two models. \n\n[^15]: This is equivalent of using a paired t-test, instead of a regular two-sample t-test.  \nIt seems code cells cannot be placed in footnotes, but if you were to import `from scipy.stats import ttest_rel` and then run `ttest_rel(lr_res, gbt_res)`, you would get an equivalent result. \n\nWe can now follow a standard interpretation of a linear model.\nThe `C(model)[T.lr]` variable (i.e., setting `lr` to be encoded as 1 while `gbt` was encoded as 0) quantifies the average difference in squared errors (variance) between the linear regression and the boosting trees models (2.4).\nThis allows us to directly assess the difference in model performance, but since we get asymptotic inference too, we can further place uncertainty around that difference - enabling us to tell whether it is statistically significant or not (p-value 0.0001), or at least what is a likely range for that error to be in the \"population\" of data from which the dataset came from (95% confidence intervals [(1.2, 3.6)]).\n\nThis direct comparison is the proper way to assess difference in performance.\nWhen deep learning model do get confidence intervals around them, they are often around the performance metrics itself. \nBut then, translating that to confidence intervals around the difference between two models (and their own confidence intervals) is not necessarily straightforward, \nand requires extra caution not to misinterpret the significance [@gelman2006difference].\n\nThis was relatively cool. \nIf you stop reading here it's ok.\nYou are now able to compare models relatively simply and with some notion of uncertainty. \n\n\n## Going further\n\n### Non-inferiority testing\nNoninferiority is an interesting, well-established statistical concept, that I believe might benefit the deep-learning evaluation literature.\nImagine a clinical setting where you want to test a new therapeutic treatment. \nThis treatment may not reduce mortality more than the standard of care, but it doesn't mean it's useless.\nMaybe it can be taken at home once a month, instead of 3 times a week at the clinic or have much, much fewer side effects? \nWouldn't that improve patients' life?\nWell, it will improve their life assuming the mortality rate of the new treatment is not that worse than the current standard of care.\nIf both treatment have _similar_ or _comparable_ mortality rate, but the newer one is cheaper/easier to administer, etc. wouldn't we prefer it over the existing treatment?\n\nOur statistical testing perspective has now shifted - we no longer care if the new treatment is _superior_ to the current treatment,\nonly that it is _noninferior_. \nIn practical terms, it means we no longer compare our interval against 0 (for difference or 1 for ratio),\nbut against some other margin which we're willing to accept as _similar_ for all practical considerations. \nFigure @fig-noninferiority_nejm summarizes nicely what we'd like to compare against in a noninferiority setup.\n\n![Noninferiority hypothesis testing taken from @mauri2017challenges](noninferiority_nejm.jpg){#fig-noninferiority_nejm}\n\nWe can apply the same logic translating \"treatments\" to \"models\".\nWouldn't we want to assess whether models have comparable performance and then select the model that is simpler/cheaper to train or has a smaller carbon footprint?\nIs it really worth spending 100 times more FLOPs for a point improvement of 0.1 in some metric, \nthat we don't even know how the variance around that 0.1 improvement looks like?\nWe should compare models in a way that will show us the fuller picture and allow us to make a truly informed decision about model selection.\n\n\n### Going Bayesian\nNow, I'm positive that the regression framework I presented here is somewhat equivalent to what happens in R when you use `anova(mod1, mod2)` (ANOVA with two models). \nIn R, you often specify a `test` parameter to `anova` (like `\"Chisq\"` for a likelihood ratio test),\nbut this requires knowing the degrees of freedom the models have in order to properly set up the null distribution to draw p-values from. \nNow, go figure what the degrees of freedom of ChatGPT[^9].\nI'm not sure if the Wald test used for the regression p-value (or confidence intervals) sidesteps it, but regardless, \nadopting a Bayesian framework will save us from asymptotic requirements, as the uncertainty will be directly assessed from the data at hand.\n\n[^9]: Or any deep learning model, for that matter. It seems the classical statistical theory does not always correspond to observed evidence from these types of models.\n\nAdopting a Bayesian framework will come with multiple strengths:\n\n* First, it will allow us to focus on magnitude of difference and estimation instead of null-hypothesis significant testing [@kruschke2013bayesian]; this will be even more important for non-inferiority tests.\n* Second, it will allow us to interpret the uncertainty intervals more naturally and give probabilistic estimates to how likely it is that one model is better than the other (replacing confidence intervals with credible intervals).\n* Third, the Bayesian regression framework is more easily expandable to allow for multiple datasets (mixed/multilevel models), multiple models, and more proper modeling (generalized linear models).\n\n\n#### Regression modeling in Python with `bambi`\n`bambi` is great, our very own `brms`. \nIt's a powerful tool to specify regression formulas (like above for `statsmodels`) but then it will utilize `PyMC` under the hood to perform an MCMC-based Bayesian estimation, and do lots of additional good stuff like specifying priors automatically etc.\nRepeating the model comparison from above is as simple as:\n\n::: {#0c75279a .cell execution_count=9}\n``` {.python .cell-code}\nimport bambi as bmb\n\n# ols = smf.ols(\"li ~ 1 + C(model)\", data=sqe).fit()\nbols = bmb.Model(\"li ~ 1 + model + (1|id)\", data=sqe).fit(\n   cores=1,  # `quarto render` fails for cores>1 for unknown reason\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [sigma, Intercept, model, 1|id_sigma, 1|id_offset]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"d2b270c8f8bf4af5a4076877ecc303fc\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 7 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n```\n:::\n:::\n\n\nHere we account for the repeated measures by adding an intercept for each , which accounts to the fact that different data points have different characteristics making them easier or harder to adjust, and we want to adjust for that\nIdeally, after fitting a Bayesian model you should evaluate the fit using some diagnostics and `az.trace_plot()`, which I will skip (but I checked, it's ok).\n\n\n#### Bayesian noninferiority testing\nThe Bayesian inference framework, focusing on estimation and providing a full posterior distribution of the difference in performance, makes it really easy to test for noninferiority. \nWe can use the Region of Practical Equivalence (ROPE) to specify an intervals that is equivalent to the null, \nmeaning it has practically no effect of interest and is therefore negligible (e.g., within 1% change in performance between model is good enough [@kruschke2018rejecting]).\n\n::: {#636a3917 .cell execution_count=10}\n``` {.python .cell-code}\nimport arviz as az\n\nloss_std = sqe[\"li\"].std()\naz.plot_posterior(\n    bols, var_names=\"model\",\n    ref_val=0, \n    rope=[-0.1*loss_std, 0.1*loss_std],  \n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n<!-- \nFor example, we see the High Density Interval (HDI, the interval equivalent to a confidence interval in some sense) does not overlap with the ROPE at all, \nalthough 1% of the entire posterior distribution does overlap with the ROPE, suggesting there's a 1% chance the the difference in performance is actually equivalent. \nAdditionally, 0.3% of the distribution is below 0, suggesting there's a miniscule (but existing) chance that gradient boosting trees are actually worse than linear regression on this prediction task, but that is practically negligible.  \n-->\n\nFor example, we see both the High Density Interval (HDI, the interval equivalent to a confidence interval in some sense) \nand the entire posterior distribution do overlap some with the ROPE, suggesting there's a ~4% chance the difference in performance is actually equivalent. \nAdditionally, ~0.5% of the distribution is below 0, suggesting there's a miniscule (but existing) chance that gradient boosting trees are actually worse than linear regression on this prediction task, but that the magnitude in which they are worse is practically negligible.\nHowever, the lion share of the evidence suggests the linear regression prediction model has higher errors than the boosting trees.\n\n\n### Extensions\nThis regression framework for comparing models is easily extendable, especially the Bayesian one.\nHere are some examples:\n\n1. **Multiple datasets: assess overall performance over multiple datasets**.  \n   By concatenating the individual-level errors from multiple datasets, but still focusing on the `model` variable that estimates the difference between models.\n   In such case, however, the response variable (errors) might no longer be individual and identically distributed (IID), because it originates from the same models.\n   Ignoring these correlations between observations and assuming they are IID might make our estimates overly confident, which is the counterproductive to our main goal of assessing uncertainty properly. \n   Luckily, we can apply multilevel modeling (mixed linear models) to regress away these dependencies simply by specifying\n   `li ~ 1 + model + (1 + model | dataset) + (1 | id)`[^10] [^11]. \n2. **Multiple models: Compare multiple models**. \n   Extending the dummy encoding of the model to any other contrast encoding between models can be done in a straightforward way, and allow comparing multiple models in multiple ways simultaneously. \n3. **Multiple metrics: simultaneously**\n   If we have several metrics that interest us than we can model the difference between models on all of them simultaneously by stacking the different errors side-by-side and making a multivariate regression model. \n4. **Many types of metrics**\n   So far I've discussed the squared error, but this framework can support non-continuous errors (metric) too.\n   For example, a classification model can be evaluated using the log-loss in a similar way to how squared-errors were done above, \n   but we can also use the 0-1 loss instead--which is a much more interpretable loss--and use a logistic-regression instead of a linear regression model.\n3. **Proper modeling of the errors**. \n   I've used a linear model so far, but it assumes the response (in our case the individual-level errors) can range between $-\\infty$ to $\\infty$. \n   But errors are almost always strictly positive, and if we want to respect that support, we should, instead, \n   model it with a Gamma regression - a generalized linear model with a Gamma distribution (and a log-link), \n   which operates on the $(0, \\infty)$ range.[^12]\n4. **Incorporate metadata**\n   Models (and datasets) may come with plenty of metadata (e.g., number of parameters). \n   These metadata can be incorporated into the regression as additional covariates and allow us to make even more interesting conclusions than just comparing models.\n   For example, incorporating model size will enable us to discuss about difference in performance between models *while holding model size constant*. \n   This will make cross-player comparison (e.g., GPT vs. Gemini vs. Claude) much more juicy.\n\n\n[^10]: There are frequentist mixed models, and these correlation structures might also be accounted for using Generalized Estimating Equations (GEE), but their uncertainties are not always trustworthy. Bayesian mixed models, however, still shine. \n[^11]: Once in the realm of multiple models multiple dataset, it may be interesting to evaluate performance in the item-response framework to also get a gist of what datasets are easier or harder.\n[^12]: Actually, since errors can be truly 0, there might be a need to extend the model to be a hurdle Gamma model.\n\n\nI can quickly demonstrate some of these[^13]. \nLet's generate more data from a different dataset, by repeating the process above:\n\n[^13]: no multiple metrics because `bambi` does not support multivariate gamma models\n\n::: {#b158f09c .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_friedman3\n\nX, y = make_friedman3(n_samples=400, noise=1, random_state=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\nlr = LinearRegression().fit(X_train, y_train)\ngbt = GradientBoostingRegressor().fit(X_train, y_train)\n\nlr_res = (y_test - lr.predict(X_test))**2\ngbt_res = (y_test - gbt.predict(X_test))**2\n\nfriedman3_sqe = pd.concat({\"lr\": pd.Series(lr_res), \"gbt\": pd.Series(gbt_res)}, names=[\"model\", \"id\"]).rename(\"li\")\nfriedman3_sqe = friedman3_sqe.reset_index()\n```\n:::\n\n\nNow let's stack this dataset on top of the previous dataset\n\n::: {#9a8f2280 .cell execution_count=12}\n``` {.python .cell-code}\nsqe = pd.concat(\n    {\"friedman1\": sqe, \"friedman3\": friedman3_sqe},\n    names=[\"dataset\"],\n).reset_index().drop(columns=\"level_1\")\nsqe\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>model</th>\n      <th>id</th>\n      <th>li</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>friedman1</td>\n      <td>lr</td>\n      <td>0</td>\n      <td>0.089986</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>friedman1</td>\n      <td>lr</td>\n      <td>1</td>\n      <td>1.319738</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>friedman1</td>\n      <td>lr</td>\n      <td>2</td>\n      <td>6.044722</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>friedman1</td>\n      <td>lr</td>\n      <td>3</td>\n      <td>6.061478</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>friedman1</td>\n      <td>lr</td>\n      <td>4</td>\n      <td>13.876234</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>friedman3</td>\n      <td>gbt</td>\n      <td>195</td>\n      <td>0.277374</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>friedman3</td>\n      <td>gbt</td>\n      <td>196</td>\n      <td>0.010730</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>friedman3</td>\n      <td>gbt</td>\n      <td>197</td>\n      <td>0.078106</td>\n    </tr>\n    <tr>\n      <th>798</th>\n      <td>friedman3</td>\n      <td>gbt</td>\n      <td>198</td>\n      <td>3.454848</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>friedman3</td>\n      <td>gbt</td>\n      <td>199</td>\n      <td>0.285768</td>\n    </tr>\n  </tbody>\n</table>\n<p>800 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\nNow we can fit a multilevel Gamma regression on the combined data:\n\n::: {#c1207981 .cell execution_count=13}\n``` {.python .cell-code}\ngamma_model = bmb.Model(\n    # \"li ~ 1 + model + (1 + model | dataset) + (1 | id)\", \n     \"li ~ 1 + model * C(dataset) + (1 | id)\", \n    data=sqe,\n    family=\"gamma\",\n    link=\"log\",\n)\ngamma_idata = gamma_model.fit(\n    random_seed=11,\n    cores=1  # `quarto render` fails for cores>1 for unknown reason\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (2 chains in 1 job)\nNUTS: [alpha, Intercept, model, C(dataset), model:C(dataset), 1|id_sigma, 1|id_offset]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"d85f9335c3e0423bb6ee9894b92c5753\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 15 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n```\n:::\n:::\n\n\nNote that I've used an interaction `li ~ 1 + model * C(dataset) + (1 | id)`, which is the no-pooling (\"memoryless\" a-la McElreath) version of the varying slopes `li ~ 1 + model + (1 + model | dataset) + (1 | id)`.\nThey are equivalent, as they both provide an effect per dataset (`C(dataset) -> (1 | dataset)`) as well as allowing the models to perform differently per dataset (`model:C(dataset) -> (model | dataset)`).\nI am not proud of this substitution but I could not get the sampler to properly sample the hierarchical without divergences.\nThe `model:dataset` product term also means we can't just look at the `model` parameter to assess the difference (as it depends on `dataset`), so instead I will use a marginal effects approach :\n\n::: {#dc866be7 .cell execution_count=14}\n``` {.python .cell-code}\nbmb.interpret.comparisons(\n    model=gamma_model,\n    idata=gamma_idata,\n    contrast=\"model\",\n    average_by=True,\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\204048756\\miniforge3\\envs\\bambi\\Lib\\site-packages\\arviz\\rcparams.py:368: FutureWarning: stats.hdi_prob is deprecated since 0.18.0, use stats.ci_prob instead\n  warnings.warn(\nDefault computed for contrast variable: model\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>estimate_type</th>\n      <th>value</th>\n      <th>estimate</th>\n      <th>lower_3.0%</th>\n      <th>upper_97.0%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>model</td>\n      <td>diff</td>\n      <td>(gbt, lr)</td>\n      <td>1.023786</td>\n      <td>-0.21463</td>\n      <td>2.500189</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhich is in line with the results above showing larger residuals for linear regression than for gradient boosting trees.\n\n## Conclusions\nI've presented a framework to make model comparison rigorously and meaningfully by incorporating uncertainty measure around performance difference.\nThe framework is super easy because it builds on top of regression models, widely available in every scientific software, taking advantage of them just being fancy extendable averaging machines. \nThis opens up many possibilities of properly comparing multiple models on multiple datasets and multiple metrics.\nMost importantly, it enables us to perform noninferiority tests and understand whether two models are comparable (have similar performance) even though one of them may be much smaller / easier to train / cheaper to train / have smaller carbon footprint. \nThis may encourage researchers to come up with substantially new approaches, rather than chasing negligible improvements of fraction of a percentage in leaderboards.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"1a3c8fc60dbd4bb4a0fb5ce581d51483\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"470f89dcc4d44032ba1931cd897e87d7\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"d2b270c8f8bf4af5a4076877ecc303fc\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_1a3c8fc60dbd4bb4a0fb5ce581d51483\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">                                                                                                                   \\n <span style=\\\"font-weight: bold\\\"> Progress                  </span> <span style=\\\"font-weight: bold\\\"> Draws </span> <span style=\\\"font-weight: bold\\\"> Divergences </span> <span style=\\\"font-weight: bold\\\"> Step size </span> <span style=\\\"font-weight: bold\\\"> Grad evals </span> <span style=\\\"font-weight: bold\\\"> Sampling Speed </span> <span style=\\\"font-weight: bold\\\"> Elapsed </span> <span style=\\\"font-weight: bold\\\"> Remaining </span> \\n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   2000    0             0.45        15           524.40 draws/s   0:00:03   0:00:00    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   2000    0             0.44        15           273.39 draws/s   0:00:07   0:00:00    \\n                                                                                                                   \\n</pre>\\n\",\"text/plain\":\"                                                                                                                   \\n \\u001b[1m \\u001b[0m\\u001b[1mProgress                 \\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDraws\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDivergences\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mStep size\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mGrad evals\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mSampling Speed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mElapsed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mRemaining\\u001b[0m\\u001b[1m \\u001b[0m \\n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \\n  ━━━━━━━━━━━━━━━━━━━━━━━━━   2000    0             0.45        15           524.40 draws/s   0:00:03   0:00:00    \\n  ━━━━━━━━━━━━━━━━━━━━━━━━━   2000    0             0.44        15           273.39 draws/s   0:00:07   0:00:00    \\n                                                                                                                   \\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"d85f9335c3e0423bb6ee9894b92c5753\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_470f89dcc4d44032ba1931cd897e87d7\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">                                                                                                                   \\n <span style=\\\"font-weight: bold\\\"> Progress                  </span> <span style=\\\"font-weight: bold\\\"> Draws </span> <span style=\\\"font-weight: bold\\\"> Divergences </span> <span style=\\\"font-weight: bold\\\"> Step size </span> <span style=\\\"font-weight: bold\\\"> Grad evals </span> <span style=\\\"font-weight: bold\\\"> Sampling Speed </span> <span style=\\\"font-weight: bold\\\"> Elapsed </span> <span style=\\\"font-weight: bold\\\"> Remaining </span> \\n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   2000    0             0.39        15           278.72 draws/s   0:00:07   0:00:00    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   2000    0             0.45        15           129.89 draws/s   0:00:15   0:00:00    \\n                                                                                                                   \\n</pre>\\n\",\"text/plain\":\"                                                                                                                   \\n \\u001b[1m \\u001b[0m\\u001b[1mProgress                 \\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDraws\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDivergences\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mStep size\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mGrad evals\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mSampling Speed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mElapsed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mRemaining\\u001b[0m\\u001b[1m \\u001b[0m \\n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \\n  ━━━━━━━━━━━━━━━━━━━━━━━━━   2000    0             0.39        15           278.72 draws/s   0:00:07   0:00:00    \\n  ━━━━━━━━━━━━━━━━━━━━━━━━━   2000    0             0.45        15           129.89 draws/s   0:00:15   0:00:00    \\n                                                                                                                   \\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}